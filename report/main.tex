\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{geometry}
\usepackage{fancyhdr}
\usepackage{multirow}
\usepackage{array}

\geometry{margin=1in}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    citecolor=blue,
    urlcolor=blue
}

\title{\textbf{Parameter-Efficient Fine-Tuning of Small Language Models:\\ A Cross-Domain Evaluation Using LoRA}}

\author{
    Aryan Pawar \\
    Mahindra University \\
    \texttt{se22uari195@mahindrauniversity.edu.in}
}

\date{\today}

\begin{document}

\maketitle

\begin{abstract}
The increasing computational demands of fine-tuning large language models have motivated the development of parameter-efficient fine-tuning (PEFT) methods. This work presents a comprehensive empirical evaluation of Low-Rank Adaptation (LoRA) applied to Pythia-410M, a small language model (SLM) with 410 million parameters, across five diverse text domains. We systematically evaluate model performance on TinyStories, SQuAD, Yelp Reviews, WikiText-2, and AG News datasets, achieving validation perplexities ranging from 6.46 to 30.55. Our results demonstrate that LoRA enables effective domain adaptation while updating only 0.39\% of model parameters (1.57M trainable parameters), reducing training time to approximately 12 minutes per dataset on NVIDIA T4 GPU. We observe significant performance variation across domains, with narrative text (TinyStories, PPL=6.46) showing superior adaptation compared to news classification tasks (AG News, PPL=30.55). All experiments are tracked using Weights \& Biases, providing reproducible benchmarks for future PEFT research on small-scale language models.
\end{abstract}

\textbf{Keywords:} Parameter-Efficient Fine-Tuning, LoRA, Small Language Models, Transfer Learning, Domain Adaptation, Pythia

\section{Introduction}

\subsection{Motivation}

The transformer architecture \cite{vaswani2017attention} has revolutionized natural language processing, enabling models like GPT-2 \cite{radford2019language} and GPT-3 \cite{brown2020language} to achieve remarkable performance across diverse tasks. However, the standard approach of full fine-tuning—updating all model parameters—becomes computationally prohibitive as models scale to billions of parameters. For a 175B parameter model like GPT-3, full fine-tuning requires storing and updating all parameters, consuming substantial memory and computational resources.

Parameter-Efficient Fine-Tuning (PEFT) methods address this challenge by updating only a small fraction of model parameters while maintaining competitive performance. Among these approaches, Low-Rank Adaptation (LoRA) \cite{hu2021lora} has emerged as particularly effective, introducing trainable low-rank decomposition matrices into transformer layers while keeping the original model frozen. This enables fine-tuning with a fraction of the memory and computational cost of full fine-tuning.

\subsection{Research Questions}

This work investigates the following research questions:

\begin{enumerate}
    \item \textbf{RQ1}: How effectively does LoRA enable domain adaptation for small language models across diverse text types?
    \item \textbf{RQ2}: What is the computational efficiency of LoRA fine-tuning in terms of training time, memory usage, and parameter count?
    \item \textbf{RQ3}: How does model performance vary across different text domains, and what factors influence this variation?
    \item \textbf{RQ4}: Can LoRA-based fine-tuning achieve competitive performance with minimal parameter updates ($<$1\% trainable parameters)?
\end{enumerate}

\subsection{Contributions}

Our contributions are as follows:

\begin{itemize}
    \item \textbf{Comprehensive Evaluation}: We conduct systematic experiments across five diverse datasets spanning narratives, question-answering, reviews, encyclopedic text, and news classification.
    \item \textbf{Efficiency Analysis}: We demonstrate that LoRA enables effective fine-tuning with only 0.39\% trainable parameters and $\sim$12 minute training times per dataset.
    \item \textbf{Cross-Domain Insights}: We provide detailed analysis of performance patterns across domains, identifying text characteristics that correlate with adaptation success.
    \item \textbf{Reproducible Framework}: We release a complete implementation with experiment tracking, enabling reproduction and extension of our results.
\end{itemize}

\section{Background and Related Work}

\subsection{Transformer Language Models}

The transformer architecture \cite{vaswani2017attention} introduced self-attention mechanisms that enable modeling long-range dependencies in sequences. Modern language models like GPT-2 \cite{radford2019language} and GPT-3 \cite{brown2020language} demonstrate that scaling model size and training data leads to emergent capabilities in few-shot and zero-shot learning. However, this scaling comes with increased computational requirements for both training and fine-tuning.

Small Language Models (SLMs) in the 100M-1B parameter range, such as Pythia \cite{biderman2023pythia}, offer a practical middle ground. While less capable than larger models, SLMs can be trained and deployed with modest computational resources, making them accessible for research and production use.

\subsection{Parameter-Efficient Fine-Tuning}

Full fine-tuning updates all model parameters, requiring storage of gradients and optimizer states for every parameter. For large models, this becomes impractical. PEFT methods address this by introducing small trainable modules while keeping most of the model frozen.

Several PEFT approaches have been proposed:

\textbf{Adapter Layers} \cite{houlsby2019adapter}: Insert small feedforward networks between transformer layers. AdapterFusion \cite{pfeiffer2021adapterfusion} extends this by learning to compose multiple adapters.

\textbf{Prefix Tuning} \cite{li2021prefix}: Prepends trainable continuous prompts to input sequences, optimizing these prompts while keeping the model frozen.

\textbf{BitFit} \cite{ben2022bitfit}: Updates only bias terms in the model, achieving surprising effectiveness with minimal parameter updates.

\textbf{LoRA} \cite{hu2021lora}: Introduces low-rank decomposition matrices into attention layers, enabling efficient adaptation with competitive performance.

\subsection{Low-Rank Adaptation (LoRA)}

LoRA \cite{hu2021lora} hypothesizes that weight updates during fine-tuning have low intrinsic dimensionality. For a pre-trained weight matrix $W_0 \in \mathbb{R}^{d \times k}$, LoRA represents the update as:

\begin{equation}
    W = W_0 + \Delta W = W_0 + BA
\end{equation}

where $B \in \mathbb{R}^{d \times r}$ and $A \in \mathbb{R}^{r \times k}$ with rank $r \ll \min(d, k)$. During training, $W_0$ remains frozen while $A$ and $B$ are updated. The number of trainable parameters is reduced from $d \times k$ to $r \times (d + k)$.

LoRA offers several advantages:
\begin{itemize}
    \item \textbf{Memory Efficiency}: Only stores gradients for low-rank matrices
    \item \textbf{No Inference Latency}: Merged weights $(W_0 + BA)$ at deployment
    \item \textbf{Modular}: Multiple LoRA adapters can be trained for different tasks
    \item \textbf{Competitive Performance}: Achieves results comparable to full fine-tuning
\end{itemize}

\section{Methodology}

\subsection{Model Architecture}

We use \textbf{Pythia-410M} \cite{biderman2023pythia}, a decoder-only transformer language model with the following specifications:

\begin{table}[h]
\centering
\begin{tabular}{ll}
\toprule
\textbf{Parameter} & \textbf{Value} \\
\midrule
Total Parameters & 406,851,584 \\
Hidden Dimension & 1,024 \\
Number of Layers & 24 \\
Attention Heads & 16 \\
Vocabulary Size & 50,277 \\
Context Length & 2,048 tokens \\
\bottomrule
\end{tabular}
\caption{Pythia-410M Model Specifications}
\label{tab:model_specs}
\end{table}

Pythia is trained on The Pile \cite{gao2020pile}, a diverse 800GB corpus, and provides checkpoints at multiple training stages, enabling controlled experimentation.

\subsection{LoRA Configuration}

We apply LoRA to the query-key-value projection matrices in all attention layers. Our configuration:

\begin{table}[h]
\centering
\begin{tabular}{ll}
\toprule
\textbf{Hyperparameter} & \textbf{Value} \\
\midrule
LoRA Rank ($r$) & 16 \\
LoRA Alpha ($\alpha$) & 32 \\
LoRA Dropout & 0.1 \\
Target Modules & \texttt{query\_key\_value} \\
Bias & None \\
\midrule
Trainable Parameters & 1,572,864 \\
Total Parameters & 406,851,584 \\
Trainable \% & 0.39\% \\
\bottomrule
\end{tabular}
\caption{LoRA Hyperparameters and Parameter Counts}
\label{tab:lora_config}
\end{table}

The scaling factor $\alpha/r = 2$ controls the magnitude of LoRA updates. We freeze all original model parameters and update only the low-rank matrices $A$ and $B$.

\subsection{Training Configuration}

All experiments use consistent training hyperparameters to ensure fair comparison across datasets:

\begin{table}[h]
\centering
\begin{tabular}{ll}
\toprule
\textbf{Hyperparameter} & \textbf{Value} \\
\midrule
Optimizer & AdamW \\
Learning Rate & $1 \times 10^{-4}$ \\
LR Scheduler & Cosine \\
Warmup Steps & 100 \\
Weight Decay & 0.01 \\
Batch Size & 8 \\
Gradient Accumulation & 1 \\
Max Gradient Norm & 1.0 \\
Epochs & 1 \\
Max Sequence Length & 512 tokens \\
Precision & FP32 \\
Random Seed & 42 \\
\bottomrule
\end{tabular}
\caption{Training Hyperparameters}
\label{tab:training_config}
\end{table}

We use gradient checkpointing to reduce memory consumption and enable training on a single NVIDIA T4 GPU with 16GB VRAM.

\subsection{Datasets}

We evaluate on five datasets covering diverse domains:

\textbf{TinyStories} \cite{eldan2023tinystories}: A dataset of simple, child-appropriate narratives generated synthetically. Contains 2.1M training examples. We sample 2,000 examples for efficiency.

\textbf{SQuAD} \cite{rajpurkar2016squad}: Stanford Question Answering Dataset containing Wikipedia passages for reading comprehension. We use context passages as training data (1,000 samples).

\textbf{Yelp Reviews}: Restaurant reviews from the Yelp dataset, spanning ratings from 1-5 stars (1,000 samples).

\textbf{WikiText-2} \cite{merity2016pointer}: A collection of verified Wikipedia articles, representing formal encyclopedic writing (1,000 samples).

\textbf{AG News} \cite{zhang2015character}: News article classification dataset with four categories: World, Sports, Business, Sci/Tech (2,000 samples).

\begin{table}[h]
\centering
\begin{tabular}{llll}
\toprule
\textbf{Dataset} & \textbf{Domain} & \textbf{Samples} & \textbf{Avg Length} \\
\midrule
TinyStories & Narratives & 2,000 & Short \\
SQuAD & QA Context & 1,000 & Medium \\
Yelp Reviews & Reviews & 1,000 & Short \\
WikiText-2 & Encyclopedia & 1,000 & Long \\
AG News & News & 2,000 & Short \\
\bottomrule
\end{tabular}
\caption{Dataset Characteristics}
\label{tab:datasets}
\end{table}

All datasets are tokenized using the Pythia tokenizer with a maximum length of 512 tokens. We split each dataset 90/10 for training and validation.

\subsection{Evaluation Metrics}

We report three primary metrics:

\textbf{Validation Loss}: Cross-entropy loss on held-out validation data, computed as:
\begin{equation}
    \mathcal{L} = -\frac{1}{N}\sum_{i=1}^{N} \log P(x_i | x_{<i})
\end{equation}

\textbf{Perplexity (PPL)}: Exponential of validation loss, measuring model uncertainty:
\begin{equation}
    \text{PPL} = \exp(\mathcal{L})
\end{equation}
Lower perplexity indicates better language modeling performance.

\textbf{Bits Per Token (BPT)}: Information-theoretic measure of compression:
\begin{equation}
    \text{BPT} = \frac{\mathcal{L}}{\log 2}
\end{equation}

\subsection{Implementation Details}

Our implementation uses:
\begin{itemize}
    \item \textbf{Framework}: PyTorch 2.0, Hugging Face Transformers, PEFT library
    \item \textbf{Hardware}: Single NVIDIA T4 GPU (16GB VRAM), 2 CPU cores
    \item \textbf{Experiment Tracking}: Weights \& Biases for logging metrics, system stats
    \item \textbf{Training Time}: $\sim$12 minutes per dataset, $\sim$1 hour total
\end{itemize}

Code is available at: \url{https://github.com/aryanpawar1234/slm-lora-benchmark}

\section{Experimental Results}

\subsection{Overall Performance}

Table \ref{tab:main_results} presents the primary results across all five datasets. We observe substantial variation in performance, with perplexity ranging from 6.46 (TinyStories) to 30.55 (AG News).

\begin{table}[h]
\centering
\begin{tabular}{lccccc}
\toprule
\textbf{Dataset} & \textbf{Domain} & \textbf{Loss $\downarrow$} & \textbf{PPL $\downarrow$} & \textbf{BPT $\downarrow$} & \textbf{Time (min)} \\
\midrule
TinyStories & Narratives & \textbf{1.87} & \textbf{6.46} & \textbf{2.69} & 12 \\
SQuAD & QA Context & 2.69 & 14.68 & 3.88 & 11 \\
Yelp Reviews & Reviews & 3.23 & 25.40 & 4.67 & 11 \\
WikiText-2 & Encyclopedia & 3.41 & 30.28 & 4.92 & 11 \\
AG News & News & 3.42 & 30.55 & 4.93 & 11 \\
\midrule
\textbf{Mean} & --- & 2.92 & 21.47 & 4.22 & 11.2 \\
\textbf{Std Dev} & --- & 0.68 & 10.67 & 0.98 & 0.4 \\
\bottomrule
\end{tabular}
\caption{Performance across five datasets. All models trained with identical LoRA configuration (r=16, $\alpha$=32). Lower values indicate better performance.}
\label{tab:main_results}
\end{table}

\subsection{Domain-Specific Analysis}

\textbf{Narratives (TinyStories)}: Achieves the lowest perplexity (6.46), indicating strong adaptation. The simple, repetitive structure of children's stories aligns well with the model's pre-training on natural language.

\textbf{Question-Answering (SQuAD)}: Moderate perplexity (14.68) on Wikipedia contexts. The formal, informative writing style presents moderate difficulty.

\textbf{Reviews (Yelp)}: Higher perplexity (25.40) reflects the informal, varied nature of user-generated content with diverse vocabulary and colloquialisms.

\textbf{Encyclopedic Text (WikiText-2)}: High perplexity (30.28) despite Wikipedia being in the pre-training corpus. The specific articles in WikiText-2 may require domain-specific knowledge.

\textbf{News Classification (AG News)}: Highest perplexity (30.55), possibly due to the classification-oriented nature of the task and diverse news topics requiring specialized vocabulary.

\subsection{Computational Efficiency}

Table \ref{tab:efficiency} demonstrates the efficiency gains of LoRA:

\begin{table}[h]
\centering
\begin{tabular}{lcc}
\toprule
\textbf{Metric} & \textbf{Full Fine-Tuning} & \textbf{LoRA} \\
\midrule
Trainable Parameters & 406,851,584 (100\%) & 1,572,864 (0.39\%) \\
Training Time (per epoch) & $\sim$2 hours* & 11-12 min \\
GPU Memory & $\sim$16 GB & $\sim$8 GB \\
Checkpoint Size & 1.6 GB & 6 MB \\
\bottomrule
\end{tabular}
\caption{Efficiency comparison. *Estimated based on typical full fine-tuning times.}
\label{tab:efficiency}
\end{table}

Key efficiency observations:
\begin{itemize}
    \item \textbf{Parameter Reduction}: 257$\times$ fewer trainable parameters
    \item \textbf{Speed}: $\sim$10$\times$ faster training per epoch
    \item \textbf{Memory}: 50\% reduction in GPU memory usage
    \item \textbf{Storage}: 266$\times$ smaller adapter checkpoints
\end{itemize}

\subsection{Training Dynamics}

Figure \ref{fig:training_curves} shows training loss curves for all datasets (placeholder - add W\&B screenshots):

\begin{figure}[h]
\centering
\fbox{\parbox{0.8\textwidth}{\centering [Insert W\&B training loss curves here] \\ 5 subplots showing loss vs. steps for each dataset}}
\caption{Training loss curves across datasets. All models converge within 200-230 steps.}
\label{fig:training_curves}
\end{figure}

All models converge smoothly without signs of overfitting or instability. The cosine learning rate schedule enables stable optimization.

\section{Discussion}

\subsection{Performance Patterns}

Our results reveal clear performance patterns related to text characteristics:

\textbf{Text Simplicity}: Simpler, more predictable text (TinyStories) achieves lower perplexity. The repetitive narrative structure and limited vocabulary reduce modeling difficulty.

\textbf{Domain Specificity}: Specialized domains (AG News, WikiText-2) show higher perplexity, suggesting the need for domain-specific knowledge not fully captured by general pre-training.

\textbf{Formality vs. Informality}: Formal text (SQuAD, WikiText-2) shows moderate perplexity, while informal user-generated content (Yelp) presents challenges due to linguistic diversity.

\subsection{LoRA Effectiveness}

Despite updating only 0.39\% of parameters, LoRA achieves effective adaptation across all domains. This validates the low-rank hypothesis: weight updates during fine-tuning lie in a low-dimensional subspace.

The consistent training times ($\sim$11-12 minutes) across datasets demonstrate LoRA's scalability. The small adapter size (6 MB) enables efficient storage and deployment of multiple domain-specific models.

\subsection{Comparison to Related Work}

While direct comparison is difficult due to different base models and datasets, our results align with findings from the LoRA paper \cite{hu2021lora}, which reported competitive performance with full fine-tuning using similar parameter budgets.

Our work extends prior research by:
\begin{itemize}
    \item Evaluating small models (410M parameters vs. GPT-3's 175B)
    \item Covering diverse text domains in a single study
    \item Providing reproducible benchmarks with open-source implementation
\end{itemize}

\subsection{Limitations}

Several limitations should be noted:

\textbf{Single Epoch Training}: We train for only one epoch due to computational constraints. Additional epochs may improve performance.

\textbf{Limited Hyperparameter Search}: We use a single LoRA rank (r=16). Optimal ranks may vary by dataset.

\textbf{Small Dataset Samples}: We use 1,000-2,000 samples per dataset. Larger samples may yield different results.

\textbf{Single Model}: We evaluate only Pythia-410M. Results may differ for other model architectures and sizes.

\section{Conclusion and Future Work}

\subsection{Summary}

This work presents a comprehensive evaluation of LoRA-based parameter-efficient fine-tuning for small language models across five diverse text domains. Our key findings:

\begin{enumerate}
    \item LoRA enables effective domain adaptation with only 0.39\% trainable parameters
    \item Performance varies significantly across domains (PPL: 6.46-30.55)
    \item Training efficiency is excellent: $\sim$12 minutes per dataset on T4 GPU
    \item Simpler, more structured text (narratives) shows superior adaptation
\end{enumerate}

These results demonstrate that PEFT methods like LoRA make fine-tuning accessible even for resource-constrained settings, enabling practical deployment of domain-adapted language models.

\subsection{Future Directions}

Several directions for future work emerge:

\textbf{Hyperparameter Optimization}: Systematic search over LoRA rank, learning rate, and other hyperparameters may improve performance.

\textbf{Multi-Epoch Training}: Extending training beyond one epoch with appropriate regularization to prevent overfitting.

\textbf{Larger Models}: Evaluating whether our findings generalize to larger models (1B-7B parameters).

\textbf{Task-Specific Evaluation}: Beyond perplexity, evaluate on downstream tasks (classification, generation quality).

\textbf{Alternative PEFT Methods}: Compare LoRA to adapters, prefix tuning, and prompt tuning.

\textbf{Mixture of Adapters}: Investigate combining multiple domain-specific LoRA adapters for multi-task performance.

\textbf{Quantization}: Combine LoRA with 4-bit or 8-bit quantization for further efficiency gains.

\subsection{Broader Impact}

By demonstrating effective fine-tuning with minimal resources, this work contributes to democratizing access to language model adaptation. Researchers and practitioners with limited computational budgets can leverage PEFT methods to create domain-specific models, fostering innovation in resource-constrained settings.

\section*{Acknowledgments}

We thank Weights \& Biases for providing experiment tracking infrastructure, and the Hugging Face team for the Transformers and PEFT libraries. This work was conducted using Google Colab's free GPU resources.

\begin{thebibliography}{99}

\bibitem{vaswani2017attention}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., \& Polosukhin, I. (2017).
Attention is all you need.
\textit{Advances in Neural Information Processing Systems}, 30.

\bibitem{radford2019language}
Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., \& Sutskever, I. (2019).
Language models are unsupervised multitask learners.
\textit{OpenAI Blog}, 1(8), 9.

\bibitem{brown2020language}
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., ... \& Amodei, D. (2020).
Language models are few-shot learners.
\textit{Advances in Neural Information Processing Systems}, 33, 1877-1901.

\bibitem{hu2021lora}
Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., \& Chen, W. (2021).
LoRA: Low-rank adaptation of large language models.
\textit{arXiv preprint arXiv:2106.09685}.

\bibitem{fedus2021switch}
Fedus, W., Zoph, B., \& Shazeer, N. (2021).
Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity.
\textit{arXiv preprint arXiv:2101.03961}.

\bibitem{pfeiffer2021adapterfusion}
Pfeiffer, J., Kamath, A., Rücklé, A., Cho, K., \& Gurevych, I. (2021).
AdapterFusion: Non-destructive task composition for transfer learning.
\textit{Proceedings of EACL}, 487-503.

\bibitem{ben2022bitfit}
Ben Zaken, E., Ravfogel, S., \& Goldberg, Y. (2022).
BitFit: Simple parameter-efficient fine-tuning for transformer-based masked language-models.
\textit{Proceedings of ACL}, 1-9.

\bibitem{houlsby2019adapter}
Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., De Laroussilhe, Q., Gesmundo, A., Attariyan, M., \& Gelly, S. (2019).
Parameter-efficient transfer learning for NLP.
\textit{Proceedings of ICML}, 2790-2799.

\bibitem{li2021prefix}
Li, X. L., \& Liang, P. (2021).
Prefix-tuning: Optimizing continuous prompts for generation.
\textit{Proceedings of ACL-IJCNLP}, 4582-4597.

\bibitem{biderman2023pythia}
Biderman, S., Schoelkopf, H., Anthony, Q., Bradley, H., O'Brien, K., Hallahan, E., ... \& Belrose, N. (2023).
Pythia: A suite for analyzing large language models across training and scaling.
\textit{Proceedings of ICML}, 2397-2430.

\bibitem{gao2020pile}
Gao, L., Biderman, S., Black, S., Golding, L., Hoppe, T., Foster, C., ... \& Leahy, C. (2020).
The Pile: An 800GB dataset of diverse text for language modeling.
\textit{arXiv preprint arXiv:2101.00027}.

\bibitem{eldan2023tinystories}
Eldan, R., \& Li, Y. (2023).
TinyStories: How small can language models be and still speak coherent English?
\textit{arXiv preprint arXiv:2305.07759}.

\bibitem{rajpurkar2016squad}
Rajpurkar, P., Zhang, J., Lopyrev, K., \& Liang, P. (2016).
SQuAD: 100,000+ questions for machine comprehension of text.
\textit{Proceedings of EMNLP}, 2383-2392.

\bibitem{merity2016pointer}
Merity, S., Xiong, C., Bradbury, J., \& Socher, R. (2016).
Pointer sentinel mixture models.
\textit{arXiv preprint arXiv:1609.07843}.

\bibitem{zhang2015character}
Zhang, X., Zhao, J., \& LeCun, Y. (2015).
Character-level convolutional networks for text classification.
\textit{Advances in Neural Information Processing Systems}, 28.

\end{thebibliography}

\appendix

\section{Hyperparameter Sensitivity}

Future work should investigate sensitivity to key hyperparameters:
\begin{itemize}
    \item LoRA rank: \{4, 8, 16, 32, 64\}
    \item Learning rate: \{1e-5, 5e-5, 1e-4, 5e-4\}
    \item Batch size: \{4, 8, 16, 32\}
\end{itemize}

\section{Weights \& Biases Links}

Experiment runs available at:
\begin{itemize}
    \item TinyStories: \url{https://wandb.ai/aryanpawar1515-mahindra-university/slm-lora-benchmark/runs/hrr5g7ws}
    \item AG News: \url{https://wandb.ai/aryanpawar1515-mahindra-university/slm-lora-benchmark/runs/txpy0k4n}
    \item Project: \url{https://wandb.ai/aryanpawar1515-mahindra-university/slm-lora-benchmark}
\end{itemize}

\section{Code Availability}

Complete implementation available at: \url{https://github.com/aryanpawar1234/slm-lora-benchmark}

\end{document}
