{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/aryanpawar1234/slm-lora-benchmark.git\n",
        "%cd slm-lora-benchmark\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8zrB2NdyPGG",
        "outputId": "0f30ec7e-6210-485c-d25d-86d76963b1ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'slm-lora-benchmark'...\n",
            "remote: Enumerating objects: 139, done.\u001b[K\n",
            "remote: Counting objects: 100% (139/139), done.\u001b[K\n",
            "remote: Compressing objects: 100% (107/107), done.\u001b[K\n",
            "remote: Total 139 (delta 41), reused 104 (delta 21), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (139/139), 70.81 KiB | 1.16 MiB/s, done.\n",
            "Resolving deltas: 100% (41/41), done.\n",
            "/content/slm-lora-benchmark\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ps_yAL-Myd5Z",
        "outputId": "dcbb6cd4-0bdb-4a16-e44a-305ee116cd55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mfkmRFQ5yhhE",
        "outputId": "40d7b40d-7375-4cef-b092-b6da06f83cc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 2)) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision>=0.15.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 3)) (0.23.0+cu126)\n",
            "Requirement already satisfied: torchaudio>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 4)) (2.8.0+cu126)\n",
            "Requirement already satisfied: transformers>=4.35.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 7)) (4.57.1)\n",
            "Requirement already satisfied: datasets>=2.14.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 8)) (4.0.0)\n",
            "Requirement already satisfied: tokenizers>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 9)) (0.22.1)\n",
            "Requirement already satisfied: peft>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 10)) (0.17.1)\n",
            "Requirement already satisfied: accelerate>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 11)) (1.11.0)\n",
            "Requirement already satisfied: safetensors>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 12)) (0.6.2)\n",
            "Requirement already satisfied: wandb>=0.15.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 15)) (0.22.3)\n",
            "Requirement already satisfied: pyyaml>=6.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 18)) (6.0.3)\n",
            "Requirement already satisfied: omegaconf>=2.3.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 19)) (2.3.0)\n",
            "Requirement already satisfied: numpy>=1.24.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 22)) (2.0.2)\n",
            "Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 23)) (2.2.2)\n",
            "Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 24)) (1.16.3)\n",
            "Requirement already satisfied: scikit-learn>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 27)) (1.6.1)\n",
            "Collecting evaluate>=0.4.0 (from -r requirements.txt (line 28))\n",
            "  Downloading evaluate-0.4.6-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 31)) (4.67.1)\n",
            "Requirement already satisfied: python-dotenv>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 32)) (1.2.1)\n",
            "Collecting colorama>=0.4.6 (from -r requirements.txt (line 33))\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: rich>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 34)) (13.9.4)\n",
            "Collecting jupyter>=1.0.0 (from -r requirements.txt (line 37))\n",
            "  Downloading jupyter-1.1.1-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting ipywidgets>=8.0.0 (from -r requirements.txt (line 38))\n",
            "  Downloading ipywidgets-8.1.8-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting notebook>=7.0.0 (from -r requirements.txt (line 39))\n",
            "  Downloading notebook-7.5.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pytest>=7.4.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 42)) (8.4.2)\n",
            "Collecting pytest-cov>=4.1.0 (from -r requirements.txt (line 43))\n",
            "  Downloading pytest_cov-7.0.0-py3-none-any.whl.metadata (31 kB)\n",
            "Collecting pytest-mock>=3.11.0 (from -r requirements.txt (line 44))\n",
            "  Downloading pytest_mock-3.15.1-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting black>=23.0.0 (from -r requirements.txt (line 47))\n",
            "  Downloading black-25.11.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.2/85.2 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting flake8>=6.0.0 (from -r requirements.txt (line 48))\n",
            "  Downloading flake8-7.3.0-py2.py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting isort>=5.12.0 (from -r requirements.txt (line 49))\n",
            "  Downloading isort-7.0.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting mypy>=1.5.0 (from -r requirements.txt (line 50))\n",
            "  Downloading mypy-1.18.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (2.2 kB)\n",
            "Collecting bitsandbytes>=0.41.0 (from -r requirements.txt (line 53))\n",
            "  Downloading bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r requirements.txt (line 2)) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r requirements.txt (line 2)) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r requirements.txt (line 2)) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r requirements.txt (line 2)) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r requirements.txt (line 2)) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r requirements.txt (line 2)) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r requirements.txt (line 2)) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r requirements.txt (line 2)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r requirements.txt (line 2)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r requirements.txt (line 2)) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r requirements.txt (line 2)) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r requirements.txt (line 2)) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r requirements.txt (line 2)) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r requirements.txt (line 2)) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r requirements.txt (line 2)) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r requirements.txt (line 2)) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r requirements.txt (line 2)) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r requirements.txt (line 2)) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r requirements.txt (line 2)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r requirements.txt (line 2)) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r requirements.txt (line 2)) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r requirements.txt (line 2)) (3.4.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision>=0.15.0->-r requirements.txt (line 3)) (11.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.35.0->-r requirements.txt (line 7)) (0.36.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.35.0->-r requirements.txt (line 7)) (25.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.35.0->-r requirements.txt (line 7)) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers>=4.35.0->-r requirements.txt (line 7)) (2.32.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.14.0->-r requirements.txt (line 8)) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.14.0->-r requirements.txt (line 8)) (0.3.8)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets>=2.14.0->-r requirements.txt (line 8)) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.14.0->-r requirements.txt (line 8)) (0.70.16)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from peft>=0.7.0->-r requirements.txt (line 10)) (5.9.5)\n",
            "Requirement already satisfied: click>=8.0.1 in /usr/local/lib/python3.12/dist-packages (from wandb>=0.15.0->-r requirements.txt (line 15)) (8.3.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb>=0.15.0->-r requirements.txt (line 15)) (3.1.45)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from wandb>=0.15.0->-r requirements.txt (line 15)) (4.5.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.12/dist-packages (from wandb>=0.15.0->-r requirements.txt (line 15)) (5.29.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.12/dist-packages (from wandb>=0.15.0->-r requirements.txt (line 15)) (2.11.10)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb>=0.15.0->-r requirements.txt (line 15)) (2.44.0)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.12/dist-packages (from omegaconf>=2.3.0->-r requirements.txt (line 19)) (4.9.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.0.0->-r requirements.txt (line 23)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.0.0->-r requirements.txt (line 23)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.0.0->-r requirements.txt (line 23)) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.3.0->-r requirements.txt (line 27)) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.3.0->-r requirements.txt (line 27)) (3.6.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=13.0.0->-r requirements.txt (line 34)) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=13.0.0->-r requirements.txt (line 34)) (2.19.2)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.12/dist-packages (from jupyter>=1.0.0->-r requirements.txt (line 37)) (6.6.3)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.12/dist-packages (from jupyter>=1.0.0->-r requirements.txt (line 37)) (7.16.6)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.12/dist-packages (from jupyter>=1.0.0->-r requirements.txt (line 37)) (6.17.1)\n",
            "Collecting jupyterlab (from jupyter>=1.0.0->-r requirements.txt (line 37))\n",
            "  Downloading jupyterlab-4.5.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting comm>=0.1.3 (from ipywidgets>=8.0.0->-r requirements.txt (line 38))\n",
            "  Downloading comm-0.2.3-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: ipython>=6.1.0 in /usr/local/lib/python3.12/dist-packages (from ipywidgets>=8.0.0->-r requirements.txt (line 38)) (7.34.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.12/dist-packages (from ipywidgets>=8.0.0->-r requirements.txt (line 38)) (5.7.1)\n",
            "Collecting widgetsnbextension~=4.0.14 (from ipywidgets>=8.0.0->-r requirements.txt (line 38))\n",
            "  Downloading widgetsnbextension-4.0.15-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in /usr/local/lib/python3.12/dist-packages (from ipywidgets>=8.0.0->-r requirements.txt (line 38)) (3.0.16)\n",
            "Requirement already satisfied: jupyter-server<3,>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from notebook>=7.0.0->-r requirements.txt (line 39)) (2.14.0)\n",
            "Collecting jupyterlab-server<3,>=2.28.0 (from notebook>=7.0.0->-r requirements.txt (line 39))\n",
            "  Downloading jupyterlab_server-2.28.0-py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: notebook-shim<0.3,>=0.2 in /usr/local/lib/python3.12/dist-packages (from notebook>=7.0.0->-r requirements.txt (line 39)) (0.2.4)\n",
            "Requirement already satisfied: tornado>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from notebook>=7.0.0->-r requirements.txt (line 39)) (6.5.1)\n",
            "Requirement already satisfied: iniconfig>=1 in /usr/local/lib/python3.12/dist-packages (from pytest>=7.4.0->-r requirements.txt (line 42)) (2.3.0)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.12/dist-packages (from pytest>=7.4.0->-r requirements.txt (line 42)) (1.6.0)\n",
            "Collecting coverage>=7.10.6 (from coverage[toml]>=7.10.6->pytest-cov>=4.1.0->-r requirements.txt (line 43))\n",
            "  Downloading coverage-7.12.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (9.1 kB)\n",
            "Collecting mypy-extensions>=0.4.3 (from black>=23.0.0->-r requirements.txt (line 47))\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting pathspec>=0.9.0 (from black>=23.0.0->-r requirements.txt (line 47))\n",
            "  Downloading pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting pytokens>=0.3.0 (from black>=23.0.0->-r requirements.txt (line 47))\n",
            "  Downloading pytokens-0.3.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting mccabe<0.8.0,>=0.7.0 (from flake8>=6.0.0->-r requirements.txt (line 48))\n",
            "  Downloading mccabe-0.7.0-py2.py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting pycodestyle<2.15.0,>=2.14.0 (from flake8>=6.0.0->-r requirements.txt (line 48))\n",
            "  Downloading pycodestyle-2.14.0-py2.py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting pyflakes<3.5.0,>=3.4.0 (from flake8>=6.0.0->-r requirements.txt (line 48))\n",
            "  Downloading pyflakes-3.4.0-py2.py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.14.0->-r requirements.txt (line 8)) (3.13.2)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb>=0.15.0->-r requirements.txt (line 15)) (4.0.12)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.35.0->-r requirements.txt (line 7)) (1.2.0)\n",
            "Collecting jedi>=0.16 (from ipython>=6.1.0->ipywidgets>=8.0.0->-r requirements.txt (line 38))\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.12/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.0->-r requirements.txt (line 38)) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.12/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.0->-r requirements.txt (line 38)) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.0->-r requirements.txt (line 38)) (3.0.52)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.12/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.0->-r requirements.txt (line 38)) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.12/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.0->-r requirements.txt (line 38)) (0.2.1)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.12/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.0->-r requirements.txt (line 38)) (4.9.0)\n",
            "Requirement already satisfied: anyio>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from jupyter-server<3,>=2.4.0->notebook>=7.0.0->-r requirements.txt (line 39)) (4.11.0)\n",
            "Requirement already satisfied: argon2-cffi>=21.1 in /usr/local/lib/python3.12/dist-packages (from jupyter-server<3,>=2.4.0->notebook>=7.0.0->-r requirements.txt (line 39)) (25.1.0)\n",
            "Requirement already satisfied: jupyter-client>=7.4.4 in /usr/local/lib/python3.12/dist-packages (from jupyter-server<3,>=2.4.0->notebook>=7.0.0->-r requirements.txt (line 39)) (7.4.9)\n",
            "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.12/dist-packages (from jupyter-server<3,>=2.4.0->notebook>=7.0.0->-r requirements.txt (line 39)) (5.9.1)\n",
            "Requirement already satisfied: jupyter-events>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from jupyter-server<3,>=2.4.0->notebook>=7.0.0->-r requirements.txt (line 39)) (0.12.0)\n",
            "Requirement already satisfied: jupyter-server-terminals>=0.4.4 in /usr/local/lib/python3.12/dist-packages (from jupyter-server<3,>=2.4.0->notebook>=7.0.0->-r requirements.txt (line 39)) (0.5.3)\n",
            "Requirement already satisfied: nbformat>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from jupyter-server<3,>=2.4.0->notebook>=7.0.0->-r requirements.txt (line 39)) (5.10.4)\n",
            "Requirement already satisfied: overrides>=5.0 in /usr/local/lib/python3.12/dist-packages (from jupyter-server<3,>=2.4.0->notebook>=7.0.0->-r requirements.txt (line 39)) (7.7.0)\n",
            "Requirement already satisfied: prometheus-client>=0.9 in /usr/local/lib/python3.12/dist-packages (from jupyter-server<3,>=2.4.0->notebook>=7.0.0->-r requirements.txt (line 39)) (0.23.1)\n",
            "Requirement already satisfied: pyzmq>=24 in /usr/local/lib/python3.12/dist-packages (from jupyter-server<3,>=2.4.0->notebook>=7.0.0->-r requirements.txt (line 39)) (26.2.1)\n",
            "Requirement already satisfied: send2trash>=1.8.2 in /usr/local/lib/python3.12/dist-packages (from jupyter-server<3,>=2.4.0->notebook>=7.0.0->-r requirements.txt (line 39)) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.12/dist-packages (from jupyter-server<3,>=2.4.0->notebook>=7.0.0->-r requirements.txt (line 39)) (0.18.1)\n",
            "Requirement already satisfied: websocket-client>=1.7 in /usr/local/lib/python3.12/dist-packages (from jupyter-server<3,>=2.4.0->notebook>=7.0.0->-r requirements.txt (line 39)) (1.9.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->-r requirements.txt (line 2)) (3.0.3)\n",
            "Collecting async-lru>=1.0.0 (from jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 37))\n",
            "  Downloading async_lru-2.0.5-py3-none-any.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: httpx<1,>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 37)) (0.28.1)\n",
            "Collecting jupyter-lsp>=2.0.0 (from jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 37))\n",
            "  Downloading jupyter_lsp-2.3.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.12/dist-packages (from ipykernel->jupyter>=1.0.0->-r requirements.txt (line 37)) (1.8.15)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.12/dist-packages (from ipykernel->jupyter>=1.0.0->-r requirements.txt (line 37)) (1.6.0)\n",
            "Requirement already satisfied: babel>=2.10 in /usr/local/lib/python3.12/dist-packages (from jupyterlab-server<3,>=2.28.0->notebook>=7.0.0->-r requirements.txt (line 39)) (2.17.0)\n",
            "Collecting json5>=0.9.0 (from jupyterlab-server<3,>=2.28.0->notebook>=7.0.0->-r requirements.txt (line 39))\n",
            "  Downloading json5-0.12.1-py3-none-any.whl.metadata (36 kB)\n",
            "Requirement already satisfied: jsonschema>=4.18.0 in /usr/local/lib/python3.12/dist-packages (from jupyterlab-server<3,>=2.28.0->notebook>=7.0.0->-r requirements.txt (line 39)) (4.25.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=13.0.0->-r requirements.txt (line 34)) (0.1.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from nbconvert->jupyter>=1.0.0->-r requirements.txt (line 37)) (4.13.5)\n",
            "Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.12/dist-packages (from bleach[css]!=5.0.0->nbconvert->jupyter>=1.0.0->-r requirements.txt (line 37)) (6.3.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.12/dist-packages (from nbconvert->jupyter>=1.0.0->-r requirements.txt (line 37)) (0.7.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.12/dist-packages (from nbconvert->jupyter>=1.0.0->-r requirements.txt (line 37)) (0.3.0)\n",
            "Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.12/dist-packages (from nbconvert->jupyter>=1.0.0->-r requirements.txt (line 37)) (3.1.4)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from nbconvert->jupyter>=1.0.0->-r requirements.txt (line 37)) (0.10.2)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from nbconvert->jupyter>=1.0.0->-r requirements.txt (line 37)) (1.5.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb>=0.15.0->-r requirements.txt (line 15)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb>=0.15.0->-r requirements.txt (line 15)) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb>=0.15.0->-r requirements.txt (line 15)) (0.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->-r requirements.txt (line 23)) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.35.0->-r requirements.txt (line 7)) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.35.0->-r requirements.txt (line 7)) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.35.0->-r requirements.txt (line 7)) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.35.0->-r requirements.txt (line 7)) (2025.10.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->-r requirements.txt (line 2)) (1.3.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.14.0->-r requirements.txt (line 8)) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.14.0->-r requirements.txt (line 8)) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.14.0->-r requirements.txt (line 8)) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.14.0->-r requirements.txt (line 8)) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.14.0->-r requirements.txt (line 8)) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.14.0->-r requirements.txt (line 8)) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.14.0->-r requirements.txt (line 8)) (1.22.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->notebook>=7.0.0->-r requirements.txt (line 39)) (1.3.1)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.12/dist-packages (from argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->notebook>=7.0.0->-r requirements.txt (line 39)) (25.1.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.12/dist-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert->jupyter>=1.0.0->-r requirements.txt (line 37)) (0.5.1)\n",
            "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from bleach[css]!=5.0.0->nbconvert->jupyter>=1.0.0->-r requirements.txt (line 37)) (1.4.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb>=0.15.0->-r requirements.txt (line 15)) (5.0.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.25.0->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 37)) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.25.0->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 37)) (0.16.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.12/dist-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets>=8.0.0->-r requirements.txt (line 38)) (0.8.5)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.28.0->notebook>=7.0.0->-r requirements.txt (line 39)) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.28.0->notebook>=7.0.0->-r requirements.txt (line 39)) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.28.0->notebook>=7.0.0->-r requirements.txt (line 39)) (0.28.0)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.12/dist-packages (from jupyter-client>=7.4.4->jupyter-server<3,>=2.4.0->notebook>=7.0.0->-r requirements.txt (line 39)) (0.4)\n",
            "Requirement already satisfied: python-json-logger>=2.0.4 in /usr/local/lib/python3.12/dist-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook>=7.0.0->-r requirements.txt (line 39)) (4.0.0)\n",
            "Requirement already satisfied: rfc3339-validator in /usr/local/lib/python3.12/dist-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook>=7.0.0->-r requirements.txt (line 39)) (0.1.4)\n",
            "Requirement already satisfied: rfc3986-validator>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook>=7.0.0->-r requirements.txt (line 39)) (0.1.1)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.12/dist-packages (from nbformat>=5.3.0->jupyter-server<3,>=2.4.0->notebook>=7.0.0->-r requirements.txt (line 39)) (2.21.2)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.12/dist-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets>=8.0.0->-r requirements.txt (line 38)) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets>=8.0.0->-r requirements.txt (line 38)) (0.2.14)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->nbconvert->jupyter>=1.0.0->-r requirements.txt (line 37)) (2.8)\n",
            "Requirement already satisfied: fqdn in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook>=7.0.0->-r requirements.txt (line 39)) (1.5.1)\n",
            "Requirement already satisfied: isoduration in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook>=7.0.0->-r requirements.txt (line 39)) (20.11.0)\n",
            "Requirement already satisfied: jsonpointer>1.13 in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook>=7.0.0->-r requirements.txt (line 39)) (3.0.0)\n",
            "Requirement already satisfied: rfc3987-syntax>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook>=7.0.0->-r requirements.txt (line 39)) (1.1.0)\n",
            "Requirement already satisfied: uri-template in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook>=7.0.0->-r requirements.txt (line 39)) (1.3.0)\n",
            "Requirement already satisfied: webcolors>=24.6.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook>=7.0.0->-r requirements.txt (line 39)) (25.10.0)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->notebook>=7.0.0->-r requirements.txt (line 39)) (2.0.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->notebook>=7.0.0->-r requirements.txt (line 39)) (2.23)\n",
            "Requirement already satisfied: lark>=1.2.2 in /usr/local/lib/python3.12/dist-packages (from rfc3987-syntax>=1.1.0->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook>=7.0.0->-r requirements.txt (line 39)) (1.3.1)\n",
            "Requirement already satisfied: arrow>=0.15.0 in /usr/local/lib/python3.12/dist-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook>=7.0.0->-r requirements.txt (line 39)) (1.4.0)\n",
            "Downloading evaluate-0.4.6-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading jupyter-1.1.1-py2.py3-none-any.whl (2.7 kB)\n",
            "Downloading ipywidgets-8.1.8-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.8/139.8 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading notebook-7.5.0-py3-none-any.whl (14.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.5/14.5 MB\u001b[0m \u001b[31m141.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytest_cov-7.0.0-py3-none-any.whl (22 kB)\n",
            "Downloading pytest_mock-3.15.1-py3-none-any.whl (10 kB)\n",
            "Downloading black-25.11.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m95.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading flake8-7.3.0-py2.py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.9/57.9 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading isort-7.0.0-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.7/94.7 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy-1.18.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (13.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m148.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl (59.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading comm-0.2.3-py3-none-any.whl (7.3 kB)\n",
            "Downloading coverage-7.12.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (252 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m252.3/252.3 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyterlab-4.5.0-py3-none-any.whl (12.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m142.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyterlab_server-2.28.0-py3-none-any.whl (59 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.8/59.8 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mccabe-0.7.0-py2.py3-none-any.whl (7.3 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Downloading pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
            "Downloading pycodestyle-2.14.0-py2.py3-none-any.whl (31 kB)\n",
            "Downloading pyflakes-3.4.0-py2.py3-none-any.whl (63 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.6/63.6 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytokens-0.3.0-py3-none-any.whl (12 kB)\n",
            "Downloading widgetsnbextension-4.0.15-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m106.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading async_lru-2.0.5-py3-none-any.whl (6.1 kB)\n",
            "Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m80.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading json5-0.12.1-py3-none-any.whl (36 kB)\n",
            "Downloading jupyter_lsp-2.3.0-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.7/76.7 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: widgetsnbextension, pytokens, pyflakes, pycodestyle, pathspec, mypy-extensions, mccabe, json5, jedi, isort, coverage, comm, colorama, async-lru, pytest-mock, mypy, flake8, black, pytest-cov, ipywidgets, bitsandbytes, evaluate, jupyterlab-server, jupyter-lsp, jupyterlab, notebook, jupyter\n",
            "  Attempting uninstall: widgetsnbextension\n",
            "    Found existing installation: widgetsnbextension 3.6.10\n",
            "    Uninstalling widgetsnbextension-3.6.10:\n",
            "      Successfully uninstalled widgetsnbextension-3.6.10\n",
            "  Attempting uninstall: ipywidgets\n",
            "    Found existing installation: ipywidgets 7.7.1\n",
            "    Uninstalling ipywidgets-7.7.1:\n",
            "      Successfully uninstalled ipywidgets-7.7.1\n",
            "  Attempting uninstall: notebook\n",
            "    Found existing installation: notebook 6.5.7\n",
            "    Uninstalling notebook-6.5.7:\n",
            "      Successfully uninstalled notebook-6.5.7\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "jupyter-kernel-gateway 2.5.2 requires notebook<7.0,>=5.7.6, but you have notebook 7.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed async-lru-2.0.5 bitsandbytes-0.48.2 black-25.11.0 colorama-0.4.6 comm-0.2.3 coverage-7.12.0 evaluate-0.4.6 flake8-7.3.0 ipywidgets-8.1.8 isort-7.0.0 jedi-0.19.2 json5-0.12.1 jupyter-1.1.1 jupyter-lsp-2.3.0 jupyterlab-4.5.0 jupyterlab-server-2.28.0 mccabe-0.7.0 mypy-1.18.2 mypy-extensions-1.1.0 notebook-7.5.0 pathspec-0.12.1 pycodestyle-2.14.0 pyflakes-3.4.0 pytest-cov-7.0.0 pytest-mock-3.15.1 pytokens-0.3.0 widgetsnbextension-4.0.15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "wandb.login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7HkwhpiHy-IP",
        "outputId": "46e19f8d-d1b2-4710-acc5-89c74cf232bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maryanpawar1515\u001b[0m (\u001b[33maryanpawar1515-mahindra-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fix the __init__.py file\n",
        "with open('/content/slm-lora-benchmark/src/utils/__init__.py', 'w') as f:\n",
        "    f.write('''\"\"\"Utility modules for configuration, logging, and reproducibility\"\"\"\n",
        "\n",
        "from .config_loader import load_config, merge_configs\n",
        "from .logging_utils import setup_logger, log_metrics\n",
        "from .reproducibility import set_seed, print_environment_info\n",
        "from .wandb_utils import WandbLogger\n",
        "from .checkpoint_utils import CheckpointManager\n",
        "from .memory import print_memory_stats, clear_memory\n",
        "\n",
        "__all__ = [\n",
        "    \"load_config\",\n",
        "    \"merge_configs\",\n",
        "    \"setup_logger\",\n",
        "    \"log_metrics\",\n",
        "    \"set_seed\",\n",
        "    \"print_environment_info\",\n",
        "    \"WandbLogger\",\n",
        "    \"CheckpointManager\",\n",
        "    \"print_memory_stats\",\n",
        "    \"clear_memory\",\n",
        "]\n",
        "''')\n",
        "\n",
        "print(\"Fixed!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0y47Mk1zzFxG",
        "outputId": "0fbc204b-3ad7-4827-8b1d-67b418097a62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fixed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fix the imports to match actual filenames\n",
        "with open('/content/slm-lora-benchmark/src/utils/__init__.py', 'w') as f:\n",
        "    f.write('''\"\"\"Utility modules for configuration, logging, and reproducibility\"\"\"\n",
        "\n",
        "from .config_loader import load_config, merge_configs\n",
        "from .logging_utils import setup_logger, log_metrics\n",
        "from .reproducibility import set_seed, print_environment_info\n",
        "from .wandb_logger import WandbLogger\n",
        "from .checkpoint import CheckpointManager\n",
        "from .memory import print_memory_stats, clear_memory\n",
        "\n",
        "__all__ = [\n",
        "    \"load_config\",\n",
        "    \"merge_configs\",\n",
        "    \"setup_logger\",\n",
        "    \"log_metrics\",\n",
        "    \"set_seed\",\n",
        "    \"print_environment_info\",\n",
        "    \"WandbLogger\",\n",
        "    \"CheckpointManager\",\n",
        "    \"print_memory_stats\",\n",
        "    \"clear_memory\",\n",
        "]\n",
        "''')\n",
        "\n",
        "print(\"Fixed imports!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "usr0P2b63iaV",
        "outputId": "54f90ea7-b778-4043-f954-a7ebdb5f1cb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fixed imports!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fix models __init__.py\n",
        "with open('/content/slm-lora-benchmark/src/models/__init__.py', 'w') as f:\n",
        "    f.write('''\"\"\"Model factory and LoRA adapter modules\"\"\"\n",
        "\n",
        "from .model_factory import ModelFactory, load_model_and_tokenizer\n",
        "from .lora_adapter import apply_lora, create_peft_config, create_lora_model\n",
        "from .model_utils import count_parameters, print_trainable_parameters\n",
        "\n",
        "__all__ = [\n",
        "    \"ModelFactory\",\n",
        "    \"load_model_and_tokenizer\",\n",
        "    \"apply_lora\",\n",
        "    \"create_peft_config\",\n",
        "    \"create_lora_model\",\n",
        "    \"count_parameters\",\n",
        "    \"print_trainable_parameters\",\n",
        "]\n",
        "''')\n",
        "\n",
        "print(\"Fixed models __init__.py!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKVRojXw3i7d",
        "outputId": "d8bc09a1-ea34-456e-e388-5184bc478ac1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fixed models __init__.py!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove TextPreprocessor from imports\n",
        "with open('/content/slm-lora-benchmark/src/data/__init__.py', 'w') as f:\n",
        "    f.write('''\"\"\"Data loading and preprocessing modules\"\"\"\n",
        "\n",
        "from .dataset_loader import DatasetLoader, load_dataset_from_config\n",
        "from .datamodule import DataModule, create_datamodule\n",
        "\n",
        "__all__ = [\n",
        "    \"DatasetLoader\",\n",
        "    \"load_dataset_from_config\",\n",
        "    \"DataModule\",\n",
        "    \"create_datamodule\",\n",
        "]\n",
        "''')\n",
        "\n",
        "print(\"Fixed data __init__.py - removed TextPreprocessor!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0Ahmj_S3nzd",
        "outputId": "7e55fcbc-6b31-416b-ddbc-4bf9dd398ea3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fixed data __init__.py - removed TextPreprocessor!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clear Python cache and restart\n",
        "import sys\n",
        "import importlib\n",
        "\n",
        "# Remove cached modules\n",
        "for module in list(sys.modules.keys()):\n",
        "    if module.startswith('src'):\n",
        "        del sys.modules[module]\n",
        "\n",
        "print(\"Cache cleared!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yuokpPqy3rUT",
        "outputId": "8d7440aa-80b7-47c4-f699-bfe100ba6de7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cache cleared!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-create collator.py\n",
        "with open('/content/slm-lora-benchmark/src/data/collator.py', 'w') as f:\n",
        "    f.write('''\"\"\"Data collator for language modeling\"\"\"\n",
        "\n",
        "import torch\n",
        "from typing import Dict, List, Any\n",
        "from transformers import PreTrainedTokenizer\n",
        "from dataclasses import dataclass\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class DataCollatorForLanguageModeling:\n",
        "    \"\"\"Data collator for causal language modeling\"\"\"\n",
        "\n",
        "    tokenizer: PreTrainedTokenizer\n",
        "    mlm: bool = False\n",
        "\n",
        "    def __call__(self, examples: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:\n",
        "        \"\"\"Collate batch of examples\"\"\"\n",
        "        batch = {\n",
        "            \"input_ids\": [],\n",
        "            \"attention_mask\": []\n",
        "        }\n",
        "\n",
        "        for example in examples:\n",
        "            batch[\"input_ids\"].append(example[\"input_ids\"])\n",
        "            if \"attention_mask\" in example:\n",
        "                batch[\"attention_mask\"].append(example[\"attention_mask\"])\n",
        "\n",
        "        batch[\"input_ids\"] = torch.tensor(batch[\"input_ids\"], dtype=torch.long)\n",
        "\n",
        "        if batch[\"attention_mask\"]:\n",
        "            batch[\"attention_mask\"] = torch.tensor(batch[\"attention_mask\"], dtype=torch.long)\n",
        "        else:\n",
        "            batch[\"attention_mask\"] = (batch[\"input_ids\"] != self.tokenizer.pad_token_id).long()\n",
        "\n",
        "        batch[\"labels\"] = batch[\"input_ids\"].clone()\n",
        "        batch[\"labels\"][batch[\"labels\"] == self.tokenizer.pad_token_id] = -100\n",
        "\n",
        "        return batch\n",
        "''')\n",
        "\n",
        "print(\"Created collator.py!\")\n",
        "\n",
        "# Verify it was created\n",
        "!ls -la /content/slm-lora-benchmark/src/data/collator.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_YuDC1Mv3v1k",
        "outputId": "f3e827c7-9be1-4271-c75d-862ec1842827"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created collator.py!\n",
            "-rw-r--r-- 1 root root 1237 Nov 21 10:04 /content/slm-lora-benchmark/src/data/collator.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/slm-lora-benchmark/scripts/train.py', 'w') as f:\n",
        "    f.write('''\"\"\"\n",
        "Main training script for SLM LoRA fine-tuning\n",
        "Run with: python scripts/train.py\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import argparse\n",
        "import torch\n",
        "\n",
        "# Add src to path\n",
        "sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n",
        "\n",
        "from src.utils.config_loader import load_config\n",
        "from src.utils.logging_utils import setup_logger\n",
        "from src.utils.reproducibility import set_seed, print_environment_info\n",
        "from src.utils.wandb_logger import WandbLogger\n",
        "from src.utils.checkpoint import CheckpointManager\n",
        "\n",
        "from src.models.model_factory import ModelFactory\n",
        "from src.models.lora_adapter import create_lora_model\n",
        "from src.models.model_utils import print_trainable_parameters\n",
        "\n",
        "from src.data.datamodule import create_datamodule\n",
        "\n",
        "from src.training.optimization import create_optimizer, create_scheduler\n",
        "from src.training.trainer import train_model\n",
        "\n",
        "\n",
        "def parse_args():\n",
        "    \"\"\"Parse command line arguments\"\"\"\n",
        "    parser = argparse.ArgumentParser(description=\"Train SLM with LoRA\")\n",
        "\n",
        "    parser.add_argument(\"--configs_dir\", type=str, default=\"configs\",\n",
        "                      help=\"Directory with config files\")\n",
        "    parser.add_argument(\"--output_dir\", type=str, default=\"outputs/runs\",\n",
        "                      help=\"Output directory\")\n",
        "    parser.add_argument(\"--dataset_names\", nargs=\"+\", default=None,\n",
        "                      help=\"Dataset names to use (default: all)\")\n",
        "    parser.add_argument(\"--wandb_project\", type=str, default=\"slm-lora-benchmark\",\n",
        "                      help=\"W&B project name\")\n",
        "    parser.add_argument(\"--wandb_name\", type=str, default=None,\n",
        "                      help=\"W&B run name\")\n",
        "    parser.add_argument(\"--seed\", type=int, default=42,\n",
        "                      help=\"Random seed\")\n",
        "\n",
        "    return parser.parse_args()\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main training function\"\"\"\n",
        "\n",
        "    # Parse arguments\n",
        "    args = parse_args()\n",
        "\n",
        "    # Setup logging\n",
        "    logger = setup_logger(\n",
        "        name=\"train\",\n",
        "        log_dir=os.path.join(args.output_dir, \"logs\"),\n",
        "        log_to_file=True\n",
        "    )\n",
        "\n",
        "    logger.info(\"=\" * 80)\n",
        "    logger.info(\"SLM LoRA TRAINING\")\n",
        "    logger.info(\"=\" * 80)\n",
        "\n",
        "    # Print environment info\n",
        "    print_environment_info()\n",
        "\n",
        "    # Set seed\n",
        "    set_seed(args.seed)\n",
        "\n",
        "    # Load configurations\n",
        "    logger.info(\"\\\\nLoading configurations...\")\n",
        "\n",
        "    datasets_config = load_config(os.path.join(args.configs_dir, \"datasets.yaml\"))\n",
        "    model_config = load_config(os.path.join(args.configs_dir, \"model_config.yaml\"))\n",
        "    lora_config = load_config(os.path.join(args.configs_dir, \"lora_config.yaml\"))\n",
        "    training_config = load_config(os.path.join(args.configs_dir, \"training_config.yaml\"))\n",
        "\n",
        "    # Combine configs\n",
        "    config = {\n",
        "        \"datasets\": datasets_config,\n",
        "        \"model\": model_config[\"model\"],\n",
        "        \"tokenizer\": model_config.get(\"tokenizer\", {}),\n",
        "        \"lora\": lora_config[\"lora\"],\n",
        "        \"training\": training_config[\"training\"],\n",
        "        \"wandb\": training_config.get(\"wandb\", {}),\n",
        "        \"output\": training_config.get(\"output\", {})\n",
        "    }\n",
        "\n",
        "    logger.info(\"Configurations loaded successfully\")\n",
        "\n",
        "    # Initialize W&B\n",
        "    logger.info(\"\\\\nInitializing Weights & Biases...\")\n",
        "\n",
        "    wandb_logger = WandbLogger(\n",
        "        project=args.wandb_project,\n",
        "        name=args.wandb_name,\n",
        "        config=config,\n",
        "        tags=config[\"wandb\"].get(\"tags\", [\"pythia-410m\", \"lora\"]),\n",
        "        notes=config[\"wandb\"].get(\"notes\", \"LoRA fine-tuning experiment\"),\n",
        "        group=config[\"wandb\"].get(\"group\", \"experiments\"),\n",
        "        job_type=\"train\",\n",
        "        mode=config[\"wandb\"].get(\"mode\", \"online\")\n",
        "    )\n",
        "\n",
        "    # Load model and tokenizer\n",
        "    logger.info(\"\\\\nLoading model and tokenizer...\")\n",
        "\n",
        "    model, tokenizer = ModelFactory.from_config(\n",
        "        model_config=config[\"model\"],\n",
        "        tokenizer_config=config.get(\"tokenizer\")\n",
        "    )\n",
        "\n",
        "    logger.info(f\"Model loaded: {config['model']['name']}\")\n",
        "    logger.info(f\"Tokenizer vocab size: {len(tokenizer)}\")\n",
        "\n",
        "    # Apply LoRA\n",
        "    logger.info(\"\\\\nApplying LoRA adapters...\")\n",
        "\n",
        "    model = create_lora_model(\n",
        "        model=model,\n",
        "        lora_config_dict=config[\"lora\"],\n",
        "        prepare_for_kbit=config[\"model\"].get(\"load_in_8bit\", False)\n",
        "    )\n",
        "\n",
        "    print_trainable_parameters(model)\n",
        "\n",
        "    # Setup data\n",
        "    logger.info(\"\\\\nSetting up datasets and dataloaders...\")\n",
        "\n",
        "    datamodule = create_datamodule(\n",
        "        config=config,\n",
        "        tokenizer=tokenizer,\n",
        "        dataset_names=args.dataset_names\n",
        "    )\n",
        "\n",
        "    datamodule.setup()\n",
        "\n",
        "    train_dataloader = datamodule.get_train_dataloader()\n",
        "    val_dataloader = datamodule.get_val_dataloader()\n",
        "\n",
        "    logger.info(f\"Training samples: {len(datamodule.train_dataset)}\")\n",
        "    logger.info(f\"Validation samples: {len(datamodule.val_dataset)}\")\n",
        "    logger.info(f\"Training batches: {len(train_dataloader)}\")\n",
        "\n",
        "    # Create optimizer and scheduler\n",
        "    logger.info(\"\\\\nCreating optimizer and scheduler...\")\n",
        "\n",
        "    optimizer = create_optimizer(model, config)\n",
        "\n",
        "    num_training_steps = len(train_dataloader) * config[\"training\"][\"num_epochs\"]\n",
        "    scheduler = create_scheduler(optimizer, num_training_steps, config)\n",
        "\n",
        "    logger.info(f\"Optimizer: {optimizer.__class__.__name__}\")\n",
        "    logger.info(f\"Scheduler: {config['training'].get('lr_scheduler_type', 'cosine')}\")\n",
        "    logger.info(f\"Total training steps: {num_training_steps}\")\n",
        "\n",
        "    # Setup checkpoint manager\n",
        "    logger.info(\"\\\\nSetting up checkpoint manager...\")\n",
        "\n",
        "    checkpoint_dir = os.path.join(args.output_dir, \"checkpoints\")\n",
        "    checkpoint_manager = CheckpointManager(\n",
        "        checkpoint_dir=checkpoint_dir,\n",
        "        max_checkpoints=config[\"training\"].get(\"save_total_limit\", 3),\n",
        "        metric_name=\"val_loss\",\n",
        "        mode=\"min\"\n",
        "    )\n",
        "\n",
        "    # Get device\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    logger.info(f\"\\\\nUsing device: {device}\")\n",
        "\n",
        "    # Train model\n",
        "    logger.info(\"\\\\n\" + \"=\" * 80)\n",
        "    logger.info(\"STARTING TRAINING\")\n",
        "    logger.info(\"=\" * 80 + \"\\\\n\")\n",
        "\n",
        "    results = train_model(\n",
        "        model=model,\n",
        "        train_dataloader=train_dataloader,\n",
        "        val_dataloader=val_dataloader,\n",
        "        optimizer=optimizer,\n",
        "        scheduler=scheduler,\n",
        "        config=config,\n",
        "        wandb_logger=wandb_logger,\n",
        "        checkpoint_manager=checkpoint_manager,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    # Log final results\n",
        "    logger.info(\"\\\\n\" + \"=\" * 80)\n",
        "    logger.info(\"TRAINING COMPLETED\")\n",
        "    logger.info(\"=\" * 80)\n",
        "    logger.info(f\"Best validation loss: {results['best_val_loss']:.4f}\")\n",
        "    logger.info(f\"Total training time: {results['total_time']/3600:.2f} hours\")\n",
        "    logger.info(f\"Best checkpoint: {checkpoint_manager.best_checkpoint_path}\")\n",
        "\n",
        "    # Finish W&B\n",
        "    wandb_logger.finish()\n",
        "\n",
        "    logger.info(\"\\\\nTraining script completed successfully!\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "''')\n",
        "\n",
        "print(\"Fixed train.py to match your repo structure!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30af5amU3yIp",
        "outputId": "d1b2d90b-aec1-4ce8-b5dc-3c448d76d23f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fixed train.py to match your repo structure!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fix training __init__.py to match your actual files\n",
        "with open('/content/slm-lora-benchmark/src/training/__init__.py', 'w') as f:\n",
        "    f.write('''\"\"\"Training modules\"\"\"\n",
        "\n",
        "from .trainer import Trainer, train_model\n",
        "from .loss import compute_token_loss, compute_sequence_loss, compute_perplexity\n",
        "from .optimization import create_optimizer, create_scheduler\n",
        "\n",
        "__all__ = [\n",
        "    \"Trainer\",\n",
        "    \"train_model\",\n",
        "    \"compute_token_loss\",\n",
        "    \"compute_sequence_loss\",\n",
        "    \"compute_perplexity\",\n",
        "    \"create_optimizer\",\n",
        "    \"create_scheduler\",\n",
        "]\n",
        "''')\n",
        "\n",
        "print(\"Fixed training __init__.py!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6l_lfErA34In",
        "outputId": "ae8350ae-645f-4164-b3bc-1b621a5e5e98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fixed training __init__.py!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Update datasets.yaml to remove openwebtext and pile_subset\n",
        "with open('/content/slm-lora-benchmark/configs/datasets.yaml', 'w') as f:\n",
        "    f.write('''# Dataset Configuration for SLM LoRA Benchmark\n",
        "# Datasets with deprecated scripts removed\n",
        "\n",
        "datasets:\n",
        "  # Core Language Modeling Datasets\n",
        "  wikitext_103:\n",
        "    name: \"Salesforce/wikitext\"\n",
        "    subset: \"wikitext-103-raw-v1\"\n",
        "    split: \"train\"\n",
        "    text_column: \"text\"\n",
        "    max_samples: 50000\n",
        "    description: \"Large-scale Wikipedia articles\"\n",
        "\n",
        "  wikitext_2:\n",
        "    name: \"Salesforce/wikitext\"\n",
        "    subset: \"wikitext-2-raw-v1\"\n",
        "    split: \"train\"\n",
        "    text_column: \"text\"\n",
        "    max_samples: 10000\n",
        "    description: \"Smaller Wikipedia corpus\"\n",
        "\n",
        "  tinystories:\n",
        "    name: \"roneneldan/TinyStories\"\n",
        "    subset: null\n",
        "    split: \"train\"\n",
        "    text_column: \"text\"\n",
        "    max_samples: 40000\n",
        "    description: \"Simple narrative stories\"\n",
        "\n",
        "  # News & Articles\n",
        "  ag_news:\n",
        "    name: \"ag_news\"\n",
        "    subset: null\n",
        "    split: \"train\"\n",
        "    text_column: \"text\"\n",
        "    max_samples: 60000\n",
        "    description: \"News articles classification\"\n",
        "\n",
        "  cnn_dailymail:\n",
        "    name: \"cnn_dailymail\"\n",
        "    subset: \"3.0.0\"\n",
        "    split: \"train\"\n",
        "    text_column: \"article\"\n",
        "    max_samples: 25000\n",
        "    description: \"CNN and Daily Mail news articles\"\n",
        "\n",
        "  xsum:\n",
        "    name: \"xsum\"\n",
        "    subset: null\n",
        "    split: \"train\"\n",
        "    text_column: \"document\"\n",
        "    max_samples: 50000\n",
        "    description: \"BBC news summaries\"\n",
        "\n",
        "  # Reviews\n",
        "  yelp_reviews:\n",
        "    name: \"yelp_review_full\"\n",
        "    subset: null\n",
        "    split: \"train\"\n",
        "    text_column: \"text\"\n",
        "    max_samples: 100000\n",
        "    description: \"Restaurant reviews\"\n",
        "\n",
        "  # Dialogue & QA\n",
        "  squad:\n",
        "    name: \"squad\"\n",
        "    subset: null\n",
        "    split: \"train\"\n",
        "    text_column: \"context\"\n",
        "    max_samples: 87599\n",
        "    description: \"Reading comprehension contexts\"\n",
        "\n",
        "# Preprocessing settings\n",
        "preprocessing:\n",
        "  max_length: 512\n",
        "  stride: 256\n",
        "  min_length: 32\n",
        "  remove_empty: true\n",
        "  lowercase: false\n",
        "  remove_special_chars: false\n",
        "\n",
        "# Tokenization settings\n",
        "tokenization:\n",
        "  padding: \"max_length\"\n",
        "  truncation: true\n",
        "  return_attention_mask: true\n",
        "  return_token_type_ids: false\n",
        "\n",
        "# Data loading settings\n",
        "dataloader:\n",
        "  batch_size: 8\n",
        "  num_workers: 2\n",
        "  pin_memory: true\n",
        "  shuffle: true\n",
        "  drop_last: true\n",
        "  prefetch_factor: 2\n",
        "\n",
        "# Validation split settings\n",
        "validation:\n",
        "  split_ratio: 0.1\n",
        "  seed: 42\n",
        "''')\n",
        "\n",
        "print(\"Updated datasets.yaml - removed openwebtext and pile_subset!\")\n",
        "print(\"\\nNow using 8 clean datasets:\")\n",
        "print(\"✅ wikitext_103, wikitext_2, tinystories\")\n",
        "print(\"✅ ag_news, cnn_dailymail, xsum\")\n",
        "print(\"✅ yelp_reviews, squad\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Givk6lc439MZ",
        "outputId": "abb5d33f-532b-4671-e7ca-c350880657cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated datasets.yaml - removed openwebtext and pile_subset!\n",
            "\n",
            "Now using 8 clean datasets:\n",
            "✅ wikitext_103, wikitext_2, tinystories\n",
            "✅ ag_news, cnn_dailymail, xsum\n",
            "✅ yelp_reviews, squad\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Update datasets.yaml with ONLY working datasets\n",
        "with open('/content/slm-lora-benchmark/configs/datasets.yaml', 'w') as f:\n",
        "    f.write('''# Dataset Configuration - Script-free datasets only\n",
        "\n",
        "datasets:\n",
        "  # Core Language Modeling Datasets\n",
        "  wikitext_103:\n",
        "    name: \"Salesforce/wikitext\"\n",
        "    subset: \"wikitext-103-raw-v1\"\n",
        "    split: \"train\"\n",
        "    text_column: \"text\"\n",
        "    max_samples: 50000\n",
        "    description: \"Large-scale Wikipedia articles\"\n",
        "\n",
        "  wikitext_2:\n",
        "    name: \"Salesforce/wikitext\"\n",
        "    subset: \"wikitext-2-raw-v1\"\n",
        "    split: \"train\"\n",
        "    text_column: \"text\"\n",
        "    max_samples: 10000\n",
        "    description: \"Smaller Wikipedia corpus\"\n",
        "\n",
        "  tinystories:\n",
        "    name: \"roneneldan/TinyStories\"\n",
        "    subset: null\n",
        "    split: \"train\"\n",
        "    text_column: \"text\"\n",
        "    max_samples: 40000\n",
        "    description: \"Simple narrative stories\"\n",
        "\n",
        "  # News & Articles\n",
        "  ag_news:\n",
        "    name: \"ag_news\"\n",
        "    subset: null\n",
        "    split: \"train\"\n",
        "    text_column: \"text\"\n",
        "    max_samples: 60000\n",
        "    description: \"News articles classification\"\n",
        "\n",
        "  cnn_dailymail:\n",
        "    name: \"cnn_dailymail\"\n",
        "    subset: \"3.0.0\"\n",
        "    split: \"train\"\n",
        "    text_column: \"article\"\n",
        "    max_samples: 25000\n",
        "    description: \"CNN and Daily Mail news articles\"\n",
        "\n",
        "  # Reviews\n",
        "  yelp_reviews:\n",
        "    name: \"yelp_review_full\"\n",
        "    subset: null\n",
        "    split: \"train\"\n",
        "    text_column: \"text\"\n",
        "    max_samples: 100000\n",
        "    description: \"Restaurant reviews\"\n",
        "\n",
        "  # QA\n",
        "  squad:\n",
        "    name: \"squad\"\n",
        "    subset: null\n",
        "    split: \"train\"\n",
        "    text_column: \"context\"\n",
        "    max_samples: 87599\n",
        "    description: \"Reading comprehension contexts\"\n",
        "\n",
        "# Preprocessing settings\n",
        "preprocessing:\n",
        "  max_length: 512\n",
        "  stride: 256\n",
        "  min_length: 32\n",
        "  remove_empty: true\n",
        "  lowercase: false\n",
        "  remove_special_chars: false\n",
        "\n",
        "# Tokenization settings\n",
        "tokenization:\n",
        "  padding: \"max_length\"\n",
        "  truncation: true\n",
        "  return_attention_mask: true\n",
        "  return_token_type_ids: false\n",
        "\n",
        "# Data loading settings\n",
        "dataloader:\n",
        "  batch_size: 8\n",
        "  num_workers: 2\n",
        "  pin_memory: true\n",
        "  shuffle: true\n",
        "  drop_last: true\n",
        "  prefetch_factor: 2\n",
        "\n",
        "# Validation split settings\n",
        "validation:\n",
        "  split_ratio: 0.1\n",
        "  seed: 42\n",
        "''')\n",
        "\n",
        "print(\"✅ Updated datasets.yaml with 7 WORKING datasets:\")\n",
        "print(\"   - wikitext_103, wikitext_2, tinystories\")\n",
        "print(\"   - ag_news, cnn_dailymail\")\n",
        "print(\"   - yelp_reviews, squad\")\n",
        "print(\"\\n🚀 These are all script-free and will work!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YkXDZdmn4Hb-",
        "outputId": "38a42a82-0f19-480a-bc3c-f0899fd035d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Updated datasets.yaml with 7 WORKING datasets:\n",
            "   - wikitext_103, wikitext_2, tinystories\n",
            "   - ag_news, cnn_dailymail\n",
            "   - yelp_reviews, squad\n",
            "\n",
            "🚀 These are all script-free and will work!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/slm-lora-benchmark/src/training/optimization.py', 'w') as f:\n",
        "    f.write('''\"\"\"Optimizer and scheduler creation\"\"\"\n",
        "\n",
        "import torch\n",
        "from torch.optim import AdamW, Adam\n",
        "from transformers import get_scheduler\n",
        "\n",
        "def create_optimizer(model, config):\n",
        "    \"\"\"Create optimizer from config\"\"\"\n",
        "    training_config = config.get(\"training\", {})\n",
        "\n",
        "    optimizer_name = training_config.get(\"optim\", \"adamw_torch\")\n",
        "    lr = float(training_config.get(\"learning_rate\", 1e-4))  # Convert to float\n",
        "    weight_decay = float(training_config.get(\"weight_decay\", 0.01))  # Convert to float\n",
        "\n",
        "    if optimizer_name == \"adamw_torch\":\n",
        "        optimizer = AdamW(\n",
        "            model.parameters(),\n",
        "            lr=lr,\n",
        "            weight_decay=weight_decay,\n",
        "            betas=(float(training_config.get(\"adam_beta1\", 0.9)),\n",
        "                   float(training_config.get(\"adam_beta2\", 0.999))),\n",
        "            eps=float(training_config.get(\"adam_epsilon\", 1e-8))with open('/content/slm-lora-benchmark/src/training/trainer.py', 'w') as f:\n",
        "    f.write('''\"\"\"Main training loop with W&B logging\"\"\"\n",
        "\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from typing import Dict, Any, Optional\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "\n",
        "from src.utils.wandb_logger import WandbLogger\n",
        "from src.utils.checkpoint import CheckpointManager\n",
        "from src.utils.memory import print_memory_stats\n",
        "\n",
        "\n",
        "class Trainer:\n",
        "    \"\"\"Trainer with comprehensive W&B logging\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: nn.Module,\n",
        "        train_dataloader: DataLoader,\n",
        "        val_dataloader: DataLoader,\n",
        "        optimizer: torch.optim.Optimizer,\n",
        "        scheduler: Optional[Any],\n",
        "        config: Dict[str, Any],\n",
        "        wandb_logger: WandbLogger,\n",
        "        checkpoint_manager: CheckpointManager,\n",
        "        device: str = \"cuda\"\n",
        "    ):\n",
        "        self.model = model\n",
        "        self.train_dataloader = train_dataloader\n",
        "        self.val_dataloader = val_dataloader\n",
        "        self.optimizer = optimizer\n",
        "        self.scheduler = scheduler\n",
        "        self.config = config\n",
        "        self.wandb_logger = wandb_logger\n",
        "        self.checkpoint_manager = checkpoint_manager\n",
        "        self.device = device\n",
        "\n",
        "        self.global_step = 0\n",
        "        self.current_epoch = 0\n",
        "        self.best_val_loss = float('inf')\n",
        "\n",
        "        # Training config\n",
        "        training_config = config.get(\"training\", {})\n",
        "        self.num_epochs = training_config.get(\"num_epochs\", 3)\n",
        "        self.gradient_accumulation_steps = training_config.get(\"gradient_accumulation_steps\", 4)\n",
        "        self.max_grad_norm = training_config.get(\"max_grad_norm\", 1.0)\n",
        "        self.logging_steps = training_config.get(\"logging_steps\", 10)\n",
        "        self.eval_steps = training_config.get(\"eval_steps\", 500)\n",
        "        self.save_steps = training_config.get(\"save_steps\", 1000)\n",
        "\n",
        "        # EMA loss tracking\n",
        "        self.ema_loss = None\n",
        "        self.ema_alpha = 0.9\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"Main training loop\"\"\"\n",
        "        print(\"Starting training...\")\n",
        "        start_time = time.time()\n",
        "\n",
        "        for epoch in range(self.num_epochs):\n",
        "            self.current_epoch = epoch\n",
        "            print(f\"\\\\n{'='*80}\")\n",
        "            print(f\"Epoch {epoch + 1}/{self.num_epochs}\")\n",
        "            print(f\"{'='*80}\")\n",
        "\n",
        "            # Train epoch\n",
        "            train_metrics = self.train_epoch()\n",
        "\n",
        "            # Validate\n",
        "            val_metrics = self.validate()\n",
        "\n",
        "            # Log epoch summary\n",
        "            self.log_epoch_summary(epoch, train_metrics, val_metrics)\n",
        "\n",
        "            # Save checkpoint\n",
        "            is_best = val_metrics['val_loss'] < self.best_val_loss\n",
        "            if is_best:\n",
        "                self.best_val_loss = val_metrics['val_loss']\n",
        "\n",
        "            self.checkpoint_manager.save_checkpoint(\n",
        "                model=self.model,\n",
        "                optimizer=self.optimizer,\n",
        "                epoch=epoch,\n",
        "                step=self.global_step,\n",
        "                metrics=val_metrics,\n",
        "                is_best=is_best\n",
        "            )\n",
        "\n",
        "        total_time = time.time() - start_time\n",
        "        print(f\"\\\\nTraining completed in {total_time/3600:.2f} hours\")\n",
        "\n",
        "        return {\n",
        "            'best_val_loss': self.best_val_loss,\n",
        "            'total_time': total_time\n",
        "        }\n",
        "\n",
        "    def train_epoch(self):\n",
        "        \"\"\"Train single epoch\"\"\"\n",
        "        self.model.train()\n",
        "\n",
        "        total_loss = 0\n",
        "        total_tokens = 0\n",
        "        samples_trained = 0\n",
        "        epoch_start_time = time.time()\n",
        "\n",
        "        progress_bar = tqdm(self.train_dataloader, desc=f\"Training Epoch {self.current_epoch + 1}\")\n",
        "\n",
        "        for batch_idx, batch in enumerate(progress_bar):\n",
        "            step_start_time = time.time()\n",
        "\n",
        "            # Forward pass\n",
        "            loss, metrics = self.training_step(batch)\n",
        "\n",
        "            # Backward pass\n",
        "            loss = loss / self.gradient_accumulation_steps\n",
        "            loss.backward()\n",
        "\n",
        "            # Update weights\n",
        "            if (batch_idx + 1) % self.gradient_accumulation_steps == 0:\n",
        "                # Gradient clipping\n",
        "                grad_norm = torch.nn.utils.clip_grad_norm_(\n",
        "                    self.model.parameters(),\n",
        "                    self.max_grad_norm\n",
        "                )\n",
        "\n",
        "                self.optimizer.step()\n",
        "                self.optimizer.zero_grad()\n",
        "\n",
        "                if self.scheduler:\n",
        "                    self.scheduler.step()\n",
        "\n",
        "                self.global_step += 1\n",
        "\n",
        "                # Calculate step metrics\n",
        "                step_time = time.time() - step_start_time\n",
        "                tokens_per_sec = metrics['n_tokens'] / step_time\n",
        "\n",
        "                # Update EMA loss\n",
        "                if self.ema_loss is None:\n",
        "                    self.ema_loss = metrics['token_loss']\n",
        "                else:\n",
        "                    self.ema_loss = self.ema_alpha * self.ema_loss + (1 - self.ema_alpha) * metrics['token_loss']\n",
        "\n",
        "                # Log to W&B\n",
        "                if self.global_step % self.logging_steps == 0:\n",
        "                    self.log_training_metrics(metrics, grad_norm, tokens_per_sec, step_time)\n",
        "\n",
        "                # Evaluate\n",
        "                if self.global_step % self.eval_steps == 0:\n",
        "                    val_metrics = self.validate()\n",
        "                    self.model.train()  # Back to training mode\n",
        "\n",
        "                # Save checkpoint\n",
        "                if self.global_step % self.save_steps == 0:\n",
        "                    self.checkpoint_manager.save_checkpoint(\n",
        "                        model=self.model,\n",
        "                        optimizer=self.optimizer,\n",
        "                        epoch=self.current_epoch,\n",
        "                        step=self.global_step,\n",
        "                        metrics=metrics\n",
        "                    )\n",
        "\n",
        "            total_loss += metrics['token_loss'] * metrics['n_tokens']\n",
        "            total_tokens += metrics['n_tokens']\n",
        "\n",
        "            # Track samples\n",
        "            batch_size = batch['input_ids'].size(0)\n",
        "            samples_trained += batch_size\n",
        "\n",
        "            # Update progress bar\n",
        "            progress_bar.set_postfix({\n",
        "                'loss': f\"{metrics['token_loss']:.4f}\",\n",
        "                'ppl': f\"{metrics['ppl']:.2f}\",\n",
        "                'lr': f\"{self.get_lr():.2e}\",\n",
        "                'samples': samples_trained\n",
        "            })\n",
        "\n",
        "        avg_loss = total_loss / total_tokens if total_tokens > 0 else 0\n",
        "        epoch_time = time.time() - epoch_start_time\n",
        "\n",
        "        return {\n",
        "            'train_loss': avg_loss,\n",
        "            'train_ppl': math.exp(avg_loss) if avg_loss < 100 else float('inf'),\n",
        "            'epoch_time': epoch_time,\n",
        "            'samples_trained': samples_trained\n",
        "        }\n",
        "\n",
        "    def training_step(self, batch):\n",
        "        \"\"\"Single training step\"\"\"\n",
        "        # Move batch to device\n",
        "        batch = {k: v.to(self.device) if isinstance(v, torch.Tensor) else v\n",
        "                for k, v in batch.items()}\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = self.model(**batch)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        # Calculate metrics\n",
        "        n_tokens = (batch['labels'] != -100).sum().item()\n",
        "        token_loss = loss.item()\n",
        "        ppl = math.exp(token_loss) if token_loss < 100 else float('inf')\n",
        "        bpt = token_loss / math.log(2)  # Bits per token\n",
        "\n",
        "        metrics = {\n",
        "            'token_loss': token_loss,\n",
        "            'ppl': ppl,\n",
        "            'bpt': bpt,\n",
        "            'n_tokens': n_tokens\n",
        "        }\n",
        "\n",
        "        return loss, metrics\n",
        "\n",
        "    def validate(self):\n",
        "        \"\"\"Validation loop\"\"\"\n",
        "        self.model.eval()\n",
        "\n",
        "        total_loss = 0\n",
        "        total_tokens = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(self.val_dataloader, desc=\"Validating\"):\n",
        "                batch = {k: v.to(self.device) if isinstance(v, torch.Tensor) else v\n",
        "                        for k, v in batch.items()}\n",
        "\n",
        "                outputs = self.model(**batch)\n",
        "                loss = outputs.loss\n",
        "\n",
        "                n_tokens = (batch['labels'] != -100).sum().item()\n",
        "                total_loss += loss.item() * n_tokens\n",
        "                total_tokens += n_tokens\n",
        "\n",
        "        avg_loss = total_loss / total_tokens if total_tokens > 0 else 0\n",
        "        ppl = math.exp(avg_loss) if avg_loss < 100 else float('inf')\n",
        "        bpt = avg_loss / math.log(2)\n",
        "\n",
        "        val_metrics = {\n",
        "            'val_loss': avg_loss,\n",
        "            'val_ppl': ppl,\n",
        "            'val_bpt': bpt,\n",
        "            'val_n_tokens': total_tokens\n",
        "        }\n",
        "\n",
        "        # Log to W&B\n",
        "        self.wandb_logger.log({\n",
        "            'val/token_loss': avg_loss,\n",
        "            'val/ppl': ppl,\n",
        "            'val/bpt': bpt,\n",
        "            'val/n_tokens': total_tokens\n",
        "        }, step=self.global_step)\n",
        "\n",
        "        print(f\"\\\\nValidation - Loss: {avg_loss:.4f}, PPL: {ppl:.2f}, BPT: {bpt:.4f}\")\n",
        "\n",
        "        return val_metrics\n",
        "\n",
        "    def log_training_metrics(self, metrics, grad_norm, tokens_per_sec, step_time):\n",
        "        \"\"\"Log training metrics to W&B\"\"\"\n",
        "        log_dict = {\n",
        "            'train/step': self.global_step,\n",
        "            'train/epoch': self.current_epoch,\n",
        "            'train/token_loss': metrics['token_loss'],\n",
        "            'train/ppl': metrics['ppl'],\n",
        "            'train/bpt': metrics['bpt'],\n",
        "            'train/lr': self.get_lr(),\n",
        "            'train/grad_norm': grad_norm.item() if isinstance(grad_norm, torch.Tensor) else grad_norm,\n",
        "            'train/throughput_tokens_per_sec': tokens_per_sec,\n",
        "            'train/step_time': step_time,\n",
        "            'train/ema_token_loss': self.ema_loss,\n",
        "        }\n",
        "\n",
        "        # Add LoRA rank if available\n",
        "        if hasattr(self.model, 'peft_config'):\n",
        "            log_dict['train/lora_rank'] = list(self.model.peft_config.values())[0].r\n",
        "\n",
        "        self.wandb_logger.log(log_dict, step=self.global_step)\n",
        "\n",
        "    def log_epoch_summary(self, epoch, train_metrics, val_metrics):\n",
        "        \"\"\"Log epoch summary\"\"\"\n",
        "        summary = {\n",
        "            'epoch/train_loss': train_metrics['train_loss'],\n",
        "            'epoch/train_ppl': train_metrics['train_ppl'],\n",
        "            'epoch/val_loss': val_metrics['val_loss'],\n",
        "            'epoch/val_ppl': val_metrics['val_ppl'],\n",
        "            'epoch/time': train_metrics['epoch_time'],\n",
        "            'epoch/samples_trained': train_metrics['samples_trained']\n",
        "        }\n",
        "\n",
        "        self.wandb_logger.log(summary, step=self.global_step)\n",
        "\n",
        "    def get_lr(self):\n",
        "        \"\"\"Get current learning rate\"\"\"\n",
        "        return self.optimizer.param_groups[0]['lr']\n",
        "\n",
        "\n",
        "def train_model(\n",
        "    model,\n",
        "    train_dataloader,\n",
        "    val_dataloader,\n",
        "    optimizer,\n",
        "    scheduler,\n",
        "    config,\n",
        "    wandb_logger,\n",
        "    checkpoint_manager,\n",
        "    device=\"cuda\"\n",
        "):\n",
        "    \"\"\"Convenience function for training\"\"\"\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        train_dataloader=train_dataloader,\n",
        "        val_dataloader=val_dataloader,\n",
        "        optimizer=optimizer,\n",
        "        scheduler=scheduler,\n",
        "        config=config,\n",
        "        wandb_logger=wandb_logger,\n",
        "        checkpoint_manager=checkpoint_manager,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    return trainer.train()\n",
        "''')\n",
        "\n",
        "print(\"✅ Fixed trainer.py - removed 600 sample limit!\")\n",
        "        )\n",
        "    else:\n",
        "        optimizer = AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "    return optimizer\n",
        "\n",
        "def create_scheduler(optimizer, num_training_steps, config):\n",
        "    \"\"\"Create learning rate scheduler\"\"\"\n",
        "    training_config = config.get(\"training\", {})\n",
        "\n",
        "    scheduler_type = training_config.get(\"lr_scheduler_type\", \"cosine\")\n",
        "    warmup_steps = int(training_config.get(\"warmup_steps\", 100))\n",
        "\n",
        "    scheduler = get_scheduler(\n",
        "        scheduler_type,\n",
        "        optimizer=optimizer,\n",
        "        num_warmup_steps=warmup_steps,\n",
        "        num_training_steps=num_training_steps\n",
        "    )\n",
        "\n",
        "    return scheduler\n",
        "''')\n",
        "\n",
        "print(\"Fixed optimization.py - added float() conversions!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "XTdvjWik4KEw",
        "outputId": "3268daf4-248d-4a39-9b7b-4d4bc9f2169e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (ipython-input-2327952468.py, line 26)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-2327952468.py\"\u001b[0;36m, line \u001b[0;32m26\u001b[0m\n\u001b[0;31m    import time\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/slm-lora-benchmark/src/training/trainer.py', 'w') as f:\n",
        "    f.write('''\"\"\"Main training loop with W&B logging\"\"\"\n",
        "\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from typing import Dict, Any, Optional\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "\n",
        "from src.utils.wandb_logger import WandbLogger\n",
        "from src.utils.checkpoint import CheckpointManager\n",
        "from src.utils.memory import print_memory_stats\n",
        "\n",
        "\n",
        "class Trainer:\n",
        "    \"\"\"Trainer with comprehensive W&B logging\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: nn.Module,\n",
        "        train_dataloader: DataLoader,\n",
        "        val_dataloader: DataLoader,\n",
        "        optimizer: torch.optim.Optimizer,\n",
        "        scheduler: Optional[Any],\n",
        "        config: Dict[str, Any],\n",
        "        wandb_logger: WandbLogger,\n",
        "        checkpoint_manager: CheckpointManager,\n",
        "        device: str = \"cuda\"\n",
        "    ):\n",
        "        self.model = model\n",
        "        self.train_dataloader = train_dataloader\n",
        "        self.val_dataloader = val_dataloader\n",
        "        self.optimizer = optimizer\n",
        "        self.scheduler = scheduler\n",
        "        self.config = config\n",
        "        self.wandb_logger = wandb_logger\n",
        "        self.checkpoint_manager = checkpoint_manager\n",
        "        self.device = device\n",
        "\n",
        "        self.global_step = 0\n",
        "        self.current_epoch = 0\n",
        "        self.best_val_loss = float('inf')\n",
        "\n",
        "        # Training config\n",
        "        training_config = config.get(\"training\", {})\n",
        "        self.num_epochs = training_config.get(\"num_epochs\", 3)\n",
        "        self.gradient_accumulation_steps = training_config.get(\"gradient_accumulation_steps\", 4)\n",
        "        self.max_grad_norm = training_config.get(\"max_grad_norm\", 1.0)\n",
        "        self.logging_steps = training_config.get(\"logging_steps\", 10)\n",
        "        self.eval_steps = training_config.get(\"eval_steps\", 500)\n",
        "        self.save_steps = training_config.get(\"save_steps\", 1000)\n",
        "\n",
        "        # EMA loss tracking\n",
        "        self.ema_loss = None\n",
        "        self.ema_alpha = 0.9\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"Main training loop\"\"\"\n",
        "        print(\"Starting training...\")\n",
        "        start_time = time.time()\n",
        "\n",
        "        for epoch in range(self.num_epochs):\n",
        "            self.current_epoch = epoch\n",
        "            print(f\"\\\\n{'='*80}\")\n",
        "            print(f\"Epoch {epoch + 1}/{self.num_epochs}\")\n",
        "            print(f\"{'='*80}\")\n",
        "\n",
        "            # Train epoch\n",
        "            train_metrics = self.train_epoch()\n",
        "\n",
        "            # Validate\n",
        "            val_metrics = self.validate()\n",
        "\n",
        "            # Log epoch summary\n",
        "            self.log_epoch_summary(epoch, train_metrics, val_metrics)\n",
        "\n",
        "            # Save checkpoint\n",
        "            is_best = val_metrics['val_loss'] < self.best_val_loss\n",
        "            if is_best:\n",
        "                self.best_val_loss = val_metrics['val_loss']\n",
        "\n",
        "            self.checkpoint_manager.save_checkpoint(\n",
        "                model=self.model,\n",
        "                optimizer=self.optimizer,\n",
        "                epoch=epoch,\n",
        "                step=self.global_step,\n",
        "                metrics=val_metrics,\n",
        "                is_best=is_best\n",
        "            )\n",
        "\n",
        "        total_time = time.time() - start_time\n",
        "        print(f\"\\\\nTraining completed in {total_time/3600:.2f} hours\")\n",
        "\n",
        "        return {\n",
        "            'best_val_loss': self.best_val_loss,\n",
        "            'total_time': total_time\n",
        "        }\n",
        "\n",
        "    def train_epoch(self):\n",
        "        \"\"\"Train single epoch\"\"\"\n",
        "        self.model.train()\n",
        "\n",
        "        total_loss = 0\n",
        "        total_tokens = 0\n",
        "        samples_trained = 0\n",
        "        epoch_start_time = time.time()\n",
        "\n",
        "        progress_bar = tqdm(self.train_dataloader, desc=f\"Training Epoch {self.current_epoch + 1}\")\n",
        "\n",
        "        for batch_idx, batch in enumerate(progress_bar):\n",
        "            step_start_time = time.time()\n",
        "\n",
        "            # Forward pass\n",
        "            loss, metrics = self.training_step(batch)\n",
        "\n",
        "            # Backward pass\n",
        "            loss = loss / self.gradient_accumulation_steps\n",
        "            loss.backward()\n",
        "\n",
        "            # Update weights\n",
        "            if (batch_idx + 1) % self.gradient_accumulation_steps == 0:\n",
        "                # Gradient clipping\n",
        "                grad_norm = torch.nn.utils.clip_grad_norm_(\n",
        "                    self.model.parameters(),\n",
        "                    self.max_grad_norm\n",
        "                )\n",
        "\n",
        "                self.optimizer.step()\n",
        "                self.optimizer.zero_grad()\n",
        "\n",
        "                if self.scheduler:\n",
        "                    self.scheduler.step()\n",
        "\n",
        "                self.global_step += 1\n",
        "\n",
        "                # Calculate step metrics\n",
        "                step_time = time.time() - step_start_time\n",
        "                tokens_per_sec = metrics['n_tokens'] / step_time\n",
        "\n",
        "                # Update EMA loss\n",
        "                if self.ema_loss is None:\n",
        "                    self.ema_loss = metrics['token_loss']\n",
        "                else:\n",
        "                    self.ema_loss = self.ema_alpha * self.ema_loss + (1 - self.ema_alpha) * metrics['token_loss']\n",
        "\n",
        "                # Log to W&B\n",
        "                if self.global_step % self.logging_steps == 0:\n",
        "                    self.log_training_metrics(metrics, grad_norm, tokens_per_sec, step_time)\n",
        "\n",
        "                # Evaluate\n",
        "                if self.global_step % self.eval_steps == 0:\n",
        "                    val_metrics = self.validate()\n",
        "                    self.model.train()  # Back to training mode\n",
        "\n",
        "                # Save checkpoint\n",
        "                if self.global_step % self.save_steps == 0:\n",
        "                    self.checkpoint_manager.save_checkpoint(\n",
        "                        model=self.model,\n",
        "                        optimizer=self.optimizer,\n",
        "                        epoch=self.current_epoch,\n",
        "                        step=self.global_step,\n",
        "                        metrics=metrics\n",
        "                    )\n",
        "\n",
        "            total_loss += metrics['token_loss'] * metrics['n_tokens']\n",
        "            total_tokens += metrics['n_tokens']\n",
        "\n",
        "            # Track samples\n",
        "            batch_size = batch['input_ids'].size(0)\n",
        "            samples_trained += batch_size\n",
        "\n",
        "            # Update progress bar\n",
        "            progress_bar.set_postfix({\n",
        "                'loss': f\"{metrics['token_loss']:.4f}\",\n",
        "                'ppl': f\"{metrics['ppl']:.2f}\",\n",
        "                'lr': f\"{self.get_lr():.2e}\",\n",
        "                'samples': samples_trained\n",
        "            })\n",
        "\n",
        "        avg_loss = total_loss / total_tokens if total_tokens > 0 else 0\n",
        "        epoch_time = time.time() - epoch_start_time\n",
        "\n",
        "        return {\n",
        "            'train_loss': avg_loss,\n",
        "            'train_ppl': math.exp(avg_loss) if avg_loss < 100 else float('inf'),\n",
        "            'epoch_time': epoch_time,\n",
        "            'samples_trained': samples_trained\n",
        "        }\n",
        "\n",
        "    def training_step(self, batch):\n",
        "        \"\"\"Single training step\"\"\"\n",
        "        # Move batch to device\n",
        "        batch = {k: v.to(self.device) if isinstance(v, torch.Tensor) else v\n",
        "                for k, v in batch.items()}\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = self.model(**batch)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        # Calculate metrics\n",
        "        n_tokens = (batch['labels'] != -100).sum().item()\n",
        "        token_loss = loss.item()\n",
        "        ppl = math.exp(token_loss) if token_loss < 100 else float('inf')\n",
        "        bpt = token_loss / math.log(2)  # Bits per token\n",
        "\n",
        "        metrics = {\n",
        "            'token_loss': token_loss,\n",
        "            'ppl': ppl,\n",
        "            'bpt': bpt,\n",
        "            'n_tokens': n_tokens\n",
        "        }\n",
        "\n",
        "        return loss, metrics\n",
        "\n",
        "    def validate(self):\n",
        "        \"\"\"Validation loop\"\"\"\n",
        "        self.model.eval()\n",
        "\n",
        "        total_loss = 0\n",
        "        total_tokens = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(self.val_dataloader, desc=\"Validating\"):\n",
        "                batch = {k: v.to(self.device) if isinstance(v, torch.Tensor) else v\n",
        "                        for k, v in batch.items()}\n",
        "\n",
        "                outputs = self.model(**batch)\n",
        "                loss = outputs.loss\n",
        "\n",
        "                n_tokens = (batch['labels'] != -100).sum().item()\n",
        "                total_loss += loss.item() * n_tokens\n",
        "                total_tokens += n_tokens\n",
        "\n",
        "        avg_loss = total_loss / total_tokens if total_tokens > 0 else 0\n",
        "        ppl = math.exp(avg_loss) if avg_loss < 100 else float('inf')\n",
        "        bpt = avg_loss / math.log(2)\n",
        "\n",
        "        val_metrics = {\n",
        "            'val_loss': avg_loss,\n",
        "            'val_ppl': ppl,\n",
        "            'val_bpt': bpt,\n",
        "            'val_n_tokens': total_tokens\n",
        "        }\n",
        "\n",
        "        # Log to W&B\n",
        "        self.wandb_logger.log({\n",
        "            'val/token_loss': avg_loss,\n",
        "            'val/ppl': ppl,\n",
        "            'val/bpt': bpt,\n",
        "            'val/n_tokens': total_tokens\n",
        "        }, step=self.global_step)\n",
        "\n",
        "        print(f\"\\\\nValidation - Loss: {avg_loss:.4f}, PPL: {ppl:.2f}, BPT: {bpt:.4f}\")\n",
        "\n",
        "        return val_metrics\n",
        "\n",
        "    def log_training_metrics(self, metrics, grad_norm, tokens_per_sec, step_time):\n",
        "        \"\"\"Log training metrics to W&B\"\"\"\n",
        "        log_dict = {\n",
        "            'train/step': self.global_step,\n",
        "            'train/epoch': self.current_epoch,\n",
        "            'train/token_loss': metrics['token_loss'],\n",
        "            'train/ppl': metrics['ppl'],\n",
        "            'train/bpt': metrics['bpt'],\n",
        "            'train/lr': self.get_lr(),\n",
        "            'train/grad_norm': grad_norm.item() if isinstance(grad_norm, torch.Tensor) else grad_norm,\n",
        "            'train/throughput_tokens_per_sec': tokens_per_sec,\n",
        "            'train/step_time': step_time,\n",
        "            'train/ema_token_loss': self.ema_loss,\n",
        "        }\n",
        "\n",
        "        # Add LoRA rank if available\n",
        "        if hasattr(self.model, 'peft_config'):\n",
        "            log_dict['train/lora_rank'] = list(self.model.peft_config.values())[0].r\n",
        "\n",
        "        self.wandb_logger.log(log_dict, step=self.global_step)\n",
        "\n",
        "    def log_epoch_summary(self, epoch, train_metrics, val_metrics):\n",
        "        \"\"\"Log epoch summary\"\"\"\n",
        "        summary = {\n",
        "            'epoch/train_loss': train_metrics['train_loss'],\n",
        "            'epoch/train_ppl': train_metrics['train_ppl'],\n",
        "            'epoch/val_loss': val_metrics['val_loss'],\n",
        "            'epoch/val_ppl': val_metrics['val_ppl'],\n",
        "            'epoch/time': train_metrics['epoch_time'],\n",
        "            'epoch/samples_trained': train_metrics['samples_trained']\n",
        "        }\n",
        "\n",
        "        self.wandb_logger.log(summary, step=self.global_step)\n",
        "\n",
        "    def get_lr(self):\n",
        "        \"\"\"Get current learning rate\"\"\"\n",
        "        return self.optimizer.param_groups[0]['lr']\n",
        "\n",
        "\n",
        "def train_model(\n",
        "    model,\n",
        "    train_dataloader,\n",
        "    val_dataloader,\n",
        "    optimizer,\n",
        "    scheduler,\n",
        "    config,\n",
        "    wandb_logger,\n",
        "    checkpoint_manager,\n",
        "    device=\"cuda\"\n",
        "):\n",
        "    \"\"\"Convenience function for training\"\"\"\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        train_dataloader=train_dataloader,\n",
        "        val_dataloader=val_dataloader,\n",
        "        optimizer=optimizer,\n",
        "        scheduler=scheduler,\n",
        "        config=config,\n",
        "        wandb_logger=wandb_logger,\n",
        "        checkpoint_manager=checkpoint_manager,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    return trainer.train()\n",
        "''')\n",
        "\n",
        "print(\"✅ Fixed trainer.py - removed 600 sample limit!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "atVXchdu4Kor",
        "outputId": "c4de6684-7255-4bce-d71a-4417ed1393af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Fixed trainer.py - removed 600 sample limit!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/slm-lora-benchmark/configs/datasets.yaml', 'w') as f:\n",
        "    f.write('''# Dataset Configuration - Reduced for faster training\n",
        "\n",
        "datasets:\n",
        "  # Core Language Modeling Datasets\n",
        "  wikitext_103:\n",
        "    name: \"Salesforce/wikitext\"\n",
        "    subset: \"wikitext-103-raw-v1\"\n",
        "    split: \"train\"\n",
        "    text_column: \"text\"\n",
        "    max_samples: 2000\n",
        "    description: \"Large-scale Wikipedia articles\"\n",
        "\n",
        "  wikitext_2:\n",
        "    name: \"Salesforce/wikitext\"\n",
        "    subset: \"wikitext-2-raw-v1\"\n",
        "    split: \"train\"\n",
        "    text_column: \"text\"\n",
        "    max_samples: 1000\n",
        "    description: \"Smaller Wikipedia corpus\"\n",
        "\n",
        "  tinystories:\n",
        "    name: \"roneneldan/TinyStories\"\n",
        "    subset: null\n",
        "    split: \"train\"\n",
        "    text_column: \"text\"\n",
        "    max_samples: 2000\n",
        "    description: \"Simple narrative stories\"\n",
        "\n",
        "  # News & Articles\n",
        "  ag_news:\n",
        "    name: \"ag_news\"\n",
        "    subset: null\n",
        "    split: \"train\"\n",
        "    text_column: \"text\"\n",
        "    max_samples: 2000\n",
        "    description: \"News articles classification\"\n",
        "\n",
        "  cnn_dailymail:\n",
        "    name: \"cnn_dailymail\"\n",
        "    subset: \"3.0.0\"\n",
        "    split: \"train\"\n",
        "    text_column: \"article\"\n",
        "    max_samples: 1000\n",
        "    description: \"CNN and Daily Mail news articles\"\n",
        "\n",
        "  # Reviews\n",
        "  yelp_reviews:\n",
        "    name: \"yelp_review_full\"\n",
        "    subset: null\n",
        "    split: \"train\"\n",
        "    text_column: \"text\"\n",
        "    max_samples: 1000\n",
        "    description: \"Restaurant reviews\"\n",
        "\n",
        "  # QA\n",
        "  squad:\n",
        "    name: \"squad\"\n",
        "    subset: null\n",
        "    split: \"train\"\n",
        "    text_column: \"context\"\n",
        "    max_samples: 1000\n",
        "    description: \"Reading comprehension contexts\"\n",
        "\n",
        "# Preprocessing settings\n",
        "preprocessing:\n",
        "  max_length: 512\n",
        "  stride: 256\n",
        "  min_length: 32\n",
        "  remove_empty: true\n",
        "  lowercase: false\n",
        "  remove_special_chars: false\n",
        "\n",
        "# Tokenization settings\n",
        "tokenization:\n",
        "  padding: \"max_length\"\n",
        "  truncation: true\n",
        "  return_attention_mask: true\n",
        "  return_token_type_ids: false\n",
        "\n",
        "# Data loading settings\n",
        "dataloader:\n",
        "  batch_size: 8\n",
        "  num_workers: 2\n",
        "  pin_memory: true\n",
        "  shuffle: true\n",
        "  drop_last: true\n",
        "  prefetch_factor: 2\n",
        "\n",
        "# Validation split settings\n",
        "validation:\n",
        "  split_ratio: 0.1\n",
        "  seed: 42\n",
        "''')\n",
        "\n",
        "print(\"✅ Reduced dataset sizes!\")\n",
        "print(\"   Total: ~10,000 samples\")\n",
        "print(\"   Training: ~9,000 samples\")\n",
        "print(\"   Validation: ~1,000 samples\")\n",
        "print(\"   Full epoch time: ~30-40 minutes\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jz1QBoSq4S3T",
        "outputId": "bd319a83-19df-49ee-9055-f2f7d11f0b79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Reduced dataset sizes!\n",
            "   Total: ~10,000 samples\n",
            "   Training: ~9,000 samples\n",
            "   Validation: ~1,000 samples\n",
            "   Full epoch time: ~30-40 minutes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Clear Python module cache\n",
        "import sys\n",
        "for module in list(sys.modules.keys()):\n",
        "    if module.startswith('src'):\n",
        "        del sys.modules[module]\n",
        "\n",
        "print(\"✅ Python cache cleared!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K11wcQ4A4WVk",
        "outputId": "7dcdb503-50c6-40a6-f7df-acc1378e9eb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Python cache cleared!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Remove Python bytecode cache\n",
        "!find /content/slm-lora-benchmark -type d -name \"__pycache__\" -exec rm -r {} + 2>/dev/null || true\n",
        "!find /content/slm-lora-benchmark -name \"*.pyc\" -delete 2>/dev/null || true\n",
        "\n",
        "print(\"✅ Bytecode cache cleared!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VVv8kasR4W2N",
        "outputId": "3565fdb3-0b7c-439e-adfd-de8ac9aad291"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Bytecode cache cleared!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Verify updated files\n",
        "!head -60 /content/slm-lora-benchmark/src/training/trainer.py | tail -10"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zmG1Jf6R4eGa",
        "outputId": "0bbef69e-db77-4257-c928-ac0fc53aff4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        self.eval_steps = training_config.get(\"eval_steps\", 500)\n",
            "        self.save_steps = training_config.get(\"save_steps\", 1000)\n",
            "\n",
            "        # EMA loss tracking\n",
            "        self.ema_loss = None\n",
            "        self.ema_alpha = 0.9\n",
            "\n",
            "    def train(self):\n",
            "        \"\"\"Main training loop\"\"\"\n",
            "        print(\"Starting training...\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python scripts/train.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eOR37Tx64hJn",
        "outputId": "3d766f3f-21b2-4d4b-e5eb-e37c7a989410"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-21 10:12:10.307370: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763719930.326839    9772 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763719930.332844    9772 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763719930.348218    9772 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763719930.348256    9772 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763719930.348261    9772 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763719930.348264    9772 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-21 10:12:10.352846: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[2;36m[11/21/25 10:12:16]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Logging to file:                                   \n",
            "\u001b[2;36m                    \u001b[0m         outputs/runs/logs/training_20251121_101216.log     \n",
            "INFO:train:Logging to file: outputs/runs/logs/training_20251121_101216.log\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m ===================================================\n",
            "\u001b[2;36m                    \u001b[0m         =============================                      \n",
            "INFO:train:================================================================================\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m SLM LoRA TRAINING                                  \n",
            "INFO:train:SLM LoRA TRAINING\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m ===================================================\n",
            "\u001b[2;36m                    \u001b[0m         =============================                      \n",
            "INFO:train:================================================================================\n",
            "================================================================================\n",
            "ENVIRONMENT INFORMATION\n",
            "================================================================================\n",
            "Python: 3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]\n",
            "PyTorch: 2.8.0+cu126\n",
            "CUDA available: True\n",
            "CUDA version: 12.6\n",
            "GPU count: 1\n",
            "GPU name: Tesla T4\n",
            "GPU memory: 14.74 GB\n",
            "CPU count: 2\n",
            "================================================================================\n",
            "Random seed set to: 42\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m                                                    \n",
            "\u001b[2;36m                    \u001b[0m         Loading configurations\u001b[33m...\u001b[0m                          \n",
            "INFO:train:\n",
            "Loading configurations...\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Configurations loaded successfully                 \n",
            "INFO:train:Configurations loaded successfully\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m                                                    \n",
            "\u001b[2;36m                    \u001b[0m         Initializing Weights & Biases\u001b[33m...\u001b[0m                   \n",
            "INFO:train:\n",
            "Initializing Weights & Biases...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maryanpawar1515\u001b[0m (\u001b[33maryanpawar1515-mahindra-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m Waiting for wandb.init()...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m Waiting for wandb.init()...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m Waiting for wandb.init()...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m setting up run mmkrrn2k (0.3s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m setting up run mmkrrn2k (0.3s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣯\u001b[0m setting up run mmkrrn2k (0.3s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣟\u001b[0m setting up run mmkrrn2k (0.3s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⡿\u001b[0m setting up run mmkrrn2k (0.3s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m setting up run mmkrrn2k (0.8s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.22.3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/slm-lora-benchmark/wandb/run-20251121_101216-mmkrrn2k\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mpretty-pyramid-25\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aryanpawar1515-mahindra-university/slm-lora-benchmark\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aryanpawar1515-mahindra-university/slm-lora-benchmark/runs/mmkrrn2k\u001b[0m\n",
            "W&B run initialized: pretty-pyramid-25\n",
            "W&B run URL: https://wandb.ai/aryanpawar1515-mahindra-university/slm-lora-benchmark/runs/mmkrrn2k\n",
            "\u001b[2;36m[11/21/25 10:12:18]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m                                                    \n",
            "\u001b[2;36m                    \u001b[0m         Loading model and tokenizer\u001b[33m...\u001b[0m                     \n",
            "INFO:train:\n",
            "Loading model and tokenizer...\n",
            "Loading tokenizer: EleutherAI/pythia-410m\n",
            "tokenizer_config.json: 100% 396/396 [00:00<00:00, 1.87MB/s]\n",
            "tokenizer.json: 2.11MB [00:00, 61.4MB/s]\n",
            "special_tokens_map.json: 100% 99.0/99.0 [00:00<00:00, 538kB/s]\n",
            "Set pad_token to eos_token: <|endoftext|>\n",
            "Tokenizer loaded: vocab_size=50277\n",
            "Loading model: EleutherAI/pythia-410m\n",
            "config.json: 100% 570/570 [00:00<00:00, 2.69MB/s]\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "model.safetensors: 100% 911M/911M [00:08<00:00, 107MB/s]\n",
            "Gradient checkpointing enabled\n",
            "Model loaded: GPTNeoXForCausalLM\n",
            "Resizing token embeddings: 50304 -> 50277\n",
            "\u001b[2;36m[11/21/25 10:12:30]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Model loaded: EleutherAI/pythia-410m               \n",
            "INFO:train:Model loaded: EleutherAI/pythia-410m\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Tokenizer vocab size: \u001b[1;36m50277\u001b[0m                        \n",
            "INFO:train:Tokenizer vocab size: 50277\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m                                                    \n",
            "\u001b[2;36m                    \u001b[0m         Applying LoRA adapters\u001b[33m...\u001b[0m                          \n",
            "INFO:train:\n",
            "Applying LoRA adapters...\n",
            "LoRA Configuration:\n",
            "  Rank (r): 16\n",
            "  Alpha: 32\n",
            "  Dropout: 0.1\n",
            "  Target modules: ['query_key_value']\n",
            "Applying LoRA adapters...\n",
            "trainable params: 1,572,864 || all params: 406,851,584 || trainable%: 0.3866\n",
            "================================================================================\n",
            "TRAINABLE PARAMETERS\n",
            "================================================================================\n",
            "Total parameters: 406,851,584\n",
            "Trainable parameters: 1,572,864\n",
            "Non-trainable parameters: 405,278,720\n",
            "Trainable %: 0.39%\n",
            "================================================================================\n",
            "\u001b[2;36m[11/21/25 10:12:36]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m                                                    \n",
            "\u001b[2;36m                    \u001b[0m         Setting up datasets and dataloaders\u001b[33m...\u001b[0m             \n",
            "INFO:train:\n",
            "Setting up datasets and dataloaders...\n",
            "Setting up data module...\n",
            "\n",
            "Loading dataset: wikitext_103\n",
            "Loading dataset: Salesforce/wikitext\n",
            "README.md: 10.5kB [00:00, 15.2MB/s]\n",
            "wikitext-103-raw-v1/test-00000-of-00001.(…): 100% 733k/733k [00:00<00:00, 1.04MB/s]\n",
            "wikitext-103-raw-v1/train-00000-of-00002(…): 100% 157M/157M [00:01<00:00, 92.6MB/s] \n",
            "wikitext-103-raw-v1/train-00001-of-00002(…): 100% 157M/157M [00:01<00:00, 132MB/s]  \n",
            "wikitext-103-raw-v1/validation-00000-of-(…): 100% 657k/657k [00:00<00:00, 1.56MB/s]\n",
            "Generating test split: 100% 4358/4358 [00:00<00:00, 5083.59 examples/s]\n",
            "Generating train split: 100% 1801350/1801350 [00:05<00:00, 342018.85 examples/s]\n",
            "Generating validation split: 100% 3760/3760 [00:00<00:00, 459408.73 examples/s]\n",
            "Dataset loaded: 1801350 samples\n",
            "Limited to 2000 samples\n",
            "Tokenizing dataset: 100% 2000/2000 [00:00<00:00, 2737.20 examples/s]\n",
            "Tokenized dataset: 2001 samples\n",
            "\n",
            "Loading dataset: wikitext_2\n",
            "Loading dataset: Salesforce/wikitext\n",
            "wikitext-2-raw-v1/train-00000-of-00001.p(…): 100% 6.36M/6.36M [00:00<00:00, 16.9MB/s]\n",
            "Generating test split: 100% 4358/4358 [00:00<00:00, 548994.65 examples/s]\n",
            "Generating train split: 100% 36718/36718 [00:00<00:00, 800245.54 examples/s]\n",
            "Generating validation split: 100% 3760/3760 [00:00<00:00, 360940.72 examples/s]\n",
            "Dataset loaded: 36718 samples\n",
            "Limited to 1000 samples\n",
            "Tokenizing dataset: 100% 1000/1000 [00:00<00:00, 2935.20 examples/s]\n",
            "Tokenized dataset: 1000 samples\n",
            "\n",
            "Loading dataset: tinystories\n",
            "Loading dataset: roneneldan/TinyStories\n",
            "README.md: 1.06kB [00:00, 2.23MB/s]\n",
            "data/train-00000-of-00004-2d5a1467fff108(…): 100% 249M/249M [00:01<00:00, 136MB/s]\n",
            "data/train-00001-of-00004-5852b56a2bd28f(…): 100% 248M/248M [00:05<00:00, 42.2MB/s]\n",
            "data/train-00002-of-00004-a26307300439e9(…): 100% 246M/246M [00:03<00:00, 64.7MB/s]\n",
            "data/train-00003-of-00004-d243063613e5a0(…): 100% 248M/248M [00:03<00:00, 62.2MB/s]\n",
            "data/validation-00000-of-00001-869c898b5(…): 100% 9.99M/9.99M [00:00<00:00, 29.8MB/s]\n",
            "Generating train split: 100% 2119719/2119719 [00:15<00:00, 136973.13 examples/s]\n",
            "Generating validation split: 100% 21990/21990 [00:00<00:00, 115554.40 examples/s]\n",
            "Dataset loaded: 2119719 samples\n",
            "Limited to 2000 samples\n",
            "Tokenizing dataset: 100% 2000/2000 [00:02<00:00, 912.89 examples/s]\n",
            "Tokenized dataset: 2048 samples\n",
            "\n",
            "Loading dataset: ag_news\n",
            "Loading dataset: ag_news\n",
            "README.md: 8.07kB [00:00, 21.9MB/s]\n",
            "data/train-00000-of-00001.parquet: 100% 18.6M/18.6M [00:00<00:00, 30.3MB/s]\n",
            "data/test-00000-of-00001.parquet: 100% 1.23M/1.23M [00:00<00:00, 2.65MB/s]\n",
            "Generating train split: 100% 120000/120000 [00:00<00:00, 890423.76 examples/s]\n",
            "Generating test split: 100% 7600/7600 [00:00<00:00, 740457.85 examples/s]\n",
            "Dataset loaded: 120000 samples\n",
            "Limited to 2000 samples\n",
            "Tokenizing dataset: 100% 2000/2000 [00:00<00:00, 2824.87 examples/s]\n",
            "Tokenized dataset: 2000 samples\n",
            "\n",
            "Loading dataset: cnn_dailymail\n",
            "Loading dataset: cnn_dailymail\n",
            "README.md: 15.6kB [00:00, 27.3MB/s]\n",
            "3.0.0/train-00000-of-00003.parquet: 100% 257M/257M [00:05<00:00, 44.8MB/s]\n",
            "3.0.0/train-00001-of-00003.parquet: 100% 257M/257M [00:05<00:00, 47.5MB/s]\n",
            "3.0.0/train-00002-of-00003.parquet: 100% 259M/259M [00:07<00:00, 36.9MB/s]\n",
            "3.0.0/validation-00000-of-00001.parquet: 100% 34.7M/34.7M [00:01<00:00, 18.5MB/s]\n",
            "3.0.0/test-00000-of-00001.parquet: 100% 30.0M/30.0M [00:00<00:00, 52.2MB/s]\n",
            "Generating train split: 100% 287113/287113 [00:16<00:00, 17483.72 examples/s]\n",
            "Generating validation split: 100% 13368/13368 [00:00<00:00, 57698.69 examples/s]\n",
            "Generating test split: 100% 11490/11490 [00:00<00:00, 53338.25 examples/s]\n",
            "Dataset loaded: 287113 samples\n",
            "Limited to 1000 samples\n",
            "Tokenizing dataset: 100% 1000/1000 [00:02<00:00, 396.85 examples/s]\n",
            "Tokenized dataset: 2504 samples\n",
            "\n",
            "Loading dataset: yelp_reviews\n",
            "Loading dataset: yelp_review_full\n",
            "README.md: 6.72kB [00:00, 10.9MB/s]\n",
            "yelp_review_full/train-00000-of-00001.pa(…): 100% 299M/299M [00:02<00:00, 101MB/s]\n",
            "yelp_review_full/test-00000-of-00001.par(…): 100% 23.5M/23.5M [00:00<00:00, 45.1MB/s]\n",
            "Generating train split: 100% 650000/650000 [00:05<00:00, 122908.61 examples/s]\n",
            "Generating test split: 100% 50000/50000 [00:00<00:00, 298161.54 examples/s]\n",
            "Dataset loaded: 650000 samples\n",
            "Limited to 1000 samples\n",
            "Tokenizing dataset: 100% 1000/1000 [00:00<00:00, 1582.80 examples/s]\n",
            "Tokenized dataset: 1061 samples\n",
            "\n",
            "Loading dataset: squad\n",
            "Loading dataset: squad\n",
            "README.md: 7.62kB [00:00, 14.0MB/s]\n",
            "plain_text/train-00000-of-00001.parquet: 100% 14.5M/14.5M [00:00<00:00, 30.1MB/s]\n",
            "plain_text/validation-00000-of-00001.par(…): 100% 1.82M/1.82M [00:00<00:00, 2.75MB/s]\n",
            "Generating train split: 100% 87599/87599 [00:00<00:00, 296925.29 examples/s]\n",
            "Generating validation split: 100% 10570/10570 [00:00<00:00, 127349.12 examples/s]\n",
            "Dataset loaded: 87599 samples\n",
            "Limited to 1000 samples\n",
            "Tokenizing dataset: 100% 1000/1000 [00:01<00:00, 909.33 examples/s]\n",
            "Tokenized dataset: 1000 samples\n",
            "\n",
            "Combined dataset: 11614 total samples\n",
            "Train dataset: 10452 samples\n",
            "Validation dataset: 1162 samples\n",
            "Train batches: 1306\n",
            "Validation batches: 146\n",
            "\u001b[2;36m[11/21/25 10:14:33]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Training samples: \u001b[1;36m10452\u001b[0m                            \n",
            "INFO:train:Training samples: 10452\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Validation samples: \u001b[1;36m1162\u001b[0m                           \n",
            "INFO:train:Validation samples: 1162\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Training batches: \u001b[1;36m1306\u001b[0m                             \n",
            "INFO:train:Training batches: 1306\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m                                                    \n",
            "\u001b[2;36m                    \u001b[0m         Creating optimizer and scheduler\u001b[33m...\u001b[0m                \n",
            "INFO:train:\n",
            "Creating optimizer and scheduler...\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Optimizer: AdamW                                   \n",
            "INFO:train:Optimizer: AdamW\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Scheduler: cosine                                  \n",
            "INFO:train:Scheduler: cosine\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Total training steps: \u001b[1;36m1306\u001b[0m                         \n",
            "INFO:train:Total training steps: 1306\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m                                                    \n",
            "\u001b[2;36m                    \u001b[0m         Setting up checkpoint manager\u001b[33m...\u001b[0m                   \n",
            "INFO:train:\n",
            "Setting up checkpoint manager...\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m                                                    \n",
            "\u001b[2;36m                    \u001b[0m         Using device: cuda                                 \n",
            "INFO:train:\n",
            "Using device: cuda\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m                                                    \n",
            "\u001b[2;36m                    \u001b[0m         ===================================================\n",
            "\u001b[2;36m                    \u001b[0m         =============================                      \n",
            "INFO:train:\n",
            "================================================================================\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m STARTING TRAINING                                  \n",
            "INFO:train:STARTING TRAINING\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m ===================================================\n",
            "\u001b[2;36m                    \u001b[0m         =============================                      \n",
            "\u001b[2;36m                    \u001b[0m                                                            \n",
            "INFO:train:================================================================================\n",
            "\n",
            "Starting training...\n",
            "\n",
            "================================================================================\n",
            "Epoch 1/1\n",
            "================================================================================\n",
            "Training Epoch 1:   0% 0/1306 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Training Epoch 1: 100% 1306/1306 [59:38<00:00,  2.74s/it, loss=2.4596, ppl=11.70, lr=9.16e-05, samples=10448]\n",
            "Validating:   0% 0/146 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Validating: 100% 146/146 [02:07<00:00,  1.14it/s]\n",
            "\n",
            "Validation - Loss: 2.6374, PPL: 13.98, BPT: 3.8050\n",
            "Checkpoint saved: outputs/runs/checkpoints/checkpoint_epoch0_step326_20251121_111620.pt\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 326 that is less than the current step 327. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
            "Best checkpoint updated: outputs/runs/checkpoints/best.pt\n",
            "\n",
            "Training completed in 1.04 hours\n",
            "\u001b[2;36m[11/21/25 11:16:46]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m                                                    \n",
            "\u001b[2;36m                    \u001b[0m         ===================================================\n",
            "\u001b[2;36m                    \u001b[0m         =============================                      \n",
            "INFO:train:\n",
            "================================================================================\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m TRAINING COMPLETED                                 \n",
            "INFO:train:TRAINING COMPLETED\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m ===================================================\n",
            "\u001b[2;36m                    \u001b[0m         =============================                      \n",
            "INFO:train:================================================================================\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Best validation loss: \u001b[1;36m2.6374\u001b[0m                       \n",
            "INFO:train:Best validation loss: 2.6374\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Total training time: \u001b[1;36m1.04\u001b[0m hours                    \n",
            "INFO:train:Total training time: 1.04 hours\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Best checkpoint:                                   \n",
            "\u001b[2;36m                    \u001b[0m         outputs/runs/checkpoints/checkpoint_epoch0_step326_\n",
            "\u001b[2;36m                    \u001b[0m         20251121_111620.pt                                 \n",
            "INFO:train:Best checkpoint: outputs/runs/checkpoints/checkpoint_epoch0_step326_20251121_111620.pt\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m updating run metadata (0.0s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m updating run metadata (0.0s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m updating run metadata (0.0s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m updating run metadata (0.0s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m uploading summary, console lines 193-213 (0.0s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣯\u001b[0m uploading summary, console lines 193-213 (0.0s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                       train/bpt ▆▄▆▄▄▆▅▅▄▁▆▅▄▂▆▄▅▅▇▅▂▄▂▄▅▇█▅▇▂▅█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/ema_token_loss ▄▆▇█▆▇▆▆▆▄▅▂▃▃▄▃▄▄▅▄▂▃▂▁▃▃▃▃▆▃▂▄\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/epoch ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 train/grad_norm ▃▂▁▅▅▆█▆▄▆▄▄█▆▂▆▂▂▃▃▃▂▂▄▂▃▃▃▃▂▃▅\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 train/lora_rank ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                        train/lr ▁▂▃▃▄▅▆▆▇████████████████████▇▇▇\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                       train/ppl ▅▃▆▄▄▅▄▄▃▁▅▄▃▁▅▃▄▄▆▄▂▃▂▃▄▆█▄▇▂▄▇\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      train/step ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 train/step_time ▃▄█▆▄▆▁▆▆▄▆█▃▃█▃▆▇▄▆▅▇▆▅▆▄▄▄▅▅▆▄\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train/throughput_tokens_per_sec ▂▄▇▆▄▅▄▅▃▃▅▅▃▅█▃▄▅▄▃▃▆▅▄▄▆▂▅▂▅▇▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                              +5 ...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                       train/bpt 4.59738\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/ema_token_loss 2.69303\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/epoch 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 train/grad_norm 0.96757\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 train/lora_rank 16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                        train/lr 9e-05\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                       train/ppl 24.2074\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      train/step 320\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 train/step_time 2.73926\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train/throughput_tokens_per_sec 263.93942\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                              +5 ...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mpretty-pyramid-25\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/aryanpawar1515-mahindra-university/slm-lora-benchmark/runs/mmkrrn2k\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/aryanpawar1515-mahindra-university/slm-lora-benchmark\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20251121_101216-mmkrrn2k/logs\u001b[0m\n",
            "W&B run finished\n",
            "\u001b[2;36m[11/21/25 11:16:47]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m                                                    \n",
            "\u001b[2;36m                    \u001b[0m         Training script completed successfully!            \n",
            "INFO:train:\n",
            "Training script completed successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Backup everything\n",
        "!mkdir -p /content/drive/MyDrive/slm-lora-backup\n",
        "!cp -r outputs/ /content/drive/MyDrive/slm-lora-backup/\n",
        "!cp -r configs/ /content/drive/MyDrive/slm-lora-backup/\n",
        "!cp -r scripts/ /content/drive/MyDrive/slm-lora-backup/\n",
        "!cp -r src/ /content/drive/MyDrive/slm-lora-backup/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rs5W_i2f4m2y",
        "outputId": "49e9713f-4f75-4109-a059-a1cab99f6219"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python scripts/evaluate.py \\\n",
        "    --checkpoint outputs/checkpoints/best_model \\\n",
        "    --config configs/training_config.yaml"
      ],
      "metadata": {
        "id": "C_0uGOk1JXnP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python scripts/benchmark_inference.py \\\n",
        "    --checkpoint outputs/checkpoints/best_model \\\n",
        "    --num_samples 100 \\\n",
        "    --batch_sizes 1,4,8,16"
      ],
      "metadata": {
        "id": "CHMhoCYGJcGd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check what's in outputs folder\n",
        "!find outputs/ -type f -ls\n",
        "\n",
        "# Check if training actually saved anything\n",
        "!ls -lh outputs/checkpoints/\n",
        "\n",
        "# Look for error messages\n",
        "!python scripts/evaluate.py --checkpoint outputs/checkpoints/best_model --config configs/training_config.yaml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rd38X732JpWV",
        "outputId": "71809142-6d73-4756-bb2a-5d521cc5155f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  2495255      0 -rw-r--r--   1 root     root            0 Nov 21 09:40 outputs/logs/.gitkeep\n",
            "  2507122      4 -rw-r--r--   1 root     root         2429 Nov 21 11:16 outputs/runs/logs/training_20251121_101216.log\n",
            "  2507738 1601780 -rw-r--r--   1 root     root     1640214603 Nov 21 11:16 outputs/runs/checkpoints/checkpoint_epoch0_step326_20251121_111620.pt\n",
            "  2507739 1601780 -rw-r--r--   1 root     root     1640214603 Nov 21 11:16 outputs/runs/checkpoints/best.pt\n",
            "  2495257       0 -rw-r--r--   1 root     root              0 Nov 21 09:40 outputs/results/.gitkeep\n",
            "  2495253       0 -rw-r--r--   1 root     root              0 Nov 21 09:40 outputs/checkpoints/.gitkeep\n",
            "total 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate with correct path\n",
        "!python scripts/evaluate.py \\\n",
        "    --checkpoint outputs/runs/checkpoints/best.pt \\\n",
        "    --config configs/training_config.yaml\n",
        "\n",
        "# Benchmark with correct path\n",
        "!python scripts/benchmark_inference.py \\\n",
        "    --checkpoint outputs/runs/checkpoints/best.pt \\\n",
        "    --num_samples 100 \\\n",
        "    --batch_sizes 1,4,8,16"
      ],
      "metadata": {
        "id": "btEFOW6bJ0TS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and test the checkpoint manually\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Load checkpoint\n",
        "checkpoint = torch.load('outputs/runs/checkpoints/best.pt')\n",
        "print(\"Checkpoint keys:\", checkpoint.keys())\n",
        "print(\"\\n\")\n",
        "\n",
        "# Check what's inside\n",
        "if 'model_state_dict' in checkpoint:\n",
        "    print(\"Model state dict size:\", len(checkpoint['model_state_dict']))\n",
        "if 'loss' in checkpoint:\n",
        "    print(\"Final loss:\", checkpoint['loss'])\n",
        "if 'epoch' in checkpoint:\n",
        "    print(\"Epoch:\", checkpoint['epoch'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sisqzhffKDqw",
        "outputId": "a49511eb-5672-406a-ef8f-e44ca45358ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint keys: dict_keys(['epoch', 'step', 'model_state_dict', 'optimizer_state_dict', 'metrics', 'timestamp'])\n",
            "\n",
            "\n",
            "Model state dict size: 340\n",
            "Epoch: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# See what metrics were saved\n",
        "checkpoint = torch.load('outputs/runs/checkpoints/best.pt')\n",
        "print(\"Saved metrics:\", checkpoint['metrics'])\n",
        "print(\"\\nStep:\", checkpoint['step'])\n",
        "print(\"Timestamp:\", checkpoint['timestamp'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rulou60UKQx9",
        "outputId": "605f5739-08af-4ee5-fcdf-f18c303affb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved metrics: {'val_loss': 2.637437618748266, 'val_ppl': 13.977342402194248, 'val_bpt': 3.80501817322212, 'val_n_tokens': 244213}\n",
            "\n",
            "Step: 326\n",
            "Timestamp: 20251121_111620\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the training log\n",
        "!tail -50 outputs/runs/logs/training_20251121_101216.log\n",
        "\n",
        "# For the report, you need these metrics from W&B:\n",
        "# - Final training loss\n",
        "# - Validation perplexity (PPL)\n",
        "# - Bits-per-token (BPT)\n",
        "# - Tokens/sec throughput\n",
        "\n",
        "# The evaluate/benchmark scripts might not be working as expected. Check your W&B dashboard - that's where all your real metrics are. Go to:\n",
        "# https://wandb.ai/your-username/slm-lora-benchmark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TaySouN9KRU0",
        "outputId": "8a6c1642-07b9-43ed-f832-d915d105e460"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-21 10:12:16 - train - INFO - Logging to file: outputs/runs/logs/training_20251121_101216.log\n",
            "2025-11-21 10:12:16 - train - INFO - ================================================================================\n",
            "2025-11-21 10:12:16 - train - INFO - SLM LoRA TRAINING\n",
            "2025-11-21 10:12:16 - train - INFO - ================================================================================\n",
            "2025-11-21 10:12:16 - train - INFO - \n",
            "Loading configurations...\n",
            "2025-11-21 10:12:16 - train - INFO - Configurations loaded successfully\n",
            "2025-11-21 10:12:16 - train - INFO - \n",
            "Initializing Weights & Biases...\n",
            "2025-11-21 10:12:18 - train - INFO - \n",
            "Loading model and tokenizer...\n",
            "2025-11-21 10:12:30 - train - INFO - Model loaded: EleutherAI/pythia-410m\n",
            "2025-11-21 10:12:30 - train - INFO - Tokenizer vocab size: 50277\n",
            "2025-11-21 10:12:30 - train - INFO - \n",
            "Applying LoRA adapters...\n",
            "2025-11-21 10:12:36 - train - INFO - \n",
            "Setting up datasets and dataloaders...\n",
            "2025-11-21 10:14:33 - train - INFO - Training samples: 10452\n",
            "2025-11-21 10:14:33 - train - INFO - Validation samples: 1162\n",
            "2025-11-21 10:14:33 - train - INFO - Training batches: 1306\n",
            "2025-11-21 10:14:33 - train - INFO - \n",
            "Creating optimizer and scheduler...\n",
            "2025-11-21 10:14:33 - train - INFO - Optimizer: AdamW\n",
            "2025-11-21 10:14:33 - train - INFO - Scheduler: cosine\n",
            "2025-11-21 10:14:33 - train - INFO - Total training steps: 1306\n",
            "2025-11-21 10:14:33 - train - INFO - \n",
            "Setting up checkpoint manager...\n",
            "2025-11-21 10:14:33 - train - INFO - \n",
            "Using device: cuda\n",
            "2025-11-21 10:14:33 - train - INFO - \n",
            "================================================================================\n",
            "2025-11-21 10:14:33 - train - INFO - STARTING TRAINING\n",
            "2025-11-21 10:14:33 - train - INFO - ================================================================================\n",
            "\n",
            "2025-11-21 11:16:46 - train - INFO - \n",
            "================================================================================\n",
            "2025-11-21 11:16:46 - train - INFO - TRAINING COMPLETED\n",
            "2025-11-21 11:16:46 - train - INFO - ================================================================================\n",
            "2025-11-21 11:16:46 - train - INFO - Best validation loss: 2.6374\n",
            "2025-11-21 11:16:46 - train - INFO - Total training time: 1.04 hours\n",
            "2025-11-21 11:16:46 - train - INFO - Best checkpoint: outputs/runs/checkpoints/checkpoint_epoch0_step326_20251121_111620.pt\n",
            "2025-11-21 11:16:47 - train - INFO - \n",
            "Training script completed successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/slm-lora-benchmark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Djupypn_LE39",
        "outputId": "f17269e6-3c94-42cd-daba-7a34d48dfff5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/slm-lora-benchmark\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git status"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iERy5wSLLLQb",
        "outputId": "a37f9c3d-440d-4f06-99db-6f1e7083f801"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "On branch main\n",
            "Your branch is up to date with 'origin/main'.\n",
            "\n",
            "Changes not staged for commit:\n",
            "  (use \"git add <file>...\" to update what will be committed)\n",
            "  (use \"git restore <file>...\" to discard changes in working directory)\n",
            "\t\u001b[31mmodified:   configs/datasets.yaml\u001b[m\n",
            "\t\u001b[31mmodified:   scripts/train.py\u001b[m\n",
            "\t\u001b[31mmodified:   src/data/__init__.py\u001b[m\n",
            "\t\u001b[31mmodified:   src/models/__init__.py\u001b[m\n",
            "\t\u001b[31mmodified:   src/training/__init__.py\u001b[m\n",
            "\t\u001b[31mmodified:   src/training/optimization.py\u001b[m\n",
            "\t\u001b[31mmodified:   src/training/trainer.py\u001b[m\n",
            "\t\u001b[31mmodified:   src/utils/__init__.py\u001b[m\n",
            "\t\u001b[31mmodified:   wandb/debug-internal.log\u001b[m\n",
            "\t\u001b[31mmodified:   wandb/debug.log\u001b[m\n",
            "\t\u001b[31mmodified:   wandb/latest-run\u001b[m\n",
            "\n",
            "no changes added to commit (use \"git add\" and/or \"git commit -a\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git diff"
      ],
      "metadata": {
        "id": "slGsLdafLL4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/slm-lora-benchmark && git add .gitignore configs/ scripts/ src/ README.md\n",
        "!cd /content/slm-lora-benchmark && git commit -m \"Add .gitignore and complete first training run\"\n",
        "!cd /content/slm-lora-benchmark && git push origin main"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S2q53N36KPwf",
        "outputId": "859c3715-4e29-4286-937f-2d4f82351faa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "On branch main\n",
            "Your branch is up to date with 'origin/main'.\n",
            "\n",
            "nothing to commit, working tree clean\n",
            "remote: Repository not found.\n",
            "fatal: repository 'https://github.com/yourusername/slm-lora-benchmark.git/' not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git diff configs/\n",
        "!git diff scripts/\n",
        "!git diff src/"
      ],
      "metadata": {
        "id": "-fokMf8KLOhI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c26ab10f"
      },
      "source": [
        "!git config --global user.name \"aryanpawar1234\"\n",
        "!git config --global user.email \"aryanpawar1515@gmail.com\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add .gitignore first\n",
        "gitignore_content = \"\"\"# Python\n",
        "__pycache__/\n",
        "*.py[cod]\n",
        "*.so\n",
        "venv/\n",
        "*.ipynb_checkpoints\n",
        "\n",
        "# Training outputs\n",
        "outputs/\n",
        "wandb/\n",
        "*.pt\n",
        "*.pth\n",
        "*.bin\n",
        "*.safetensors\n",
        "\n",
        "# Logs\n",
        "logs/\n",
        "*.log\n",
        "\n",
        "# LaTeX\n",
        "report/*.aux\n",
        "report/*.log\n",
        "report/*.out\n",
        "report/*.pdf\n",
        "\n",
        "# OS\n",
        ".DS_Store\n",
        "*.swp\n",
        "\"\"\"\n",
        "\n",
        "with open('.gitignore', 'w') as f:\n",
        "    f.write(gitignore_content)\n",
        "\n",
        "print(\"Created .gitignore\")\n",
        "\n",
        "# Stage and commit\n",
        "!git add .gitignore\n",
        "!git add configs/datasets.yaml scripts/train.py src/\n",
        "!git commit -m \"Add .gitignore and fix imports, reduce dataset samples for faster training\"\n",
        "!git push origin main"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "imDmh8yoLgq8",
        "outputId": "ea4ca14e-bd29-4cea-85e1-fc04f7128541"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created .gitignore\n",
            "On branch main\n",
            "Your branch is ahead of 'origin/main' by 1 commit.\n",
            "  (use \"git push\" to publish your local commits)\n",
            "\n",
            "Changes not staged for commit:\n",
            "  (use \"git add <file>...\" to update what will be committed)\n",
            "  (use \"git restore <file>...\" to discard changes in working directory)\n",
            "\t\u001b[31mmodified:   wandb/debug-internal.log\u001b[m\n",
            "\t\u001b[31mmodified:   wandb/debug.log\u001b[m\n",
            "\t\u001b[31mmodified:   wandb/latest-run\u001b[m\n",
            "\n",
            "no changes added to commit (use \"git add\" and/or \"git commit -a\")\n",
            "fatal: could not read Username for 'https://github.com': No such device or address\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up Git credentials for Colab\n",
        "!git config --global user.email \"your.email@example.com\"\n",
        "!git config --global user.name \"Your Name\"\n",
        "\n",
        "# Use personal access token for authentication\n",
        "import getpass\n",
        "token = getpass.getpass('Enter your GitHub Personal Access Token: ')\n",
        "\n",
        "# Update remote URL with token\n",
        "!git remote set-url origin https://{token}@github.com/yourusername/slm-lora-benchmark.git\n",
        "\n",
        "# Now commit and push\n",
        "!git add .gitignore configs/ scripts/ src/\n",
        "!git commit -m \"Add .gitignore and fix imports, reduce samples for faster training\"\n",
        "!git push origin main"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0oAlEjoMJTOX",
        "outputId": "fc9eace1-0495-4b5b-c07d-bb3ac2595a7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your GitHub Personal Access Token: ··········\n",
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "fatal: not a git repository (or any of the parent directories): .git\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check where you are\n",
        "!pwd\n",
        "\n",
        "# Navigate to your repo\n",
        "%cd /content/slm-lora-benchmark\n",
        "\n",
        "# Verify it's a git repo\n",
        "!git status\n",
        "\n",
        "# Now set credentials and push\n",
        "!git config --global user.email \"your.email@example.com\"\n",
        "!git config --global user.name \"Your Name\"\n",
        "\n",
        "# Get token again\n",
        "import getpass\n",
        "token = getpass.getpass('Enter GitHub Token: ')\n",
        "\n",
        "# Update remote with token\n",
        "!git remote set-url origin https://{token}@github.com/yourusername/slm-lora-benchmark.git\n",
        "\n",
        "# Commit and push\n",
        "!git add .gitignore configs/ scripts/ src/\n",
        "!git commit -m \"Add .gitignore, fix imports, reduce dataset samples\"\n",
        "!git push origin main"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TYe3k3AcJevF",
        "outputId": "bdcbc176-fdf1-4744-84c3-6ca949f98359"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/slm-lora-benchmark\n",
            "/content/slm-lora-benchmark\n",
            "On branch main\n",
            "Your branch is up to date with 'origin/main'.\n",
            "\n",
            "nothing to commit, working tree clean\n",
            "Enter GitHub Token: ··········\n",
            "On branch main\n",
            "Your branch is up to date with 'origin/main'.\n",
            "\n",
            "nothing to commit, working tree clean\n",
            "remote: Repository not found.\n",
            "fatal: repository 'https://github.com/yourusername/slm-lora-benchmark.git/' not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Check what's in backup\n",
        "!ls -la /content/drive/MyDrive/slm-lora-backup/\n",
        "\n",
        "# Clone fresh repo\n",
        "%cd /content\n",
        "!git clone https://github.com/aryanpawar1234/slm-lora-benchmark.git\n",
        "%cd slm-lora-benchmark\n",
        "\n",
        "# Copy backed up files (skip outputs/ - it's too large)\n",
        "!cp -r /content/drive/MyDrive/slm-lora-backup/configs/ .\n",
        "!cp -r /content/drive/MyDrive/slm-lora-backup/scripts/ .\n",
        "!cp -r /content/drive/MyDrive/slm-lora-backup/src/ .\n",
        "\n",
        "# Create .gitignore using Python's file writing\n",
        "gitignore_content = \"\"\"# Training outputs\n",
        "outputs/\n",
        "wandb/\n",
        "*.pt\n",
        "*.pth\n",
        "*.bin\n",
        "*.safetensors\n",
        "\n",
        "# Python\n",
        "__pycache__/\n",
        "*.py[cod]\n",
        "venv/\n",
        "\n",
        "# Logs\n",
        "logs/\n",
        "*.log\n",
        "\n",
        "# OS\n",
        ".DS_Store\n",
        "\"\"\"\n",
        "\n",
        "with open('.gitignore', 'w') as f:\n",
        "    f.write(gitignore_content)\n",
        "\n",
        "print(\"Created .gitignore\")\n",
        "\n",
        "# Set credentials\n",
        "!git config --global user.email \"your.email@example.com\"\n",
        "!git config --global user.name \"Your Name\"\n",
        "\n",
        "# Get token\n",
        "import getpass\n",
        "token = getpass.getpass('GitHub Token: ')\n",
        "\n",
        "# Commit all changes\n",
        "!git add .gitignore configs/ scripts/ src/\n",
        "!git commit -m \"Update configs, scripts, and src from training run\"\n",
        "!git push https://{token}@github.com/aryanpawar1234/slm-lora-benchmark.git main"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lk12y4efL1RK",
        "outputId": "8e9cb6e0-0395-42a5-b46c-c57a9d629309"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "total 16\n",
            "drwx------ 3 root root 4096 Nov 21 11:21 configs\n",
            "drwx------ 6 root root 4096 Nov 21 11:20 outputs\n",
            "drwx------ 2 root root 4096 Nov 21 11:21 scripts\n",
            "drwx------ 9 root root 4096 Nov 21 11:21 src\n",
            "/content\n",
            "fatal: destination path 'slm-lora-benchmark' already exists and is not an empty directory.\n",
            "/content/slm-lora-benchmark\n",
            "Created .gitignore\n",
            "GitHub Token: ··········\n",
            "On branch main\n",
            "Your branch is up to date with 'origin/main'.\n",
            "\n",
            "nothing to commit, working tree clean\n",
            "Everything up-to-date\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check current status\n",
        "!git status\n",
        "\n",
        "# Try pushing with token in URL differently\n",
        "import getpass\n",
        "token = getpass.getpass('GitHub Token (make sure it has repo access): ')\n",
        "\n",
        "!git remote set-url origin https://{token}@github.com/aryanpawar1234/slm-lora-benchmark.git\n",
        "!git push origin main"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8kl-kB0MX9d",
        "outputId": "56505339-4595-4863-91aa-51414ceb2e35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "On branch main\n",
            "Your branch is ahead of 'origin/main' by 1 commit.\n",
            "  (use \"git push\" to publish your local commits)\n",
            "\n",
            "nothing to commit, working tree clean\n",
            "GitHub Token (make sure it has repo access): ··········\n",
            "Enumerating objects: 37, done.\n",
            "Counting objects: 100% (37/37), done.\n",
            "Delta compression using up to 2 threads\n",
            "Compressing objects: 100% (21/21), done.\n",
            "Writing objects: 100% (21/21), 4.92 KiB | 2.46 MiB/s, done.\n",
            "Total 21 (delta 11), reused 0 (delta 0), pack-reused 0\n",
            "remote: Resolving deltas: 100% (11/11), completed with 11 local objects.\u001b[K\n",
            "To https://github.com/aryanpawar1234/slm-lora-benchmark.git\n",
            "   aae2359..b1fbc04  main -> main\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check which dataset you just trained on\n",
        "!cat configs/datasets.yaml | head -30\n",
        "\n",
        "# Or check the training log\n",
        "!grep -i \"dataset\" outputs/runs/logs/training_*.log | head -5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NCbUChUsND7c",
        "outputId": "31457051-4476-4aa5-db2e-587beede8f5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Dataset Configuration - Reduced for faster training\n",
            "\n",
            "datasets:\n",
            "  # Core Language Modeling Datasets\n",
            "  wikitext_103:\n",
            "    name: \"Salesforce/wikitext\"\n",
            "    subset: \"wikitext-103-raw-v1\"\n",
            "    split: \"train\"\n",
            "    text_column: \"text\"\n",
            "    max_samples: 2000\n",
            "    description: \"Large-scale Wikipedia articles\"\n",
            "\n",
            "  wikitext_2:\n",
            "    name: \"Salesforce/wikitext\"\n",
            "    subset: \"wikitext-2-raw-v1\"\n",
            "    split: \"train\"\n",
            "    text_column: \"text\"\n",
            "    max_samples: 1000\n",
            "    description: \"Smaller Wikipedia corpus\"\n",
            "\n",
            "  tinystories:\n",
            "    name: \"roneneldan/TinyStories\"\n",
            "    subset: null\n",
            "    split: \"train\"\n",
            "    text_column: \"text\"\n",
            "    max_samples: 2000\n",
            "    description: \"Simple narrative stories\"\n",
            "\n",
            "  # News & Articles\n",
            "  ag_news:\n",
            "grep: outputs/runs/logs/training_*.log: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check training log from backup\n",
        "!ls /content/drive/MyDrive/slm-lora-backup/outputs/runs/logs/\n",
        "\n",
        "# Read the log\n",
        "!cat /content/drive/MyDrive/slm-lora-backup/outputs/runs/logs/training_*.log | grep -i \"dataset\\|samples\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h6eO3c3tNQ36",
        "outputId": "87875152-3120-4903-f484-c339e9cc898d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training_20251121_101216.log\n",
            "Setting up datasets and dataloaders...\n",
            "2025-11-21 10:14:33 - train - INFO - Training samples: 10452\n",
            "2025-11-21 10:14:33 - train - INFO - Validation samples: 1162\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Login to W&B\n",
        "import wandb\n",
        "wandb.login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bviekP1JPGLS",
        "outputId": "f215b463-823a-498e-94f8-53d3940edf78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maryanpawar1515\u001b[0m (\u001b[33maryanpawar1515-mahindra-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Navigate to repo\n",
        "%cd /content/slm-lora-benchmark\n",
        "\n",
        "# Train on TinyStories (Run 1)\n",
        "!python scripts/train.py \\\n",
        "    --dataset_names tinystories \\\n",
        "    --wandb_name \"pythia-410m-tinystories\"\n",
        "\n",
        "# Backup immediately after\n",
        "!cp -r outputs/ /content/drive/MyDrive/slm-lora-backup/tinystories_outputs/\n",
        "\n",
        "# Train on AG News (Run 2)\n",
        "!python scripts/train.py \\\n",
        "    --dataset_names ag_news \\\n",
        "    --wandb_name \"pythia-410m-agnews\"\n",
        "\n",
        "# Backup immediately after\n",
        "!cp -r outputs/ /content/drive/MyDrive/slm-lora-backup/agnews_outputs/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bHpfd58mOrM1",
        "outputId": "e9f8048a-ddb0-4287-8f5e-f03010d4fd5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/slm-lora-benchmark\n",
            "2025-11-22 11:05:11.781362: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763809511.801935   10857 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763809511.808196   10857 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763809511.824117   10857 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763809511.824146   10857 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763809511.824150   10857 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763809511.824153   10857 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-22 11:05:11.829013: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[2;36m[11/22/25 11:05:17]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Logging to file:                                   \n",
            "\u001b[2;36m                    \u001b[0m         outputs/runs/logs/training_20251122_110517.log     \n",
            "INFO:train:Logging to file: outputs/runs/logs/training_20251122_110517.log\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m ===================================================\n",
            "\u001b[2;36m                    \u001b[0m         =============================                      \n",
            "INFO:train:================================================================================\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m SLM LoRA TRAINING                                  \n",
            "INFO:train:SLM LoRA TRAINING\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m ===================================================\n",
            "\u001b[2;36m                    \u001b[0m         =============================                      \n",
            "INFO:train:================================================================================\n",
            "================================================================================\n",
            "ENVIRONMENT INFORMATION\n",
            "================================================================================\n",
            "Python: 3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]\n",
            "PyTorch: 2.9.0+cu126\n",
            "CUDA available: True\n",
            "CUDA version: 12.6\n",
            "GPU count: 1\n",
            "GPU name: Tesla T4\n",
            "GPU memory: 14.74 GB\n",
            "CPU count: 2\n",
            "================================================================================\n",
            "Random seed set to: 42\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m                                                    \n",
            "\u001b[2;36m                    \u001b[0m         Loading configurations\u001b[33m...\u001b[0m                          \n",
            "INFO:train:\n",
            "Loading configurations...\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Configurations loaded successfully                 \n",
            "INFO:train:Configurations loaded successfully\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m                                                    \n",
            "\u001b[2;36m                    \u001b[0m         Initializing Weights & Biases\u001b[33m...\u001b[0m                   \n",
            "INFO:train:\n",
            "Initializing Weights & Biases...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maryanpawar1515\u001b[0m (\u001b[33maryanpawar1515-mahindra-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m Waiting for wandb.init()...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m Waiting for wandb.init()...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m Waiting for wandb.init()...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m setting up run hrr5g7ws (0.3s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.23.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/slm-lora-benchmark/wandb/run-20251122_110517-hrr5g7ws\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mpythia-410m-tinystories\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aryanpawar1515-mahindra-university/slm-lora-benchmark\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aryanpawar1515-mahindra-university/slm-lora-benchmark/runs/hrr5g7ws\u001b[0m\n",
            "W&B run initialized: pythia-410m-tinystories\n",
            "W&B run URL: https://wandb.ai/aryanpawar1515-mahindra-university/slm-lora-benchmark/runs/hrr5g7ws\n",
            "\u001b[2;36m[11/22/25 11:05:19]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m                                                    \n",
            "\u001b[2;36m                    \u001b[0m         Loading model and tokenizer\u001b[33m...\u001b[0m                     \n",
            "INFO:train:\n",
            "Loading model and tokenizer...\n",
            "Loading tokenizer: EleutherAI/pythia-410m\n",
            "tokenizer_config.json: 100% 396/396 [00:00<00:00, 1.82MB/s]\n",
            "tokenizer.json: 2.11MB [00:00, 61.7MB/s]\n",
            "special_tokens_map.json: 100% 99.0/99.0 [00:00<00:00, 451kB/s]\n",
            "Set pad_token to eos_token: <|endoftext|>\n",
            "Tokenizer loaded: vocab_size=50277\n",
            "Loading model: EleutherAI/pythia-410m\n",
            "config.json: 100% 570/570 [00:00<00:00, 2.49MB/s]\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "model.safetensors: 100% 911M/911M [00:13<00:00, 65.9MB/s]\n",
            "Gradient checkpointing enabled\n",
            "Model loaded: GPTNeoXForCausalLM\n",
            "Resizing token embeddings: 50304 -> 50277\n",
            "\u001b[2;36m[11/22/25 11:05:36]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Model loaded: EleutherAI/pythia-410m               \n",
            "INFO:train:Model loaded: EleutherAI/pythia-410m\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Tokenizer vocab size: \u001b[1;36m50277\u001b[0m                        \n",
            "INFO:train:Tokenizer vocab size: 50277\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m                                                    \n",
            "\u001b[2;36m                    \u001b[0m         Applying LoRA adapters\u001b[33m...\u001b[0m                          \n",
            "INFO:train:\n",
            "Applying LoRA adapters...\n",
            "LoRA Configuration:\n",
            "  Rank (r): 16\n",
            "  Alpha: 32\n",
            "  Dropout: 0.1\n",
            "  Target modules: ['query_key_value']\n",
            "Applying LoRA adapters...\n",
            "trainable params: 1,572,864 || all params: 406,851,584 || trainable%: 0.3866\n",
            "================================================================================\n",
            "TRAINABLE PARAMETERS\n",
            "================================================================================\n",
            "Total parameters: 406,851,584\n",
            "Trainable parameters: 1,572,864\n",
            "Non-trainable parameters: 405,278,720\n",
            "Trainable %: 0.39%\n",
            "================================================================================\n",
            "\u001b[2;36m[11/22/25 11:05:37]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m                                                    \n",
            "\u001b[2;36m                    \u001b[0m         Setting up datasets and dataloaders\u001b[33m...\u001b[0m             \n",
            "INFO:train:\n",
            "Setting up datasets and dataloaders...\n",
            "Setting up data module...\n",
            "\n",
            "Loading dataset: tinystories\n",
            "Loading dataset: roneneldan/TinyStories\n",
            "README.md: 1.06kB [00:00, 1.93MB/s]\n",
            "data/train-00000-of-00004-2d5a1467fff108(…): 100% 249M/249M [00:01<00:00, 145MB/s]\n",
            "data/train-00001-of-00004-5852b56a2bd28f(…): 100% 248M/248M [00:05<00:00, 49.1MB/s]\n",
            "data/train-00002-of-00004-a26307300439e9(…): 100% 246M/246M [00:03<00:00, 77.3MB/s]\n",
            "data/train-00003-of-00004-d243063613e5a0(…): 100% 248M/248M [00:04<00:00, 59.3MB/s]\n",
            "data/validation-00000-of-00001-869c898b5(…): 100% 9.99M/9.99M [00:00<00:00, 26.5MB/s]\n",
            "Generating train split: 100% 2119719/2119719 [00:21<00:00, 99629.19 examples/s] \n",
            "Generating validation split: 100% 21990/21990 [00:00<00:00, 220442.08 examples/s]\n",
            "Dataset loaded: 2119719 samples\n",
            "Limited to 2000 samples\n",
            "Tokenizing dataset: 100% 2000/2000 [00:01<00:00, 1499.47 examples/s]\n",
            "Tokenized dataset: 2048 samples\n",
            "Train dataset: 1843 samples\n",
            "Validation dataset: 205 samples\n",
            "Train batches: 230\n",
            "Validation batches: 26\n",
            "\u001b[2;36m[11/22/25 11:06:16]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Training samples: \u001b[1;36m1843\u001b[0m                             \n",
            "INFO:train:Training samples: 1843\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Validation samples: \u001b[1;36m205\u001b[0m                            \n",
            "INFO:train:Validation samples: 205\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Training batches: \u001b[1;36m230\u001b[0m                              \n",
            "INFO:train:Training batches: 230\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m                                                    \n",
            "\u001b[2;36m                    \u001b[0m         Creating optimizer and scheduler\u001b[33m...\u001b[0m                \n",
            "INFO:train:\n",
            "Creating optimizer and scheduler...\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Optimizer: AdamW                                   \n",
            "INFO:train:Optimizer: AdamW\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Scheduler: cosine                                  \n",
            "INFO:train:Scheduler: cosine\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Total training steps: \u001b[1;36m230\u001b[0m                          \n",
            "INFO:train:Total training steps: 230\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m                                                    \n",
            "\u001b[2;36m                    \u001b[0m         Setting up checkpoint manager\u001b[33m...\u001b[0m                   \n",
            "INFO:train:\n",
            "Setting up checkpoint manager...\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m                                                    \n",
            "\u001b[2;36m                    \u001b[0m         Using device: cuda                                 \n",
            "INFO:train:\n",
            "Using device: cuda\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m                                                    \n",
            "\u001b[2;36m                    \u001b[0m         ===================================================\n",
            "\u001b[2;36m                    \u001b[0m         =============================                      \n",
            "INFO:train:\n",
            "================================================================================\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m STARTING TRAINING                                  \n",
            "INFO:train:STARTING TRAINING\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m ===================================================\n",
            "\u001b[2;36m                    \u001b[0m         =============================                      \n",
            "\u001b[2;36m                    \u001b[0m                                                            \n",
            "INFO:train:================================================================================\n",
            "\n",
            "Starting training...\n",
            "\n",
            "================================================================================\n",
            "Epoch 1/1\n",
            "================================================================================\n",
            "Training Epoch 1:   0% 0/230 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Training Epoch 1: 100% 230/230 [10:34<00:00,  2.76s/it, loss=1.8395, ppl=6.29, lr=5.70e-05, samples=1840]\n",
            "Validating:   0% 0/26 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Validating: 100% 26/26 [00:23<00:00,  1.09it/s]\n",
            "\n",
            "Validation - Loss: 1.8655, PPL: 6.46, BPT: 2.6914\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 57 that is less than the current step 58. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
            "Checkpoint saved: outputs/runs/checkpoints/checkpoint_epoch0_step57_20251122_111714.pt\n",
            "Best checkpoint updated: outputs/runs/checkpoints/best.pt\n",
            "\n",
            "Training completed in 0.20 hours\n",
            "\u001b[2;36m[11/22/25 11:18:07]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m                                                    \n",
            "\u001b[2;36m                    \u001b[0m         ===================================================\n",
            "\u001b[2;36m                    \u001b[0m         =============================                      \n",
            "INFO:train:\n",
            "================================================================================\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m TRAINING COMPLETED                                 \n",
            "INFO:train:TRAINING COMPLETED\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m ===================================================\n",
            "\u001b[2;36m                    \u001b[0m         =============================                      \n",
            "INFO:train:================================================================================\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Best validation loss: \u001b[1;36m1.8655\u001b[0m                       \n",
            "INFO:train:Best validation loss: 1.8655\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Total training time: \u001b[1;36m0.20\u001b[0m hours                    \n",
            "INFO:train:Total training time: 0.20 hours\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Best checkpoint:                                   \n",
            "\u001b[2;36m                    \u001b[0m         outputs/runs/checkpoints/checkpoint_epoch0_step57_2\n",
            "\u001b[2;36m                    \u001b[0m         0251122_111714.pt                                  \n",
            "INFO:train:Best checkpoint: outputs/runs/checkpoints/checkpoint_epoch0_step57_20251122_111714.pt\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m updating run metadata (0.0s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m updating run metadata (0.0s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m updating run metadata (0.0s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m updating run metadata (0.0s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m uploading summary, console lines 113-133 (0.0s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                       train/bpt ▇██▁▅\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/ema_token_loss ▇█▅▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/epoch ▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 train/grad_norm ▄▁▂█▃\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 train/lora_rank ▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                        train/lr ▁▃▅▆█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                       train/ppl ▇██▁▅\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      train/step ▁▃▅▆█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 train/step_time ▁█▅▅▆\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train/throughput_tokens_per_sec ▂▂█▁▅\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                              +5 ...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                       train/bpt 2.86837\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/ema_token_loss 1.92887\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/epoch 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 train/grad_norm 0.89823\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 train/lora_rank 16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                        train/lr 5e-05\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                       train/ppl 7.30238\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      train/step 50\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 train/step_time 2.81829\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train/throughput_tokens_per_sec 628.39534\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                              +5 ...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mpythia-410m-tinystories\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/aryanpawar1515-mahindra-university/slm-lora-benchmark/runs/hrr5g7ws\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/aryanpawar1515-mahindra-university/slm-lora-benchmark\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20251122_110517-hrr5g7ws/logs\u001b[0m\n",
            "W&B run finished\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m                                                    \n",
            "\u001b[2;36m                    \u001b[0m         Training script completed successfully!            \n",
            "INFO:train:\n",
            "Training script completed successfully!\n",
            "2025-11-22 11:19:08.630148: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763810348.677950   14452 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763810348.691918   14452 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763810348.724740   14452 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763810348.724781   14452 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763810348.724785   14452 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763810348.724789   14452 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-22 11:19:08.735292: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[2;36m[11/22/25 11:19:21]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Logging to file:                                   \n",
            "\u001b[2;36m                    \u001b[0m         outputs/runs/logs/training_20251122_111921.log     \n",
            "INFO:train:Logging to file: outputs/runs/logs/training_20251122_111921.log\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m ===================================================\n",
            "\u001b[2;36m                    \u001b[0m         =============================                      \n",
            "INFO:train:================================================================================\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m SLM LoRA TRAINING                                  \n",
            "INFO:train:SLM LoRA TRAINING\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m ===================================================\n",
            "\u001b[2;36m                    \u001b[0m         =============================                      \n",
            "INFO:train:================================================================================\n",
            "================================================================================\n",
            "ENVIRONMENT INFORMATION\n",
            "================================================================================\n",
            "Python: 3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]\n",
            "PyTorch: 2.9.0+cu126\n",
            "CUDA available: True\n",
            "CUDA version: 12.6\n",
            "GPU count: 1\n",
            "GPU name: Tesla T4\n",
            "GPU memory: 14.74 GB\n",
            "CPU count: 2\n",
            "================================================================================\n",
            "Random seed set to: 42\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m                                                    \n",
            "\u001b[2;36m                    \u001b[0m         Loading configurations\u001b[33m...\u001b[0m                          \n",
            "INFO:train:\n",
            "Loading configurations...\n",
            "\u001b[2;36m[11/22/25 11:19:22]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Configurations loaded successfully                 \n",
            "INFO:train:Configurations loaded successfully\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m                                                    \n",
            "\u001b[2;36m                    \u001b[0m         Initializing Weights & Biases\u001b[33m...\u001b[0m                   \n",
            "INFO:train:\n",
            "Initializing Weights & Biases...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maryanpawar1515\u001b[0m (\u001b[33maryanpawar1515-mahindra-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m Waiting for wandb.init()...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m Waiting for wandb.init()...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m Waiting for wandb.init()...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.23.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/slm-lora-benchmark/wandb/run-20251122_111922-txpy0k4n\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mpythia-410m-agnews\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aryanpawar1515-mahindra-university/slm-lora-benchmark\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aryanpawar1515-mahindra-university/slm-lora-benchmark/runs/txpy0k4n\u001b[0m\n",
            "W&B run initialized: pythia-410m-agnews\n",
            "W&B run URL: https://wandb.ai/aryanpawar1515-mahindra-university/slm-lora-benchmark/runs/txpy0k4n\n",
            "\u001b[2;36m[11/22/25 11:19:23]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m                                                    \n",
            "\u001b[2;36m                    \u001b[0m         Loading model and tokenizer\u001b[33m...\u001b[0m                     \n",
            "INFO:train:\n",
            "Loading model and tokenizer...\n",
            "Loading tokenizer: EleutherAI/pythia-410m\n",
            "Set pad_token to eos_token: <|endoftext|>\n",
            "Tokenizer loaded: vocab_size=50277\n",
            "Loading model: EleutherAI/pythia-410m\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Gradient checkpointing enabled\n",
            "Model loaded: GPTNeoXForCausalLM\n",
            "Resizing token embeddings: 50304 -> 50277\n",
            "\u001b[2;36m[11/22/25 11:19:28]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Model loaded: EleutherAI/pythia-410m               \n",
            "INFO:train:Model loaded: EleutherAI/pythia-410m\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Tokenizer vocab size: \u001b[1;36m50277\u001b[0m                        \n",
            "INFO:train:Tokenizer vocab size: 50277\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m                                                    \n",
            "\u001b[2;36m                    \u001b[0m         Applying LoRA adapters\u001b[33m...\u001b[0m                          \n",
            "INFO:train:\n",
            "Applying LoRA adapters...\n",
            "LoRA Configuration:\n",
            "  Rank (r): 16\n",
            "  Alpha: 32\n",
            "  Dropout: 0.1\n",
            "  Target modules: ['query_key_value']\n",
            "Applying LoRA adapters...\n",
            "trainable params: 1,572,864 || all params: 406,851,584 || trainable%: 0.3866\n",
            "================================================================================\n",
            "TRAINABLE PARAMETERS\n",
            "================================================================================\n",
            "Total parameters: 406,851,584\n",
            "Trainable parameters: 1,572,864\n",
            "Non-trainable parameters: 405,278,720\n",
            "Trainable %: 0.39%\n",
            "================================================================================\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m                                                    \n",
            "\u001b[2;36m                    \u001b[0m         Setting up datasets and dataloaders\u001b[33m...\u001b[0m             \n",
            "INFO:train:\n",
            "Setting up datasets and dataloaders...\n",
            "Setting up data module...\n",
            "\n",
            "Loading dataset: ag_news\n",
            "Loading dataset: ag_news\n",
            "README.md: 8.07kB [00:00, 17.3MB/s]\n",
            "data/train-00000-of-00001.parquet: 100% 18.6M/18.6M [00:00<00:00, 22.2MB/s]\n",
            "data/test-00000-of-00001.parquet: 100% 1.23M/1.23M [00:00<00:00, 2.05MB/s]\n",
            "Generating train split: 100% 120000/120000 [00:00<00:00, 760208.37 examples/s]\n",
            "Generating test split: 100% 7600/7600 [00:00<00:00, 645931.32 examples/s]\n",
            "Dataset loaded: 120000 samples\n",
            "Limited to 2000 samples\n",
            "Tokenizing dataset: 100% 2000/2000 [00:00<00:00, 2745.73 examples/s]\n",
            "Tokenized dataset: 2000 samples\n",
            "Train dataset: 1800 samples\n",
            "Validation dataset: 200 samples\n",
            "Train batches: 225\n",
            "Validation batches: 25\n",
            "\u001b[2;36m[11/22/25 11:19:33]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Training samples: \u001b[1;36m1800\u001b[0m                             \n",
            "INFO:train:Training samples: 1800\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Validation samples: \u001b[1;36m200\u001b[0m                            \n",
            "INFO:train:Validation samples: 200\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Training batches: \u001b[1;36m225\u001b[0m                              \n",
            "INFO:train:Training batches: 225\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m                                                    \n",
            "\u001b[2;36m                    \u001b[0m         Creating optimizer and scheduler\u001b[33m...\u001b[0m                \n",
            "INFO:train:\n",
            "Creating optimizer and scheduler...\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Optimizer: AdamW                                   \n",
            "INFO:train:Optimizer: AdamW\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Scheduler: cosine                                  \n",
            "INFO:train:Scheduler: cosine\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Total training steps: \u001b[1;36m225\u001b[0m                          \n",
            "INFO:train:Total training steps: 225\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m                                                    \n",
            "\u001b[2;36m                    \u001b[0m         Setting up checkpoint manager\u001b[33m...\u001b[0m                   \n",
            "INFO:train:\n",
            "Setting up checkpoint manager...\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m                                                    \n",
            "\u001b[2;36m                    \u001b[0m         Using device: cuda                                 \n",
            "INFO:train:\n",
            "Using device: cuda\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m                                                    \n",
            "\u001b[2;36m                    \u001b[0m         ===================================================\n",
            "\u001b[2;36m                    \u001b[0m         =============================                      \n",
            "INFO:train:\n",
            "================================================================================\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m STARTING TRAINING                                  \n",
            "INFO:train:STARTING TRAINING\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m ===================================================\n",
            "\u001b[2;36m                    \u001b[0m         =============================                      \n",
            "\u001b[2;36m                    \u001b[0m                                                            \n",
            "INFO:train:================================================================================\n",
            "\n",
            "Starting training...\n",
            "\n",
            "================================================================================\n",
            "Epoch 1/1\n",
            "================================================================================\n",
            "Training Epoch 1:   0% 0/225 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Training Epoch 1: 100% 225/225 [10:15<00:00,  2.74s/it, loss=3.3608, ppl=28.81, lr=5.60e-05, samples=1800]\n",
            "Validating:   0% 0/25 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Validating: 100% 25/25 [00:23<00:00,  1.08it/s]\n",
            "\n",
            "Validation - Loss: 3.4194, PPL: 30.55, BPT: 4.9331\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 56 that is less than the current step 57. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
            "Checkpoint saved: outputs/runs/checkpoints/checkpoint_epoch0_step56_20251122_113012.pt\n",
            "Best checkpoint updated: outputs/runs/checkpoints/best.pt\n",
            "\n",
            "Training completed in 0.19 hours\n",
            "\u001b[2;36m[11/22/25 11:30:50]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m                                                    \n",
            "\u001b[2;36m                    \u001b[0m         ===================================================\n",
            "\u001b[2;36m                    \u001b[0m         =============================                      \n",
            "INFO:train:\n",
            "================================================================================\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m TRAINING COMPLETED                                 \n",
            "INFO:train:TRAINING COMPLETED\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m ===================================================\n",
            "\u001b[2;36m                    \u001b[0m         =============================                      \n",
            "INFO:train:================================================================================\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Best validation loss: \u001b[1;36m3.4194\u001b[0m                       \n",
            "INFO:train:Best validation loss: 3.4194\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Total training time: \u001b[1;36m0.19\u001b[0m hours                    \n",
            "INFO:train:Total training time: 0.19 hours\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Best checkpoint:                                   \n",
            "\u001b[2;36m                    \u001b[0m         outputs/runs/checkpoints/checkpoint_epoch0_step56_2\n",
            "\u001b[2;36m                    \u001b[0m         0251122_113012.pt                                  \n",
            "INFO:train:Best checkpoint: outputs/runs/checkpoints/checkpoint_epoch0_step56_20251122_113012.pt\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m updating run metadata (0.0s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m updating run metadata (0.0s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m updating run metadata (0.0s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m updating run metadata (0.0s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m uploading config.yaml 7.5KB/7.5KB (0.2s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣯\u001b[0m uploading config.yaml 7.5KB/7.5KB (0.2s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                       train/bpt ██▂▁▃\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/ema_token_loss ██▆▄▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/epoch ▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 train/grad_norm █▁▅▄▇\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 train/lora_rank ▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                        train/lr ▁▃▅▆█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                       train/ppl ██▁▁▃\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      train/step ▁▃▅▆█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 train/step_time ▁█▅▇▆\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train/throughput_tokens_per_sec ▃▁▄█▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                              +5 ...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                       train/bpt 5.10352\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/ema_token_loss 3.47986\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/epoch 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 train/grad_norm 1.80177\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 train/lora_rank 16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                        train/lr 5e-05\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                       train/ppl 34.38059\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      train/step 50\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 train/step_time 2.75062\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train/throughput_tokens_per_sec 129.78891\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                              +5 ...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mpythia-410m-agnews\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/aryanpawar1515-mahindra-university/slm-lora-benchmark/runs/txpy0k4n\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/aryanpawar1515-mahindra-university/slm-lora-benchmark\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20251122_111922-txpy0k4n/logs\u001b[0m\n",
            "W&B run finished\n",
            "\u001b[2;36m[11/22/25 11:30:51]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m                                                    \n",
            "\u001b[2;36m                    \u001b[0m         Training script completed successfully!            \n",
            "INFO:train:\n",
            "Training script completed successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r outputs/ /content/drive/MyDrive/slm-lora-backup/final_outputs/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "SjnCEurhV_H_",
        "outputId": "ae7d7491-cdc8-436f-e0ce-0aa8d2d5e952"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3122755610.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cp -r outputs/ /content/drive/MyDrive/slm-lora-backup/final_outputs/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    147\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'also_return_output'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_system_commands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_system_compat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint:disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpip_warn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_system_compat\u001b[0;34m(shell, cmd, also_return_output)\u001b[0m\n\u001b[1;32m    452\u001b[0m   \u001b[0;31m# is expected to call this function, thus adding one level of nesting to the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m   \u001b[0;31m# stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m   result = _run_command(\n\u001b[0m\u001b[1;32m    455\u001b[0m       \u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclear_streamed_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_run_command\u001b[0;34m(cmd, clear_streamed_output)\u001b[0m\n\u001b[1;32m    202\u001b[0m       \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild_pty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0m_monitor_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_pty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_stdin_widget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0mepoll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_monitor_process\u001b[0;34m(parent_pty, epoll, p, cmd, update_stdin_widget)\u001b[0m\n\u001b[1;32m    232\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_poll_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_pty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_poll_process\u001b[0;34m(parent_pty, epoll, p, cmd, decoder, state)\u001b[0m\n\u001b[1;32m    335\u001b[0m     \u001b[0;31m# TODO(b/115527726): Rather than sleep, poll for incoming messages from\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[0;31m# the frontend in the same poll as for the output.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create backup directory in Colab\n",
        "!mkdir -p /content/backups\n",
        "\n",
        "# 1. WikiText-2\n",
        "!python scripts/train.py \\\n",
        "    --dataset_names wikitext_2 \\\n",
        "    --wandb_name \"pythia-410m-wikitext2\"\n",
        "\n",
        "!cp -r outputs/runs/checkpoints/ /content/backups/wikitext2_checkpoints/\n",
        "\n",
        "# 2. Yelp Reviews\n",
        "!python scripts/train.py \\\n",
        "    --dataset_names yelp_reviews \\\n",
        "    --wandb_name \"pythia-410m-yelp\"\n",
        "\n",
        "!cp -r outputs/runs/checkpoints/ /content/backups/yelp_checkpoints/\n",
        "\n",
        "# 3. SQuAD\n",
        "!python scripts/train.py \\\n",
        "    --dataset_names squad \\\n",
        "    --wandb_name \"pythia-410m-squad\"\n",
        "\n",
        "!cp -r outputs/runs/checkpoints/ /content/backups/squad_checkpoints/\n",
        "\n",
        "# Save all metrics at the end\n",
        "!ls -lh /content/backups/*/best.pt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0XkwuaGXwkQ",
        "outputId": "7168411e-6b28-410a-e3c2-869545368503"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 1331, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 935, in _load_unlocked\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 999, in exec_module\n",
            "  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/onnx/_internal/torchscript_exporter/symbolic_helper.py\", line 113, in <module>\n",
            "    from torch.onnx._internal.torchscript_exporter import _type_utils, jit_utils, utils\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/onnx/_internal/torchscript_exporter/jit_utils.py\", line 14, in <module>\n",
            "    from torch.onnx._internal.torchscript_exporter import registration\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/onnx/_internal/torchscript_exporter/registration.py\", line 9, in <module>\n",
            "    from torch.onnx import _constants, errors\n",
            "  File \"<frozen importlib._bootstrap>\", line 471, in _lock_unlock_module\n",
            "  File \"<frozen importlib._bootstrap>\", line 311, in acquire\n",
            "  File \"<frozen importlib._bootstrap>\", line 158, in __init__\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/slm-lora-benchmark/scripts/train.py\", line 20, in <module>\n",
            "    from src.models.model_factory import ModelFactory\n",
            "  File \"/content/slm-lora-benchmark/src/models/__init__.py\", line 3, in <module>\n",
            "    from .model_factory import ModelFactory, load_model_and_tokenizer\n",
            "  File \"/content/slm-lora-benchmark/src/models/model_factory.py\", line 5, in <module>\n",
            "    from transformers import (\n",
            "  File \"<frozen importlib._bootstrap>\", line 1412, in _handle_fromlist\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py\", line 2317, in __getattr__\n",
            "    module = self._get_module(self._class_to_module[name])\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py\", line 2345, in _get_module\n",
            "    return importlib.import_module(\".\" + module_name, self.__name__)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/importlib/__init__.py\", line 90, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/models/auto/modeling_auto.py\", line 23, in <module>\n",
            "    from .auto_factory import (\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/models/auto/auto_factory.py\", line 43, in <module>\n",
            "    from ...generation import GenerationMixin\n",
            "  File \"<frozen importlib._bootstrap>\", line 1412, in _handle_fromlist\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py\", line 2317, in __getattr__\n",
            "    module = self._get_module(self._class_to_module[name])\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py\", line 2345, in _get_module\n",
            "    return importlib.import_module(\".\" + module_name, self.__name__)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/importlib/__init__.py\", line 90, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\", line 43, in <module>\n",
            "    from ..masking_utils import create_masks_for_generate\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/masking_utils.py\", line 40, in <module>\n",
            "    from torch._dynamo._trace_wrapped_higher_order_op import TransformGetItemToIndex\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/__init__.py\", line 70, in <module>\n",
            "    from .polyfills import loader as _  # usort: skip # noqa: F401\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/polyfills/loader.py\", line 28, in <module>\n",
            "    POLYFILLED_MODULES: tuple[\"ModuleType\", ...] = tuple(\n",
            "                                                   ^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/polyfills/loader.py\", line 29, in <genexpr>\n",
            "    importlib.import_module(f\".{submodule}\", package=polyfills.__name__)\n",
            "  File \"/usr/lib/python3.12/importlib/__init__.py\", line 90, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/polyfills/_collections.py\", line 20, in <module>\n",
            "    @substitute_in_graph(_collections._count_elements)\n",
            "     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/decorators.py\", line 461, in wrapper\n",
            "    rule_map: dict[Any, type[VariableTracker]] = get_torch_obj_rule_map()\n",
            "                                                 ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/trace_rules.py\", line 2996, in get_torch_obj_rule_map\n",
            "    obj = load_object(k)\n",
            "          ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/trace_rules.py\", line 3032, in load_object\n",
            "    val = _load_obj_from_str(x[0])\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/trace_rules.py\", line 3016, in _load_obj_from_str\n",
            "    return getattr(importlib.import_module(module), obj_name)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/importlib/__init__.py\", line 90, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/onnx/__init__.py\", line 29, in <module>\n",
            "    from ._internal.torchscript_exporter import (  # Deprecated members that are excluded from __all__\n",
            "  File \"<frozen importlib._bootstrap>\", line 1357, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 420, in __exit__\n",
            "KeyboardInterrupt\n",
            "2025-11-22 12:33:20.646768: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763814800.690827   33674 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763814800.703868   33674 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763814800.756336   33674 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763814800.756384   33674 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763814800.756391   33674 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763814800.756398   33674 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "\u001b[2;36m[11/22/25 12:33:34]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Logging to file:                                   \n",
            "\u001b[2;36m                    \u001b[0m         outputs/runs/logs/training_20251122_123334.log     \n",
            "INFO:train:Logging to file: outputs/runs/logs/training_20251122_123334.log\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m ===================================================\n",
            "\u001b[2;36m                    \u001b[0m         =============================                      \n",
            "INFO:train:================================================================================\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m SLM LoRA TRAINING                                  \n",
            "INFO:train:SLM LoRA TRAINING\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m ===================================================\n",
            "\u001b[2;36m                    \u001b[0m         =============================                      \n",
            "INFO:train:================================================================================\n",
            "================================================================================\n",
            "ENVIRONMENT INFORMATION\n",
            "================================================================================\n",
            "Python: 3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]\n",
            "PyTorch: 2.9.0+cu126\n",
            "CUDA available: True\n",
            "CUDA version: 12.6\n",
            "GPU count: 1\n",
            "GPU name: Tesla T4\n",
            "GPU memory: 14.74 GB\n",
            "CPU count: 2\n",
            "================================================================================\n",
            "Random seed set to: 42\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m                                                    \n",
            "\u001b[2;36m                    \u001b[0m         Loading configurations\u001b[33m...\u001b[0m                          \n",
            "INFO:train:\n",
            "Loading configurations...\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Configurations loaded successfully                 \n",
            "INFO:train:Configurations loaded successfully\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m                                                    \n",
            "\u001b[2;36m                    \u001b[0m         Initializing Weights & Biases\u001b[33m...\u001b[0m                   \n",
            "INFO:train:\n",
            "Initializing Weights & Biases...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maryanpawar1515\u001b[0m (\u001b[33maryanpawar1515-mahindra-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m Waiting for wandb.init()...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m Waiting for wandb.init()...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m Waiting for wandb.init()...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.23.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/slm-lora-benchmark/wandb/run-20251122_123334-6uhazupm\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mpythia-410m-yelp\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aryanpawar1515-mahindra-university/slm-lora-benchmark\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aryanpawar1515-mahindra-university/slm-lora-benchmark/runs/6uhazupm\u001b[0m\n",
            "W&B run initialized: pythia-410m-yelp\n",
            "W&B run URL: https://wandb.ai/aryanpawar1515-mahindra-university/slm-lora-benchmark/runs/6uhazupm\n",
            "\u001b[2;36m[11/22/25 12:33:36]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m                                                    \n",
            "\u001b[2;36m                    \u001b[0m         Loading model and tokenizer\u001b[33m...\u001b[0m                     \n",
            "INFO:train:\n",
            "Loading model and tokenizer...\n",
            "Loading tokenizer: EleutherAI/pythia-410m\n",
            "Set pad_token to eos_token: <|endoftext|>\n",
            "Tokenizer loaded: vocab_size=50277\n",
            "Loading model: EleutherAI/pythia-410m\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Gradient checkpointing enabled\n",
            "Model loaded: GPTNeoXForCausalLM\n",
            "Resizing token embeddings: 50304 -> 50277\n",
            "\u001b[2;36m[11/22/25 12:33:41]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Model loaded: EleutherAI/pythia-410m               \n",
            "INFO:train:Model loaded: EleutherAI/pythia-410m\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Tokenizer vocab size: \u001b[1;36m50277\u001b[0m                        \n",
            "INFO:train:Tokenizer vocab size: 50277\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m                                                    \n",
            "\u001b[2;36m                    \u001b[0m         Applying LoRA adapters\u001b[33m...\u001b[0m                          \n",
            "INFO:train:\n",
            "Applying LoRA adapters...\n",
            "LoRA Configuration:\n",
            "  Rank (r): 16\n",
            "  Alpha: 32\n",
            "  Dropout: 0.1\n",
            "  Target modules: ['query_key_value']\n",
            "Applying LoRA adapters...\n",
            "trainable params: 1,572,864 || all params: 406,851,584 || trainable%: 0.3866\n",
            "================================================================================\n",
            "TRAINABLE PARAMETERS\n",
            "================================================================================\n",
            "Total parameters: 406,851,584\n",
            "Trainable parameters: 1,572,864\n",
            "Non-trainable parameters: 405,278,720\n",
            "Trainable %: 0.39%\n",
            "================================================================================\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m                                                    \n",
            "\u001b[2;36m                    \u001b[0m         Setting up datasets and dataloaders\u001b[33m...\u001b[0m             \n",
            "INFO:train:\n",
            "Setting up datasets and dataloaders...\n",
            "Setting up data module...\n",
            "\n",
            "Loading dataset: yelp_reviews\n",
            "Loading dataset: yelp_review_full\n",
            "Dataset loaded: 650000 samples\n",
            "Limited to 1000 samples\n",
            "Tokenized dataset: 1061 samples\n",
            "Train dataset: 954 samples\n",
            "Validation dataset: 107 samples\n",
            "Train batches: 119\n",
            "Validation batches: 14\n",
            "\u001b[2;36m[11/22/25 12:33:45]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Training samples: \u001b[1;36m954\u001b[0m                              \n",
            "INFO:train:Training samples: 954\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Validation samples: \u001b[1;36m107\u001b[0m                            \n",
            "INFO:train:Validation samples: 107\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Training batches: \u001b[1;36m119\u001b[0m                              \n",
            "INFO:train:Training batches: 119\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m                                                    \n",
            "\u001b[2;36m                    \u001b[0m         Creating optimizer and scheduler\u001b[33m...\u001b[0m                \n",
            "INFO:train:\n",
            "Creating optimizer and scheduler...\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Optimizer: AdamW                                   \n",
            "INFO:train:Optimizer: AdamW\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Scheduler: cosine                                  \n",
            "INFO:train:Scheduler: cosine\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Total training steps: \u001b[1;36m119\u001b[0m                          \n",
            "INFO:train:Total training steps: 119\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m                                                    \n",
            "\u001b[2;36m                    \u001b[0m         Setting up checkpoint manager\u001b[33m...\u001b[0m                   \n",
            "INFO:train:\n",
            "Setting up checkpoint manager...\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m                                                    \n",
            "\u001b[2;36m                    \u001b[0m         Using device: cuda                                 \n",
            "INFO:train:\n",
            "Using device: cuda\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m                                                    \n",
            "\u001b[2;36m                    \u001b[0m         ===================================================\n",
            "\u001b[2;36m                    \u001b[0m         =============================                      \n",
            "INFO:train:\n",
            "================================================================================\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m STARTING TRAINING                                  \n",
            "INFO:train:STARTING TRAINING\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m ===================================================\n",
            "\u001b[2;36m                    \u001b[0m         =============================                      \n",
            "\u001b[2;36m                    \u001b[0m                                                            \n",
            "INFO:train:================================================================================\n",
            "\n",
            "Starting training...\n",
            "\n",
            "================================================================================\n",
            "Epoch 1/1\n",
            "================================================================================\n",
            "Training Epoch 1: 100% 119/119 [05:34<00:00,  2.81s/it, loss=2.9277, ppl=18.68, lr=2.90e-05, samples=952]\n",
            "Validating: 100% 14/14 [00:12<00:00,  1.09it/s]\n",
            "\n",
            "Validation - Loss: 3.2348, PPL: 25.40, BPT: 4.6669\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 29 that is less than the current step 30. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
            "Checkpoint saved: outputs/runs/checkpoints/checkpoint_epoch0_step29_20251122_123932.pt\n",
            "Best checkpoint updated: outputs/runs/checkpoints/best.pt\n",
            "\n",
            "Training completed in 0.11 hours\n",
            "\u001b[2;36m[11/22/25 12:40:15]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m                                                    \n",
            "\u001b[2;36m                    \u001b[0m         ===================================================\n",
            "\u001b[2;36m                    \u001b[0m         =============================                      \n",
            "INFO:train:\n",
            "================================================================================\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m TRAINING COMPLETED                                 \n",
            "INFO:train:TRAINING COMPLETED\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m ===================================================\n",
            "\u001b[2;36m                    \u001b[0m         =============================                      \n",
            "INFO:train:================================================================================\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Best validation loss: \u001b[1;36m3.2348\u001b[0m                       \n",
            "INFO:train:Best validation loss: 3.2348\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Total training time: \u001b[1;36m0.11\u001b[0m hours                    \n",
            "INFO:train:Total training time: 0.11 hours\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Best checkpoint:                                   \n",
            "\u001b[2;36m                    \u001b[0m         outputs/runs/checkpoints/checkpoint_epoch0_step29_2\n",
            "\u001b[2;36m                    \u001b[0m         0251122_123932.pt                                  \n",
            "INFO:train:Best checkpoint: outputs/runs/checkpoints/checkpoint_epoch0_step29_20251122_123932.pt\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m updating run metadata (0.0s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m updating run metadata (0.0s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m updating run metadata (0.0s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m updating run metadata (0.0s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m uploading summary, console lines 99-119 (0.1s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                       train/bpt ▁█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/ema_token_loss ▁█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/epoch ▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 train/grad_norm ▁█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 train/lora_rank ▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                        train/lr ▁█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                       train/ppl ▁█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      train/step ▁█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 train/step_time █▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train/throughput_tokens_per_sec █▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                              +5 ...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                       train/bpt 4.86107\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/ema_token_loss 3.25222\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/epoch 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 train/grad_norm 0.92221\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 train/lora_rank 16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                        train/lr 2e-05\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                       train/ppl 29.06211\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      train/step 20\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 train/step_time 2.79762\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train/throughput_tokens_per_sec 467.18273\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                              +5 ...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mpythia-410m-yelp\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/aryanpawar1515-mahindra-university/slm-lora-benchmark/runs/6uhazupm\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/aryanpawar1515-mahindra-university/slm-lora-benchmark\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20251122_123334-6uhazupm/logs\u001b[0m\n",
            "W&B run finished\n",
            "\u001b[2;36m[11/22/25 12:40:16]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m                                                    \n",
            "\u001b[2;36m                    \u001b[0m         Training script completed successfully!            \n",
            "INFO:train:\n",
            "Training script completed successfully!\n",
            "2025-11-22 12:42:56.075488: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763815376.104056   36125 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763815376.114175   36125 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763815376.159850   36125 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763815376.159879   36125 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763815376.159884   36125 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763815376.159888   36125 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "\u001b[2;36m[11/22/25 12:43:04]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Logging to file:                                   \n",
            "\u001b[2;36m                    \u001b[0m         outputs/runs/logs/training_20251122_124304.log     \n",
            "INFO:train:Logging to file: outputs/runs/logs/training_20251122_124304.log\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m ===================================================\n",
            "\u001b[2;36m                    \u001b[0m         =============================                      \n",
            "INFO:train:================================================================================\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m SLM LoRA TRAINING                                  \n",
            "INFO:train:SLM LoRA TRAINING\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m ===================================================\n",
            "\u001b[2;36m                    \u001b[0m         =============================                      \n",
            "INFO:train:================================================================================\n",
            "================================================================================\n",
            "ENVIRONMENT INFORMATION\n",
            "================================================================================\n",
            "Python: 3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]\n",
            "PyTorch: 2.9.0+cu126\n",
            "CUDA available: True\n",
            "CUDA version: 12.6\n",
            "GPU count: 1\n",
            "GPU name: Tesla T4\n",
            "GPU memory: 14.74 GB\n",
            "CPU count: 2\n",
            "================================================================================\n",
            "Random seed set to: 42\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m                                                    \n",
            "\u001b[2;36m                    \u001b[0m         Loading configurations\u001b[33m...\u001b[0m                          \n",
            "INFO:train:\n",
            "Loading configurations...\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Configurations loaded successfully                 \n",
            "INFO:train:Configurations loaded successfully\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m                                                    \n",
            "\u001b[2;36m                    \u001b[0m         Initializing Weights & Biases\u001b[33m...\u001b[0m                   \n",
            "INFO:train:\n",
            "Initializing Weights & Biases...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maryanpawar1515\u001b[0m (\u001b[33maryanpawar1515-mahindra-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m Waiting for wandb.init()...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m Waiting for wandb.init()...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m Waiting for wandb.init()...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.23.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/slm-lora-benchmark/wandb/run-20251122_124304-wp2wo1iq\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mpythia-410m-squad\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aryanpawar1515-mahindra-university/slm-lora-benchmark\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aryanpawar1515-mahindra-university/slm-lora-benchmark/runs/wp2wo1iq\u001b[0m\n",
            "W&B run initialized: pythia-410m-squad\n",
            "W&B run URL: https://wandb.ai/aryanpawar1515-mahindra-university/slm-lora-benchmark/runs/wp2wo1iq\n",
            "\u001b[2;36m[11/22/25 12:43:06]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m                                                    \n",
            "\u001b[2;36m                    \u001b[0m         Loading model and tokenizer\u001b[33m...\u001b[0m                     \n",
            "INFO:train:\n",
            "Loading model and tokenizer...\n",
            "Loading tokenizer: EleutherAI/pythia-410m\n",
            "Set pad_token to eos_token: <|endoftext|>\n",
            "Tokenizer loaded: vocab_size=50277\n",
            "Loading model: EleutherAI/pythia-410m\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Gradient checkpointing enabled\n",
            "Model loaded: GPTNeoXForCausalLM\n",
            "Resizing token embeddings: 50304 -> 50277\n",
            "\u001b[2;36m[11/22/25 12:43:11]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Model loaded: EleutherAI/pythia-410m               \n",
            "INFO:train:Model loaded: EleutherAI/pythia-410m\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Tokenizer vocab size: \u001b[1;36m50277\u001b[0m                        \n",
            "INFO:train:Tokenizer vocab size: 50277\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m                                                    \n",
            "\u001b[2;36m                    \u001b[0m         Applying LoRA adapters\u001b[33m...\u001b[0m                          \n",
            "INFO:train:\n",
            "Applying LoRA adapters...\n",
            "LoRA Configuration:\n",
            "  Rank (r): 16\n",
            "  Alpha: 32\n",
            "  Dropout: 0.1\n",
            "  Target modules: ['query_key_value']\n",
            "Applying LoRA adapters...\n",
            "trainable params: 1,572,864 || all params: 406,851,584 || trainable%: 0.3866\n",
            "================================================================================\n",
            "TRAINABLE PARAMETERS\n",
            "================================================================================\n",
            "Total parameters: 406,851,584\n",
            "Trainable parameters: 1,572,864\n",
            "Non-trainable parameters: 405,278,720\n",
            "Trainable %: 0.39%\n",
            "================================================================================\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m                                                    \n",
            "\u001b[2;36m                    \u001b[0m         Setting up datasets and dataloaders\u001b[33m...\u001b[0m             \n",
            "INFO:train:\n",
            "Setting up datasets and dataloaders...\n",
            "Setting up data module...\n",
            "\n",
            "Loading dataset: squad\n",
            "Loading dataset: squad\n",
            "Dataset loaded: 87599 samples\n",
            "Limited to 1000 samples\n",
            "Tokenized dataset: 1000 samples\n",
            "Train dataset: 900 samples\n",
            "Validation dataset: 100 samples\n",
            "Train batches: 112\n",
            "Validation batches: 13\n",
            "\u001b[2;36m[11/22/25 12:43:13]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Training samples: \u001b[1;36m900\u001b[0m                              \n",
            "INFO:train:Training samples: 900\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Validation samples: \u001b[1;36m100\u001b[0m                            \n",
            "INFO:train:Validation samples: 100\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Training batches: \u001b[1;36m112\u001b[0m                              \n",
            "INFO:train:Training batches: 112\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m                                                    \n",
            "\u001b[2;36m                    \u001b[0m         Creating optimizer and scheduler\u001b[33m...\u001b[0m                \n",
            "INFO:train:\n",
            "Creating optimizer and scheduler...\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Optimizer: AdamW                                   \n",
            "INFO:train:Optimizer: AdamW\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Scheduler: cosine                                  \n",
            "INFO:train:Scheduler: cosine\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Total training steps: \u001b[1;36m112\u001b[0m                          \n",
            "INFO:train:Total training steps: 112\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m                                                    \n",
            "\u001b[2;36m                    \u001b[0m         Setting up checkpoint manager\u001b[33m...\u001b[0m                   \n",
            "INFO:train:\n",
            "Setting up checkpoint manager...\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m                                                    \n",
            "\u001b[2;36m                    \u001b[0m         Using device: cuda                                 \n",
            "INFO:train:\n",
            "Using device: cuda\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m                                                    \n",
            "\u001b[2;36m                    \u001b[0m         ===================================================\n",
            "\u001b[2;36m                    \u001b[0m         =============================                      \n",
            "INFO:train:\n",
            "================================================================================\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m STARTING TRAINING                                  \n",
            "INFO:train:STARTING TRAINING\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m ===================================================\n",
            "\u001b[2;36m                    \u001b[0m         =============================                      \n",
            "\u001b[2;36m                    \u001b[0m                                                            \n",
            "INFO:train:================================================================================\n",
            "\n",
            "Starting training...\n",
            "\n",
            "================================================================================\n",
            "Epoch 1/1\n",
            "================================================================================\n",
            "Training Epoch 1: 100% 112/112 [05:13<00:00,  2.80s/it, loss=2.7586, ppl=15.78, lr=2.80e-05, samples=896]\n",
            "Validating: 100% 13/13 [00:12<00:00,  1.08it/s]\n",
            "\n",
            "Validation - Loss: 2.6863, PPL: 14.68, BPT: 3.8755\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 28 that is less than the current step 29. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
            "Checkpoint saved: outputs/runs/checkpoints/checkpoint_epoch0_step28_20251122_124839.pt\n",
            "Best checkpoint updated: outputs/runs/checkpoints/best.pt\n",
            "\n",
            "Training completed in 0.10 hours\n",
            "\u001b[2;36m[11/22/25 12:49:02]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m                                                    \n",
            "\u001b[2;36m                    \u001b[0m         ===================================================\n",
            "\u001b[2;36m                    \u001b[0m         =============================                      \n",
            "INFO:train:\n",
            "================================================================================\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m TRAINING COMPLETED                                 \n",
            "INFO:train:TRAINING COMPLETED\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m ===================================================\n",
            "\u001b[2;36m                    \u001b[0m         =============================                      \n",
            "INFO:train:================================================================================\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Best validation loss: \u001b[1;36m2.6863\u001b[0m                       \n",
            "INFO:train:Best validation loss: 2.6863\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Total training time: \u001b[1;36m0.10\u001b[0m hours                    \n",
            "INFO:train:Total training time: 0.10 hours\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Best checkpoint:                                   \n",
            "\u001b[2;36m                    \u001b[0m         outputs/runs/checkpoints/checkpoint_epoch0_step28_2\n",
            "\u001b[2;36m                    \u001b[0m         0251122_124839.pt                                  \n",
            "INFO:train:Best checkpoint: outputs/runs/checkpoints/checkpoint_epoch0_step28_20251122_124839.pt\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m updating run metadata (0.0s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m updating run metadata (0.0s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m updating run metadata (0.0s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m updating run metadata (0.0s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m uploading summary, console lines 98-119 (0.0s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                       train/bpt ▁█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/ema_token_loss ▁█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/epoch ▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 train/grad_norm █▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 train/lora_rank ▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                        train/lr ▁█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                       train/ppl ▁█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      train/step ▁█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 train/step_time █▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train/throughput_tokens_per_sec ▁█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                              +5 ...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                       train/bpt 3.76481\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/ema_token_loss 2.66391\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/epoch 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 train/grad_norm 0.75562\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 train/lora_rank 16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                        train/lr 2e-05\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                       train/ppl 13.59319\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      train/step 20\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 train/step_time 2.81654\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train/throughput_tokens_per_sec 572.68833\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                              +5 ...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mpythia-410m-squad\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/aryanpawar1515-mahindra-university/slm-lora-benchmark/runs/wp2wo1iq\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/aryanpawar1515-mahindra-university/slm-lora-benchmark\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20251122_124304-wp2wo1iq/logs\u001b[0m\n",
            "W&B run finished\n",
            "\u001b[2;36m[11/22/25 12:49:03]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m                                                    \n",
            "\u001b[2;36m                    \u001b[0m         Training script completed successfully!            \n",
            "INFO:train:\n",
            "Training script completed successfully!\n",
            "-rw-r--r-- 1 root root 1.6G Nov 22 12:05 /content/backups/squad_checkpoints/best.pt\n",
            "-rw-r--r-- 1 root root 1.6G Nov 22 11:49 /content/backups/wikitext2_checkpoints/best.pt\n",
            "-rw-r--r-- 1 root root 1.6G Nov 22 11:57 /content/backups/yelp_checkpoints/best.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "datasets = {\n",
        "    'TinyStories': 'outputs/runs/checkpoints/checkpoint_epoch0_step57_20251122_111714.pt',\n",
        "    'AG News': 'outputs/runs/checkpoints/checkpoint_epoch0_step56_20251122_113012.pt',\n",
        "    'WikiText-2': '/content/backups/wikitext2_checkpoints/best.pt',\n",
        "    'Yelp': '/content/backups/yelp_checkpoints/best.pt',\n",
        "    'SQuAD': '/content/backups/squad_checkpoints/best.pt'\n",
        "}\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"FINAL RESULTS - ALL 5 DATASETS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for name, path in datasets.items():\n",
        "    try:\n",
        "        checkpoint = torch.load(path)\n",
        "        metrics = checkpoint['metrics']\n",
        "        print(f\"\\n{name}:\")\n",
        "        print(f\"  Val Loss: {metrics['val_loss']:.4f}\")\n",
        "        print(f\"  Val PPL: {metrics['val_ppl']:.2f}\")\n",
        "        print(f\"  Val BPT: {metrics['val_bpt']:.4f}\")\n",
        "    except:\n",
        "        print(f\"\\n{name}: File not found at {path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VRC9mtGcf795",
        "outputId": "519cf2b2-35a7-4c73-a48d-7a3bbab7649b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "FINAL RESULTS - ALL 5 DATASETS\n",
            "================================================================================\n",
            "\n",
            "TinyStories:\n",
            "  Val Loss: 1.8655\n",
            "  Val PPL: 6.46\n",
            "  Val BPT: 2.6914\n",
            "\n",
            "AG News:\n",
            "  Val Loss: 3.4194\n",
            "  Val PPL: 30.55\n",
            "  Val BPT: 4.9331\n",
            "\n",
            "WikiText-2:\n",
            "  Val Loss: 3.4104\n",
            "  Val PPL: 30.28\n",
            "  Val BPT: 4.9202\n",
            "\n",
            "Yelp:\n",
            "  Val Loss: 3.2349\n",
            "  Val PPL: 25.40\n",
            "  Val BPT: 4.6669\n",
            "\n",
            "SQuAD:\n",
            "  Val Loss: 2.6863\n",
            "  Val PPL: 14.68\n",
            "  Val BPT: 3.8755\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "token = getpass.getpass('GitHub Token: ')\n",
        "\n",
        "!git add .\n",
        "!git commit -m \"Complete training on 5 datasets: TinyStories, AG News, WikiText-2, Yelp, SQuAD\"\n",
        "!git push https://{token}@github.com/aryanpawar1234/slm-lora-benchmark.git main"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v6LhES8ngLmr",
        "outputId": "41be2c4d-35ea-427f-ecd5-6a1624c57a9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GitHub Token: ··········\n",
            "[main af7b71f] Complete training on 5 datasets: TinyStories, AG News, WikiText-2, Yelp, SQuAD\n",
            " 3 files changed, 3 insertions(+), 3 deletions(-)\n",
            "Enumerating objects: 11, done.\n",
            "Counting objects: 100% (11/11), done.\n",
            "Delta compression using up to 2 threads\n",
            "Compressing objects: 100% (4/4), done.\n",
            "Writing objects: 100% (6/6), 585 bytes | 585.00 KiB/s, done.\n",
            "Total 6 (delta 2), reused 0 (delta 0), pack-reused 0\n",
            "remote: Resolving deltas: 100% (2/2), completed with 2 local objects.\u001b[K\n",
            "To https://github.com/aryanpawar1234/slm-lora-benchmark.git\n",
            "   b1fbc04..af7b71f  main -> main\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2682bfbc"
      },
      "source": [
        "After setting your user identity, you can try to commit and push your changes again."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test generation on one model\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-410m\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/pythia-410m\")\n",
        "model = PeftModel.from_pretrained(model, \"/content/backups/tinystories_checkpoints/\")\n",
        "\n",
        "# Generate sample\n",
        "prompt = \"Once upon a time\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "outputs = model.generate(**inputs, max_length=100)\n",
        "print(tokenizer.decode(outputs[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 563
        },
        "id": "loANlMwEh3EY",
        "outputId": "c766442e-04ad-4550-bcc3-afa83b490ee2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Can't find 'adapter_config.json' at '/content/backups/tinystories_checkpoints/'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/peft/config.py\u001b[0m in \u001b[0;36m_get_peft_type\u001b[0;34m(cls, model_id, **hf_hub_download_kwargs)\u001b[0m\n\u001b[1;32m    316\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m                 config_file = hf_hub_download(\n\u001b[0m\u001b[1;32m    318\u001b[0m                     \u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0marg_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"repo_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"from_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"to_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 \u001b[0mvalidate_repo_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrepo_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         raise HFValidationError(\n\u001b[0m\u001b[1;32m    155\u001b[0m             \u001b[0;34m\"Repo id must be in the form 'repo_name' or 'namespace/repo_name':\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/content/backups/tinystories_checkpoints/'. Use `repo_type` argument if needed.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3036847416.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"EleutherAI/pythia-410m\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"EleutherAI/pythia-410m\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPeftModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"/content/backups/tinystories_checkpoints/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Generate sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/peft/peft_model.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, model, model_id, adapter_name, is_trainable, config, autocast_adapter_dtype, ephemeral_gpu_offload, low_cpu_mem_usage, key_mapping, **kwargs)\u001b[0m\n\u001b[1;32m    457\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0muse_auth_token\u001b[0m \u001b[0;34m:=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"use_auth_token\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m                 \u001b[0mhf_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"use_auth_token\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muse_auth_token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 459\u001b[0;31m             config = PEFT_TYPE_TO_CONFIG_MAPPING[PeftConfig._get_peft_type(model_id, **hf_kwargs)].from_pretrained(\n\u001b[0m\u001b[1;32m    460\u001b[0m                 \u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/peft/config.py\u001b[0m in \u001b[0;36m_get_peft_type\u001b[0;34m(cls, model_id, **hf_hub_download_kwargs)\u001b[0m\n\u001b[1;32m    321\u001b[0m                 )\n\u001b[1;32m    322\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Can't find '{CONFIG_NAME}' at '{model_id}'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0mloaded_attributes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_json_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Can't find 'adapter_config.json' at '/content/backups/tinystories_checkpoints/'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Navigate to repo\n",
        "%cd /content/slm-lora-benchmark\n",
        "\n",
        "# Check what we're committing\n",
        "!git status\n",
        "\n",
        "# Add everything\n",
        "!git add .gitignore configs/ scripts/ src/ report/ README.md\n",
        "\n",
        "# Commit\n",
        "!git commit -m \"Final project: Complete 5-dataset training with LaTeX report\"\n",
        "\n",
        "# Push to GitHub\n",
        "import getpass\n",
        "token = getpass.getpass('GitHub Token: ')\n",
        "\n",
        "!git push https://{token}@github.com/aryanpawar1234/slm-lora-benchmark.git main"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c27UuzY8ociN",
        "outputId": "f52caae0-1713-4029-e165-04372a128687"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/slm-lora-benchmark\n",
            "On branch main\n",
            "Your branch is ahead of 'origin/main' by 1 commit.\n",
            "  (use \"git push\" to publish your local commits)\n",
            "\n",
            "Changes not staged for commit:\n",
            "  (use \"git add <file>...\" to update what will be committed)\n",
            "  (use \"git restore <file>...\" to discard changes in working directory)\n",
            "\t\u001b[31mmodified:   wandb/debug-internal.log\u001b[m\n",
            "\t\u001b[31mmodified:   wandb/debug.log\u001b[m\n",
            "\t\u001b[31mmodified:   wandb/latest-run\u001b[m\n",
            "\n",
            "no changes added to commit (use \"git add\" and/or \"git commit -a\")\n",
            "On branch main\n",
            "Your branch is ahead of 'origin/main' by 1 commit.\n",
            "  (use \"git push\" to publish your local commits)\n",
            "\n",
            "Changes not staged for commit:\n",
            "  (use \"git add <file>...\" to update what will be committed)\n",
            "  (use \"git restore <file>...\" to discard changes in working directory)\n",
            "\t\u001b[31mmodified:   wandb/debug-internal.log\u001b[m\n",
            "\t\u001b[31mmodified:   wandb/debug.log\u001b[m\n",
            "\t\u001b[31mmodified:   wandb/latest-run\u001b[m\n",
            "\n",
            "no changes added to commit (use \"git add\" and/or \"git commit -a\")\n",
            "GitHub Token: ··········\n",
            "Everything up-to-date\n"
          ]
        }
      ]
    }
  ]
}