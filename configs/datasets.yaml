# Dataset Configuration - Reduced for faster training

datasets:
  # Core Language Modeling Datasets
  wikitext_103:
    name: "Salesforce/wikitext"
    subset: "wikitext-103-raw-v1"
    split: "train"
    text_column: "text"
    max_samples: 2000
    description: "Large-scale Wikipedia articles"

  wikitext_2:
    name: "Salesforce/wikitext"
    subset: "wikitext-2-raw-v1"
    split: "train"
    text_column: "text"
    max_samples: 1000
    description: "Smaller Wikipedia corpus"

  tinystories:
    name: "roneneldan/TinyStories"
    subset: null
    split: "train"
    text_column: "text"
    max_samples: 2000
    description: "Simple narrative stories"

  # News & Articles
  ag_news:
    name: "ag_news"
    subset: null
    split: "train"
    text_column: "text"
    max_samples: 2000
    description: "News articles classification"

  cnn_dailymail:
    name: "cnn_dailymail"
    subset: "3.0.0"
    split: "train"
    text_column: "article"
    max_samples: 1000
    description: "CNN and Daily Mail news articles"

  # Reviews
  yelp_reviews:
    name: "yelp_review_full"
    subset: null
    split: "train"
    text_column: "text"
    max_samples: 1000
    description: "Restaurant reviews"

  # QA
  squad:
    name: "squad"
    subset: null
    split: "train"
    text_column: "context"
    max_samples: 1000
    description: "Reading comprehension contexts"

# Preprocessing settings
preprocessing:
  max_length: 512
  stride: 256
  min_length: 32
  remove_empty: true
  lowercase: false
  remove_special_chars: false

# Tokenization settings
tokenization:
  padding: "max_length"
  truncation: true
  return_attention_mask: true
  return_token_type_ids: false

# Data loading settings
dataloader:
  batch_size: 8
  num_workers: 2
  pin_memory: true
  shuffle: true
  drop_last: true
  prefetch_factor: 2

# Validation split settings
validation:
  split_ratio: 0.1
  seed: 42
