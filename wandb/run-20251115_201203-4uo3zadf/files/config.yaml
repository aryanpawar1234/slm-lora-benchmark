_wandb:
    value:
        cli_version: 0.23.0
        e:
            7kalourqy0avow6gisy58690xtflxhbp:
                args:
                    - --config
                    - configs/experiments/baseline.yaml
                codePath: scripts/train.py
                codePathLocal: scripts/train.py
                cpu_count: 6
                cpu_count_logical: 12
                disk:
                    /:
                        total: "499963174912"
                        used: "147802144768"
                email: aryanpawar1515@gmail.com
                executable: /Library/Developer/CommandLineTools/usr/bin/python3
                git:
                    commit: 650934bf0a41a8c6f0a0b758c7daa6cac205368f
                    remote: https://github.com/aryanpawar1234/slm-lora-benchmark.git
                host: apples-MacBook-Pro.local
                memory:
                    total: "17179869184"
                os: macOS-15.7.1-x86_64-i386-64bit
                program: /Users/apple/Desktop/slm-lora-benchmark/scripts/train.py
                python: CPython 3.9.6
                root: /Users/apple/Desktop/slm-lora-benchmark
                startedAt: "2025-11-15T14:42:03.379950Z"
                writerId: 7kalourqy0avow6gisy58690xtflxhbp
        m: []
        python_version: 3.9.6
        t:
            "1":
                - 1
                - 5
                - 11
                - 41
                - 49
                - 51
                - 53
                - 71
                - 98
            "2":
                - 1
                - 5
                - 11
                - 41
                - 49
                - 51
                - 53
                - 71
                - 98
            "3":
                - 13
                - 16
            "4": 3.9.6
            "5": 0.23.0
            "6": 4.57.1
            "12": 0.23.0
            "13": darwin-x86_64
datasets:
    value:
        wikitext2:
            hf_name: wikitext
            split: train
            subset: wikitext-2-raw-v1
            text_field: text
            val_split: validation
experiment:
    value:
        name: baseline_gpt2_wikitext2
        use_dataset: wikitext2
include:
    value:
        - model_config.yaml
        - lora_config.yaml
        - training_config.yaml
        - datasets.yaml
lora:
    value:
        bias: none
        lora_alpha: 16
        lora_dropout: 0.05
        r: 8
        target_modules:
            - c_attn
        task_type: CAUSAL_LM
model:
    value:
        max_length: 512
        name: gpt2
        use_cache: false
training:
    value:
        batch_size: 4
        gradient_accumulation_steps: 4
        learning_rate: "5e-5"
        log_every_steps: 10
        max_grad_norm: 1
        num_epochs: 1
        output_dir: outputs/checkpoints
        save_every_steps: 200
        seed: 42
        warmup_steps: 100
        weight_decay: 0.01
wandb:
    value:
        entity: null
        project: slm-lora-benchmark
        run_name: gpt2-wikitext2-lora-baseline
